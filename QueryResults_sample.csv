QuestionUserId,QuestionerReputation,Tags,QuestionId,QuestionScore,title,QuestionBody,QuestionDate,AcceptedAnswer,AnswerUserId,AnswerReputation,AnswerScore,AnswerId,AnswerBody
"3625340","33","<image-processing><machine-learning><svm><feature-detection><feature-extraction>","27729199","0","How to find Relevent Features for Comparing Diagrams(Images)?","<p>Currently we are doing a project on diagram comparison using SVM. Diagrams mainly include flow charts and block diagram.we are stuck on determining relevant features for training the SVM. The problem is that diagram and its components to be compared can be of various scales.  Please help for the same.</p>
","2015-01-01 08:03:13","27733517","1056563","45409","0","27733517","<p>In regard solely to the difference in scales:  this seems relatively straightforward. Set up classifier(s) based on the characteristics of the images in terms of the shapes/ pixel densities/ arrangements of artifacts. Those aspects are scale-independent.  You may also wish to introduce rotation and shear invariant capabilities into the svm.</p>
"
"4409773","788","<java><machine-learning><svm><encog>","27729238","1","SVM using Encog in Java for beginners","<p>I am beginner in SVM. Could someone please help me to understand the concepts of SVM using Encog from the very basics?? It will be helpful with sample Java code.</p>
","2015-01-01 08:10:29","27808712","173355","3142","1","27808712","<p>In Encog SVM is just a classification or regression model and can be used mostly interchangably with other model types.  I modified the Hello World XOR example to use it, you can see the results below.</p>

<p>This is a decent intro to them:  <a href=""http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf"" rel=""nofollow"">http://webdoc.nyumc.org/nyumc/files/chibi/user-content/Final.pdf</a>
This is a more basic intro to modeling in general, I wrote it for neural networks, but it applies to SVM as well:  <a href=""http://www.heatonresearch.com/content/non-mathematical-introduction-using-neural-networks"" rel=""nofollow"">http://www.heatonresearch.com/content/non-mathematical-introduction-using-neural-networks</a></p>

<pre><code>package org.encog.examples.neural.xor;

import org.encog.Encog;
import org.encog.ml.data.MLData;
import org.encog.ml.data.MLDataPair;
import org.encog.ml.data.MLDataSet;
import org.encog.ml.data.basic.BasicMLDataSet;
import org.encog.ml.svm.SVM;
import org.encog.ml.svm.training.SVMTrain;

public class XORHelloWorld {

    /**
     * The input necessary for XOR.
     */
    public static double XOR_INPUT[][] = { { 0.0, 0.0 }, { 1.0, 0.0 },
            { 0.0, 1.0 }, { 1.0, 1.0 } };

    /**
     * The ideal data necessary for XOR.
     */
    public static double XOR_IDEAL[][] = { { 0.0 }, { 1.0 }, { 1.0 }, { 0.0 } };

    /**
     * The main method.
     * @param args No arguments are used.
     */
    public static void main(final String args[]) {

        // create a SVM for classification, change false to true for regression     
        SVM svm = new SVM(2,false);

        // create training data
        MLDataSet trainingSet = new BasicMLDataSet(XOR_INPUT, XOR_IDEAL);

        // train the SVM
        final SVMTrain train = new SVMTrain(svm, trainingSet);
        train.iteration();
        train.finishTraining();

        // test the SVM
        System.out.println(""SVM Results:"");
        for(MLDataPair pair: trainingSet ) {
            final MLData output = svm.compute(pair.getInput());
            System.out.println(pair.getInput().getData(0) + "","" + pair.getInput().getData(1)
                    + "", actual="" + output.getData(0) + "",ideal="" + pair.getIdeal().getData(0));
        }

        Encog.getInstance().shutdown();
    }
}
</code></pre>
"
"4408281","721","<python-2.7><machine-learning>","27730775","1","Why does not the following code snippet run successfully?","<p>I was reading Programming Collective Intelligence,chapter on Search Engines where I came across the following piece of code and upon implementation,it gave me error.Please Help.</p>

<pre><code>import urllib2
from BeautifulSoup import *
from urlparse import urljoin

class crawler:
def __init__(self,dbname):
    pass

def __del__(self):
    pass
def dbcommit(self):
    pass

def getentryid(self,table,field,value,createnew=True):
    return None

def addtoindex(self,url,soup):
    print 'Indexing %s' % url

def gettextonly(self,soup):
    return None

def seperatewords(self,text):
    return None

def isindexed(self,url):
    return False

def addlinkref(self,urlFrom,urlTo,linkText):
    pass

def crawl(self,pages,depth=2):
    for i in range(depth):
        newpages=set()
        for page in pages:
            try:
                c=urllib2.urlopen(page)
            except:
                print 'Could not open %s'%page
                continue
            soup=BeautifulSoup(c.read())
            self.addtoindex(page,soup)

            links=soup('a')
            for link in links:
                if('href' in dict(link.attrs)):
                    url=urljoin(page,link['href'])
                    if url.find(""'"")!=-1: continue
                    url=url.split('#')[0]
                    if url[0:4]=='http' and not self.isindexed(url):
                        newpages.add(url)
                    linkText=self.gettextonly(link)
                    self.addlinkref(page,url,linkTest)
                self.dbcommit()
            pages=newpages


def createindextables(self):
    pass
</code></pre>

<p>I got the following error:</p>

<pre><code>&gt;&gt;cwlr.crawl(pagelist)
Indexing http://en.wikipedia.org/wiki/Artificial_neural_network
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
&lt;ipython-input-50-97778b0c0db8&gt; in &lt;module&gt;()
----&gt; 1 cwlr.crawl(pagelist)

C:\Users\Blue\Anaconda\searchengine.py in crawl(self, pages, depth)
     47                                                 url=urljoin(page,link['href'])
     48                                                 if url.find(""'"")!=-1: continue
---&gt; 49                                                 url=url.split('#')[0]
     50                                                 if url[0:4]=='http' and not      self.isindexed(url):
 51                                                         newpages.add(url)

NameError: global name 'linkTest' is not defined
</code></pre>
","2015-01-01 12:22:49","27730829","367273","435041","1","27730829","<blockquote>
  <p>NameError: global name 'linkTest' is not defined</p>
</blockquote>

<p>You've misspelt <code>linkText</code> as <code>linkTest</code>:</p>

<pre><code>linkText=self.gettextonly(link)
      ↑
self.addlinkref(page,url,linkTest)
                               ↑
</code></pre>
"
"495815","1370","<apache-spark><machine-learning><prediction><apache-spark-mllib><predictionio>","27734329","18","Incremental training of ALS model","<p>I'm trying to find out if it is possible to have ""incremental training"" on data using MLlib in Apache Spark.</p>

<p>My platform is Prediction IO, and it's basically a wrapper for Spark (MLlib), HBase, ElasticSearch and some other Restful parts.</p>

<p>In my app data ""events"" are inserted in real-time, but to get updated prediction results I need to ""pio train"" and ""pio deploy"". This takes some time and the server goes offline during the redeploy.</p>

<p>I'm trying to figure out if I can do incremental training during the ""predict"" phase, but cannot find an answer.</p>
","2015-01-01 20:21:14","","4674497","602","0","36767722","<p>For updating Your model near-online (I write near, because face it, the true online update is impossible) by using fold-in technique, e.g.:
<a href=""http://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle2008-Online_Updating_Regularized_Kernel_Matrix_Factorization_Models.pdf"" rel=""nofollow noreferrer"">Online-Updating Regularized Kernel Matrix Factorization Models for Large-Scale Recommender Systems.</a></p>

<p>Ou You can look at code of:</p>

<ul>
<li><a href=""http://www.mymedialite.net"" rel=""nofollow noreferrer"">MyMediaLite</a></li>
<li><a href=""https://github.com/cloudera/oryx"" rel=""nofollow noreferrer"">Oryx</a> - framework build with Lambda Architecture paradigm. And it should have updates with fold-in of new users/items.</li>
</ul>

<p>It's the part of my answer for similar <a href=""https://stackoverflow.com/questions/36723429/how-can-i-handle-new-users-items-in-model-generated-by-spark-als-from-mllib"">question</a> where both problems: near-online training and handling new users/items were mixed.</p>
"
"495815","1370","<apache-spark><machine-learning><prediction><apache-spark-mllib><predictionio>","27734329","18","Incremental training of ALS model","<p>I'm trying to find out if it is possible to have ""incremental training"" on data using MLlib in Apache Spark.</p>

<p>My platform is Prediction IO, and it's basically a wrapper for Spark (MLlib), HBase, ElasticSearch and some other Restful parts.</p>

<p>In my app data ""events"" are inserted in real-time, but to get updated prediction results I need to ""pio train"" and ""pio deploy"". This takes some time and the server goes offline during the redeploy.</p>

<p>I'm trying to figure out if I can do incremental training during the ""predict"" phase, but cannot find an answer.</p>
","2015-01-01 20:21:14","","2220275","737","4","36917856","<p>I imagine you are using spark MLlib's ALS model which is performing matrix factorization. The result of the model are two matrices a user-features matrix and an item-features matrix. </p>

<p>Assuming we are going to receive a stream of data with ratings or transactions for the case of implicit, a real (100%) online update of this model will be to update both matrices for each new rating information coming by triggering a full retrain of the ALS model on the entire data again + the new rating. In this scenario one is limited by the fact that running the entire ALS model is computationally expensive and the incoming stream of data could be frequent, so it would trigger a full retrain too often.</p>

<p>So, knowing this we can look for alternatives, a single rating should not change the matrices much plus we have optimization approaches which are incremental, for example SGD. There is an interesting (still experimental) library written for the case of Explicit Ratings which does incremental updates for each batch of a DStream:</p>

<p><a href=""https://github.com/brkyvz/streaming-matrix-factorization"" rel=""nofollow"">https://github.com/brkyvz/streaming-matrix-factorization</a></p>

<p>The idea of using an incremental approach such as SGD follows the idea of as far as one moves towards the gradient (minimization problem) one guarantees that is moving towards a minimum of the error function. So even if we do an update to the single new rating, only to the user feature matrix for this specific user, and only the item-feature matrix for this specific item rated, and the update is towards the gradient, we guarantee that we move towards the minimum, of course as an approximation, but still towards the minimum. </p>

<p>The other problem comes from spark itself, and the distributed system, ideally the updates should be done sequentially, for each new incoming rating, but spark treats the incoming stream as a batch, which is distributed as an RDD, so the operations done for updating would be done for the entire batch with no guarantee of sequentiality.</p>

<p>In more details if you are using Prediction.IO for example, you could do an off line training which uses the regular train and deploy functions built in, but if you want to have the online updates you will have to access both matrices for each batch of the stream, and run updates using SGD, then ask for the new model to be deployed, this functionality of course is not in Prediction.IO you would have to build it on your own. </p>

<p>Interesting notes for SGD updates:   </p>

<p><a href=""http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf"" rel=""nofollow"">http://stanford.edu/~rezab/classes/cme323/S15/notes/lec14.pdf</a>   </p>
"
"3423045","968","<c++><opencv><machine-learning><svm>","27738986","3","Find confidence of prediction in SVM","<p>I am doing English digit classification using SVM classifier in opencv. 
I am able to predict the classes using <code>predict()</code> function. 
But I want get confidence of prediction between 0-1. Can somebody provide a method to do it using opencv</p>

<pre><code> //svm parameters used
 m_params.svm_type    = CvSVM::C_SVC;
 m_params.kernel_type = CvSVM::RBF;
 m_params.term_crit   = cvTermCriteria(CV_TERMCRIT_ITER, 500, 1e-8);

 //for training
 svmob.train_auto(m_features, m_labels, cv::Mat(), cv::Mat(), m_params, 10);

 //for prediction
 predicted = svmob.predict(testData);
</code></pre>
","2015-01-02 07:39:41","27739386","1190430","5156","10","27739386","<p>SVM during training tries to find a separating hyperplane such that trainset examples lay on different sides. There could be many such hyperplanes (or none), so to select the ""best"" we look for the one for which total distance from all classes are maximized. Indeed, the further away from the hyperplane point is located — the more confident we are in the decision. So what we are interested in is distance to the hyperplane.</p>

<p>As per OpenCV <a href=""http://docs.opencv.org/modules/ml/doc/support_vector_machines.html#cvsvm-predict"">documentation</a>, <code>CvSVM::predict</code> has a default second arguments which specifies what to return. By default, it returns classification label, but you can pass in <code>true</code> and it'll return the distance.</p>

<p>The distance itself is pretty ok, but if you want to have a confidence value in a range (0, 1), you can apply <a href=""http://en.wikipedia.org/wiki/Sigmoid_function"">sigmoidal</a> function to the result. One of such functions if logistic function.</p>

<pre><code>decision = svmob.predict(testData, true);
confidence = 1.0 / (1.0 + exp(-decision));
</code></pre>
"
"4410616","61","<python-2.7><machine-learning>","27744741","1","How to resolve the following error?","<p>I was reading the the chapter on search engines in programming collective intelligence and came across the following code snippet and tried to implement it in IPython. However, I encountered error:</p>

<pre><code>def getmatchrows(self,q):
    fieldlist='w0.urlid'
    tablelist=''
    clauselist=''
    wordids=[]

    words=q.split()
    tablenumber=0

    for word in words:
        wordrow=self.con.execute(""select rowid from wordlist where word='%s'"" % word).fetchone()
        if wordrow!=None:
            wordid=wordrow[0]
            wordids.append(wordid)
            if tablenumber&gt;0:
                tablelist+=','
                clauselist+=' and '
                clauselist+='w%d.urlid=w%d.urlid and '%(tablenumber-1,tablenumber)
            fieldlist+=',w%d.location'%tablenumber
            tablelist+='wordlocation w%d'%tablenumber
            clauselist+='w%d.wordid=%d'%(tablenumber,wordid)
            tablenumber+=1
    fullquery=""SELECT %s FROM %s WHERE %s""%(fieldlist,tablelist,clauselist)
    cur=self.con.execute(fullquery)
    rows=[row for row in cur]
    return row,wordids
</code></pre>

<p>I got the following error:</p>

<pre><code>TypeError                                 Traceback (most recent call last)
&lt;ipython-input-25-e5fade95f22f&gt; in &lt;module&gt;()
----&gt; 1 e.getmatchrows('data analysis')

C:\Users\Blue\Anaconda\searchengine.pyc in getmatchrows(self, q)
    128                                         tablelist+=','
    129                                         clauselist+=' and '
--&gt; 130                                         clauselist+='w%d.urlid=w%d.urlid and '%tablenumber-   1,tablenumber
131                                 fieldlist+=',w%d.location'%tablenumber
132                                 tablelist+='wordlocation w%d'%tablenumber

TypeError: unsupported operand type(s) for &amp;: 'str' and 'tuple'
</code></pre>
","2015-01-02 15:50:50","27744808","4408281","721","0","27744808","<p>Put parantheses at line 130:</p>

<pre><code>tablenumber-1,tablenumber&gt;&gt;(tablenumber1,tablenumber)
</code></pre>
"
"1243926","6036","<machine-learning><cross-validation>","27745033","0","Evaluating models on the entire training set with no cross-validation","<p>We have a dataset with 10,000 manually labeled instances, and a classifier that was trained on all of this data.
The classifier was then evaluated on ALL of this data to obtain a 95% success rate.</p>
<p>What exactly is wrong with this approach? Is it just that the statistic 95% is not very informative in this setup? Can there still be some value in this 95% number? While I understand that, theoretically, it is not a good idea, I don't have enough experience in this area to be sure by myself. Also note that I have neither built nor evaluated the classifier in question.</p>
<p>Common sense aside, could someone give me a very solid, authoritative reference, saying that this setup is somehow wrong?</p>
<p>For example, <a href=""http://www.saedsayad.com/model_evaluation.htm"" rel=""nofollow noreferrer"">this page</a> does say</p>
<blockquote>
<p>Evaluating model performance with the data used for training is not acceptable in data mining because it can easily generate overoptimistic and overfitted models.</p>
</blockquote>
<p>However, this is hardly an authoritative reference. In fact, this quote is plainly wrong, as the evaluation has nothing to do with generating overfitted models. It could generate overoptimistic data scientists who would choose the wrong model, but a particular evaluation strategy does not have anything to do with overfitting models per se.</p>
","2015-01-02 16:14:00","","1361822","16075","1","27745227","<p>The problem is the possibility of <a href=""http://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow"">overfitting</a>. That does not mean that there is no value in the accuracy you reported for that entire data set, as it can be considered an estimate of the upper bound for the performance of the classifier on new data.</p>

<p>It is subjective to say who constitutes a ""very solid, authoritative reference""; however <em>Machine Learning</em> by Tom Mitchell (ISBN 978-0070428072) is a widely read and oft-cited text that discusses the problem of overfitting in general and specifically with regard to decision trees and artificial neural networks. In addition to discussion of overfitting, the text also discusses various approaches to the <em>training and validation set</em> approach (e.g., cross-validation).</p>
"
"4140027","3975","<python><pandas><machine-learning><nlp><scikit-learn>","27753168","0","Wrong prediction with SVC classifier in scikit-learn?","<p>I generated my own corpus, so I split into a training text file like this:</p>

<pre><code>POS|This film was awesome, highly recommended
NEG|I did not like this film
NEU|I went to the movies
POS|this film is very interesting, i liked a lot
NEG|the film was very boring i did not like it
NEU|the cinema is big
NEU|the cinema was dark
</code></pre>

<p>And for testing I have another text review, which is unlabeled:</p>

<pre><code>I did not like this film
</code></pre>

<p>Then I do the following: </p>

<pre><code>import pandas as pd
from sklearn.feature_extraction.text import HashingVectorizer

trainingdata = pd.read_csv('/Users/user/Desktop/training.txt',
                 header=None, sep='|', names=['labels', 'movies_reviews'])


vect = HashingVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True, n_features=7)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
TestText= pd.read_csv('/Users/user/Desktop/testing.txt',
                     header=None, names=['test_opinions'])
test = vect.transform(TestText['test_opinions'])
from sklearn.svm import SVC
svm = SVC()
svm.fit(X, y)

prediction = svm.predict(test)
print prediction
</code></pre>

<p>And the prediction is:</p>

<pre><code>['NEU']
</code></pre>

<p>Then something that comes to my mind is why this prediction is wrong?. Does this is a code problem or a feature or a classification algorithm problem?, I tried to play with this and when I remove the last review from the training text file I realize that always is predicting the last element of that file. Any idea of how to fix this problem?.</p>
","2015-01-03 08:50:15","","419338","13672","1","27753656","<p>SVMs are notoriously sensitive to parameter settings. You will need to do a grid search to find the right values. I tried training two kinds of Naive Bayes on your dataset and I got perfect accuracy on the training set:</p>

<pre><code>from sklearn.naive_bayes import *
from sklearn.feature_extraction.text import *

# first option- Gaussian NB
vect = HashingVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
nb = GaussianNB().fit(X.A,y) # input needs to be dense
nb.predict(X.A) == y

# second option- MultinomialNB (input needs to be positive, use CountingVect instead)
vect = CountVectorizer(analyzer='word', ngram_range=(2,2), lowercase=True)
X = vect.fit_transform(trainingdata['movies_reviews'])
y = trainingdata['labels']
nb = MultinomialNB().fit(X,y)
nb.predict(X.A) == y
</code></pre>

<p>In both cases the output is</p>

<pre><code>Out[33]: 
0    True
1    True
2    True
3    True
4    True
5    True
6    True
Name: labels, dtype: bool
</code></pre>
"
"713087","2949","<python><macos><machine-learning><nlp><stanford-nlp>","27758540","0","Stanford pos tagger not displaying the output elements in Python (MAC)","<pre><code>#-*- coding:Utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf8')
import os
java_path = ""/usr/libexec/java_home"" # replace this
os.environ['JAVAHOME'] = java_path

from nltk.tag.stanford import POSTagger

french_postagger = POSTagger(""stanford-postagger-full-2014-10-26/models/french.tagger"", ""stanford-postagger-full-2014-10-26/stanford-postagger.jar"", encoding=""utf-8"")
english_postagger = POSTagger(""stanford-postagger-full-2014-10-26/models/english-bidirectional-distsim.tagger"", ""stanford-postagger-full-2014-10-26/stanford-postagger.jar"", encoding=""utf-8"")


print french_postagger.tag(""siddhartha is a good boy"".split())
</code></pre>

<p>the result is as follows:</p>

<pre><code>[('', u'/System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home')]
</code></pre>

<p>instead I need to see the words and their tags.</p>
","2015-01-03 19:09:24","","1716866","4731","0","29487046","<p>The problem is this part of your code:</p>

<pre><code>java_path = ""/usr/libexec/java_home"" # replace this
os.environ['JAVAHOME'] = java_path
</code></pre>

<p>Where did that code from? It looks like you should replace it. If your setup is like mine, changing that first line to <code>java_path = ""/usr/bin/java""</code> fixes the problem. Actually, if your setup is like mine, just <strong>deleting those two lines completely</strong> fixes the problem (while including them reproduces it):</p>

<pre><code>from nltk.tag.stanford import POSTagger
french_postagger = POSTagger(""models/french.tagger"", ""stanford-postagger.jar"", encoding=""utf-8"")
english_postagger = POSTagger(""models/english-bidirectional-distsim.tagger"", ""stanford-postagger.jar"", encoding=""utf-8"")

print french_postagger.tag(""siddhartha is a good boy"".split())
&gt; [[(u'siddhartha', u'ADV'), (u'is', u'VPP'), (u'a', u'V'), (u'good', u'ET'), (u'boy', u'ET')]]
</code></pre>
"
"2241766","2138","<machine-learning><classification><backpropagation>","27772237","0","Use number of misclassificatios as objective function for back propagation","<p>I'm new to machine learning (neutral network) and I have a question, please help me explain.
In back propagation, the objective function to be minimized is usually a sum of the squared error between the output and the target. However, in classification problems, the goal is often to minimize the total number of misclassifications. Why can this total number of misclassifications not be used directly as an objective function in back propagation?</p>
","2015-01-05 01:18:13","27797690","1190430","5156","0","27797690","<p>Because of mathematics. We'd really like to minimize number of misclassifications, but this target would be non-<a href=""http://en.wikipedia.org/wiki/Smoothness"" rel=""nofollow"">smooth</a> (and not even <a href=""http://en.wikipedia.org/wiki/Continuous_function"" rel=""nofollow"">continuous</a>), and thus hard to optimize.</p>

<p>So, in order to optimize it, we use smooth ""proxies"": sum-of-squares penalizes your mistakes in a continuous way. Namely, a (very) tiny deviation in input parameters results in a tiny change of output. This would not be the case if non-continuous target function was used. </p>

<p>Also, note that to find a misclassification you need to compare output to the actual answer. Since you can't directly compare floating numbers for equality using <code>==</code>, you'll need to tolerate some errors. And, it's really not a big problem to miss answer by <code>0.001</code> when answers are of much bigger magnitude. So you want to push predictions as close to real answers as you can, and you do so by minimizing total distance from predictions to answers.</p>
"
"4416307","1357","<python><python-2.7><machine-learning><nlp><scikit-learn>","27761803","1","Problems loading textual data with scikit-learn?","<p>I'm using my own data to classify into two categories some data, so let:</p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

# Load the text data
categories = [
    'CLASS_1',
    'CLASS_2',
]

text_train_subset = load_files('train',
    categories=categories)

text_test_subset = load_files('test',
    categories=categories)

# Turn the text documents into vectors of word frequencies
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(text_train_subset)
y_train = text_train_subset.target


classifier = MultinomialNB().fit(X_train, y_train)
print(""Training score: {0:.1f}%"".format(
    classifier.score(X_train, y_train) * 100))

# Evaluate the classifier on the testing set
X_test = vectorizer.transform(text_test_subset.data)
y_test = text_test_subset.target
print(""Testing score: {0:.1f}%"".format(
    classifier.score(X_test, y_test) * 100))
</code></pre>

<p>For the above code and the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">documentation</a>, I have the following directory schema:</p>

<pre><code>data_folder/

    train_folder/
        CLASS_1.txt CLASS_2.txt
    test_folder/
        test.txt
</code></pre>

<p>Then I get this error: </p>

<pre><code>    % (size, n_samples))
ValueError: Found array with dim 0. Expected 5
</code></pre>

<p>I also tried fit_transform but still the same. How can I solve this dimession problem?</p>
","2015-01-04 02:35:08","27764579","419338","13672","3","27764579","<p>The first problem is you've got the wrong directory structure. <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_files.html"" rel=""nofollow"">You need it to be like</a> </p>

<pre><code>container_folder/
    CLASS_1_folder/
        file_1.txt, file_2.txt ... 
    CLASS_2_folder/
        file_1.txt, file_2.txt, ....
</code></pre>

<p>You need to have both the train and test set in this directory structure. Alternatively, you can have all data in one directory and use <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html"" rel=""nofollow"">train_test_split</a> to split it in two.</p>

<p>Secondly, </p>

<pre><code>X_train = vectorizer.fit_transform(text_train_subset)
</code></pre>

<p>needs to be </p>

<pre><code>X_train = vectorizer.fit_transform(text_train_subset.data) # added .data
</code></pre>

<p>Here is a complete and working example:</p>

<pre><code>from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB

text_train_subset = load_files('sample-data/web')
text_test_subset = text_train_subset # load your actual test data here

# Turn the text documents into vectors of word frequencies
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(text_train_subset.data)
y_train = text_train_subset.target


classifier = MultinomialNB().fit(X_train, y_train)
print(""Training score: {0:.1f}%"".format(
    classifier.score(X_train, y_train) * 100))

# Evaluate the classifier on the testing set
X_test = vectorizer.transform(text_test_subset.data)
y_test = text_test_subset.target
print(""Testing score: {0:.1f}%"".format(
    classifier.score(X_test, y_test) * 100))
</code></pre>

<p>The directory structure of <code>sample-data/web</code> is</p>

<pre><code>sample-data/web
├── de
│   ├── apollo8.txt
│   ├── fiv.txt
│   ├── habichtsadler.txt
└── en
    ├── elizabeth_needham.txt
    ├── equipartition_theorem.txt
    ├── sunderland_echo.txt
    └── thespis.txt
</code></pre>
"
"1413388","1605","<ruby><machine-learning><cluster-analysis><k-means><hierarchical-clustering>","27771043","0","How to make one-dimensional k-means clustering using Ruby?","<p><strong>My question:</strong></p>

<p>I have searched through available Ruby gems to find one that performs k-means clustering. I've found quite a few: <a href=""http://github.com/id774/kmeans"" rel=""nofollow noreferrer"">kmeans</a>, <a href=""https://github.com/vaneyckt/kmeans-clustering"" rel=""nofollow noreferrer"">kmeans-clustering</a>, <a href=""https://github.com/reddavis/K-Means"" rel=""nofollow noreferrer"">reddavis-k_means</a> and <a href=""https://github.com/ollie/k_means_pp"" rel=""nofollow noreferrer"">k_means_pp</a>. My problem is that none of the gems deals with one-dimensional k-means clustering. They all expect input like this:</p>

<pre><code>[[1, 2], [3, 4], [5, 6]]
</code></pre>

<p>My input looks like this:</p>

<pre><code>[1, 2, 3, 4, 5, 6]
</code></pre>

<p>Hence my question: <em>How do I perform a one-dimensional k-means clustering using Ruby?</em></p>

<p><strong>The context (my task):</strong></p>

<p>I have 100 input values:</p>

<p>0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 5, 5, 8, 8, 10, 16, 18, 22, 22, 35, 50, 50</p>

<p>Each value represents a response time, i.e. the number of minutes it took for some customer service agent to respond to an email from a customer. So the first value 0 indicates that the customer only waited 0 minutes for a response.</p>

<p>I need to find out how many fast, medium-fast and slow response time instances there is. In other words, I want to cut my input values up in 3 pools, and then count how many there are in each pool.</p>

<p>The complicating factor is that I based on the overall slope steepness have to figure out where to make the cuts. There is no fixed definition of fast, medium-fast and slow. The first cut (between fast and medium-fast) should occur where the steepness of the slope starts to increase more drastically than before. The second cut (between medium-fast and slow) should occur when an even more dramatic steepness increase occur.</p>

<p>Here is a graphical representation of the input values.</p>

<p><a href=""https://i.imgur.com/pSEJ0SV.png"" rel=""nofollow noreferrer""><img src=""https://i.imgur.com/pSEJ0SV.png"" /></a></p>

<p>In the above example, common sense would probably define fast as 0-3, because there are many instances of 0, 1, 2, and 3. 4-8 or 4-10 looks like common sense choices for medium-fast. But how to determine something like this mathematically? If the response times were generally faster, then the customers would be expecting this, so an even smaller increase towards the end should trigger the cut.</p>

<p><strong>Finishing notes:</strong></p>

<p>I did find the gem <a href=""http://github.com/davidrichards/kmeans"" rel=""nofollow noreferrer"">davidrichards-kmeans</a> that deals with one-dimensional k-means clustering, but it don't seem to work properly (the example code raises a syntax error).</p>
","2015-01-04 22:25:13","27771247","1060350","70392","2","27771247","<p>k-means is the wrong tool for this job anyway.</p>

<p>It's not designed for fitting an exponential curve.</p>

<p>Here is a much more sound proposal for you:</p>

<p>Look at the plot, mark the three points, and then you have your three groups.</p>

<p>Or look at quantiles... Report the median response time, the 90% quantile, and the 99% quantile...</p>

<p>Clustering is about <strong>structure discovery</strong> in multivariate data. It's probably not what you want it to be, sorry.</p>

<p>If you insist on trying k-means, try encoding the data as</p>

<pre><code>[[1], [2], [3], [4], [5]]
</code></pre>

<p>and check if the results are at least a little bit what you want them to be (also remember that k-means is randomized. Running it multiple times may yield very different results).</p>
"
"4202221","453","<python-3.x><machine-learning><scipy><neural-network><classification>","27780868","1","Backpropagation neural network","<p>I need to use Backpropagation Neural Netwrok for multiclass classification purposes in my application. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it to my needs. It is based on the lections of Machine Learning in Coursera from Andrew Ng. 
I have tested it in IRIS dataset and achieved good results (accuracy of classification around 0.96), whereas on my real data I get terrible results. I assume there is some implementation error, because the data is very simple. But I cannot figure out what exactly is the problem.</p>

<p>What are the parameters that it make sense to adjust?
I tried with:</p>

<ul>
<li>number of units in hidden layer</li>
<li>generalization parameter (lambda)</li>
<li>number of iterations for minimization function</li>
</ul>

<p>Built-in minimization function used in this code is pretty much confusing me. It is used just once, as @goncalopp has mentioned in comment. Shouldn't it iteratively update the weights? How it can be implemented?</p>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. Can someone please point out possible improvements I could make in order to achieve better results.</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
","2015-01-05 13:46:00","27817013","1595865","18638","0","27781347","<p>The most obvious problem is that <strong>your training dataset is very small</strong>.</p>

<p>Since you're using <code>scipy.optimize.minimize</code> instead of the usual iterative gradient descent, I think it's also likely you're <a href=""https://en.wikipedia.org/wiki/Overfitting"" rel=""nofollow""><strong>overfitting</strong> your model to your training data</a>. Possibly a iterative algorithm works better, here. Don't forget to carefully <strong>monitor the validation error</strong>.</p>

<p>If you try backpropagation with gradient descent, notice that, depending on the parameters used on backpropagation, neural networks take a while to converge</p>

<p>You can try to feed the network the same training data multiple times or tweak the <a href=""https://en.wikipedia.org/wiki/Backpropagation#Phase_2:_Weight_update"" rel=""nofollow"">learning rate</a> but ideally you should use more diverse data.</p>
"
"4202221","453","<python-3.x><machine-learning><scipy><neural-network><classification>","27780868","1","Backpropagation neural network","<p>I need to use Backpropagation Neural Netwrok for multiclass classification purposes in my application. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it to my needs. It is based on the lections of Machine Learning in Coursera from Andrew Ng. 
I have tested it in IRIS dataset and achieved good results (accuracy of classification around 0.96), whereas on my real data I get terrible results. I assume there is some implementation error, because the data is very simple. But I cannot figure out what exactly is the problem.</p>

<p>What are the parameters that it make sense to adjust?
I tried with:</p>

<ul>
<li>number of units in hidden layer</li>
<li>generalization parameter (lambda)</li>
<li>number of iterations for minimization function</li>
</ul>

<p>Built-in minimization function used in this code is pretty much confusing me. It is used just once, as @goncalopp has mentioned in comment. Shouldn't it iteratively update the weights? How it can be implemented?</p>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. Can someone please point out possible improvements I could make in order to achieve better results.</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
","2015-01-05 13:46:00","27817013","4202221","453","0","27817013","<p>Correctly normalizing the data solved the problem. I used preprocessing module from sklearn. Here is example:</p>

<pre><code>from sklearn import preprocessing
import numpy as np

X_train = np.array([[ 1., -1.,  2.],
                    [ 2.,  0.,  0.],
                    [ 0.,  1., -1.]])


min_max_scaler = preprocessing.MinMaxScaler()
X_train_minmax = min_max_scaler.fit_transform(X_train)
print(X_train_minmax)

X_test = np.array([[ -3., -1.,  4.]])
X_test_minmax = min_max_scaler.transform(X_test)
print(X_test_minmax)
</code></pre>

<p>And the output is:</p>

<pre><code>[[ 0.5     0.      1.    ]
 [ 1.      0.5     0.3333]
 [ 0.      1.      0.    ]]


[[-1.5     0.      1.6667]]
</code></pre>
"
"3958112","83","<matlab><machine-learning><bayesian>","27790562","-2","How to understand the ""calculate the priors based on occurence in the training set"" in the function","<p>I have a function from a toolbox, I paste it here. I cannot understand the last part,
which begins from ""<strong><code>% // calculate the priors based on occurence in the training set</code></strong>"" ? Can
anybody explain it for me? Thank you so much!</p>

<pre><code>function [scratch] = train_gnb(trainpats,traintargs, in_args, cv_args)

% // Use a Gaussian Naive Bayes classifier to learn regressors.
%
% // [SCRATCH] = TRAIN_GNB(TRAINPATS, TRAINTARGS, IN_ARGS, CV_ARGS)
%
% // The Gaussian Naive Bayes classifier makes the assumption that
% // each data point is conditionally independent of the others, given
% // a class label, and that, furthermore, the likelihood function for
% // each class is normal.  The likelihood of a given data point X,
% // where Y is one of K labels, is thus:
%
% // Pr ( X | Y==K) = Product_N ( Normal(X_N | theta_K) ) 
% 
% // The GNB is trained by finding the Normal MLE's for each subset of
% // the training set that have the same label.  Each voxel has a
% // scalar mean and a scalar variance.
%
% // OPTIONAL ARGUMENTS:
%
% // UNIFORM_PRIOR (default = true): If uniform_prior is true,
% // then the algorithm will assume that no classes are
% // inherently more likely than others, and will use 1/K as
% // the prior probability for each of K classes.  If
% // uniform_prior is false, then train_gnb will estimate the
% // priors from the data using laplace smoothing: if N_k is
% // the number of times class k is observed in the training
% // set and N is the total number of training datapoints, then
% // Pr(Y == k) = (N_k + 1) / (N + K).  This way, no cluster is
% // ever assigned a 0 prior.

% // License:
% // =====================================================================
%
% // This is part of the Princeton MVPA toolbox, released under
% // the GPL. See http://www.csbmb.princeton.edu/mvpa for more
% // information.
% 
%  // The Princeton MVPA toolbox is available free and
% // unsupported to those who might find it useful. We do not
% // take any responsibility whatsoever for any problems that
% // you have related to the use of the MVPA toolbox.
%
% // ======================================================================

defaults.uniform_prior = true;

args = mergestructs(in_args, defaults);

nConds = size(traintargs,1);
[nVox nTimepoints] = size(trainpats);

% // find a gaussian distribution for each voxel for each category

scratch.mu = NaN(nVox, nConds);
scratch.sigma = NaN(nVox, nConds);

for k = 1:nConds

  % // grab the subset of the data with a label of category k
    k_idx = find(traintargs(k, :) == 1);

    if numel(k_idx) &lt; 1
      error('Condition %g has no data points.', k);
    end

    data = trainpats(:, k_idx);

    % calculate the maximum likelihood estimators (mean and variance)
    [ mu_hat, sigma_hat] = normfit(data');

    scratch.mu(:,k) = mu_hat;
    scratch.sigma(:,k) = sigma_hat;

end

% // calculate the priors based on occurence in the training set
scratch.prior = NaN(nConds, 1);
if (args.uniform_prior)
  scratch.prior = ones(nConds,1) / nConds;
else

  for k = 1:nConds  
    scratch.prior(k) = (1 + numel( find(traintargs(k, :) == 1))) / ...
        (nConds + nTimepoints);    
  end

end
</code></pre>
","2015-01-06 01:04:40","27790969","4280372","1595","3","27790969","<p>The ""prior"" is the ""prior distribution"", which is the distribution describing the likelihood of each class.  This is relevant when it comes time to look at a new data point and, based on your training data, to decide which class it is.  If you know a priori that one class is more likely to occur than another class, it will affect the decision on the class to which the new point belongs.</p>

<p>A common assumption for the prior distribution  is a ""uniform prior"" which means that, when you go to test a new data point, we assume that each class is as like as likely to occur as any other class.  A uniform prior is a good assumption, but may not model the data very well.  </p>

<p>A better model would be to assume that your training data is a good representation of all data.  You then measure the distribution of each class in your training data.  This becomes your prior.</p>

<p>So, back to your example code, your question is about the section of code that defines the prior.  This section of code is described in the block comments at the top of your code.  See the section that reads:</p>

<pre><code>% UNIFORM_PRIOR (default = true): If uniform_prior is true,
% then the algorithm will assume that no classes are
% inherently more likely than others, and will use 1/K as
% the prior probability for each of K classes.  If
% uniform_prior is false, then train_gnb will estimate the
% priors from the data using laplace smoothing: if N_k is
% the number of times class k is observed in the training
% set and N is the total number of training datapoints, then
% Pr(Y == k) = (N_k + 1) / (N + K).  This way, no cluster is
% ever assigned a 0 prior.
</code></pre>

<p>In the code itself, you see the initial <code>if (args.uniform_prior)</code> which determines whether you're assuming the uniform prior or not....</p>

<p>If you <em>are</em> assuming a uniform prior, then the line <code>scratch.prior = ones(nConds,1) / nConds;</code> sets the prior to all the same value...ie, a uniform distribution.  Apparently the number of classes is defined by <code>nConds</code> so that the likelihood of a new data point being in any one class is basically <code>1 / nConds</code>.</p>

<p>If you are <em>not</em> assuming a uniform prior, the <code>for</code> loop goes through your training data and counts the number of occurrences of each class...via the portion of the line <code>numel( find(traintargs(k, :) == 1))</code>.  The rest of this line of code normalizes and smooths this value, using (I guess) the laplace smoothing technique discussed in the block comment at the top.</p>

<p>I hope that this helps!</p>

<p>Chip</p>
"
"3423045","968","<opencv><machine-learning><svm><libsvm>","27792564","1","How Feature length depend on prediction in SVM classifier","<p>Currently I am doing English alphabet classification using SVM classifier in opencv.
I have following doubts in doing above thing</p>

<ol>
<li><p>How length of feature vector depends on the classification ?
(What will happen if feature length increases (my current feature length is 125))</p></li>
<li><p>Is time taken for prediction depend on number of data used for training ? </p></li>
<li><p>Why we need normalization of feature vector (will this improve accuracy of prediction and time required for the prediction of the class) ?</p></li>
<li><p>How to determine best method for normalizing feature vector ?</p></li>
</ol>
","2015-01-06 05:25:24","27793215","4370183","371","2","27793215","<p>1) Length of features does not matter per se, what matters is predictive quality of features</p>

<p>2) No, it does not depend on number of samples, but it depends on number of features (prediction is generally very fast)</p>

<p>3) Normalization is required if features are in very different ranges of values</p>

<p>4) There are basically standarization (mean, stdev) and scaling (xmax -> +1, xmean -> -1 or 0) - you could do both and see which one is better</p>
"
"3515225","453","<opencv><image-processing><machine-learning><pattern-matching>","27793348","10","How to define our own kernel for Pattern recognition in OPENCV?","<p>I wanted to write my own kernel for Image classification on OpenCV.</p>

<p>But for SVM (Built in function for Opencv) the kernel is already defined.</p>

<p>My question is, is there anything in OpenCV that would allow me to define my kernel?</p>

<p>Actually,I wanted to implement multiple kernel learning for Image classification.</p>
","2015-01-06 06:35:43","51817940","1698143","1886","2","51817940","<p>I have looked around an answer and one of the workaround (mentioned in <a href=""https://stackoverflow.com/questions/9752402/build-a-custom-svm-kernel-matrix-with-opencv"">linked question</a> as well) is to use altenative SVM libraries like LibSVM, etc. (and LibSVM is really good one).</p>

<p>Though, if you want to stay in OpenCV only (and thats what appears by your question), then there is a <a href=""http://answers.opencv.org/question/67129/loadsave-svm-custom-kernel/"" rel=""nofollow noreferrer"">similar question posted on OpenCV forums</a> and somehow workaround is (copying verbatim):</p>

<blockquote>
  <p>This message means that SVM with custom kernel does not support loading from file. You can try following things:</p>
  
  <ol>
  <li><p>Use one of the standard kernels (obviously)</p></li>
  <li><p>Implement your kernel as standard and optionally contribute it to the mainline</p></li>
  <li><p>Set kernel to standard before saving to file and back to custom after loading from file (workaround)</p></li>
  <li><p>Implement mechanism for saving/loading custom kernels with parameters (can be hard)</p></li>
  </ol>
</blockquote>
"
"4202221","453","<python-3.x><numpy><machine-learning><classification><knn>","27798767","0","Load data from file and normalize","<p>How to normalize data loaded from file? Here what I have. Data looks kind of like this:</p>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
</code></pre>

<p>Last value in each line is a target. I want to have the same structure of the data but with normalized values.</p>

<pre><code>import numpy as np
dataset = np.loadtxt('infrared_data.txt', delimiter=',')

# select first 5 columns as the data
X = dataset[:, 0:5]

# is that correct? Should I normalize along 0 axis?
normalized_X = preprocessing.normalize(X, axis=0)

y = dataset[:, 5]
</code></pre>

<p>Now the question is, how to pack correctly <code>normalized_X</code> and <code>y</code> back, that it has the structure:</p>

<pre><code>dataset = [[normalized_X[0], y[0]],[normalized_X[1], y[1]],...]
</code></pre>
","2015-01-06 12:31:58","27802886","325565","236567","1","27802886","<p>It sounds like you're asking for <a href=""http://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html"" rel=""nofollow""><code>np.column_stack</code></a>.  For example, let's set up some dummy data:</p>

<pre><code>import numpy as np
x = np.arange(25).reshape(5, 5)
y = np.arange(5) + 1000
</code></pre>

<p>Which gives us:</p>

<pre><code>X:
array([[ 0,  1,  2,  3,  4],
       [ 5,  6,  7,  8,  9],
       [10, 11, 12, 13, 14],
       [15, 16, 17, 18, 19],
       [20, 21, 22, 23, 24]])
Y:
array([1000, 1001, 1002, 1003, 1004])
</code></pre>

<p>And we want:</p>

<pre><code>new = np.column_stack([x, y])
</code></pre>

<p>Which gives us:</p>

<pre><code>New:
array([[   0,    1,    2,    3,    4, 1000],
       [   5,    6,    7,    8,    9, 1001],
       [  10,   11,   12,   13,   14, 1002],
       [  15,   16,   17,   18,   19, 1003],
       [  20,   21,   22,   23,   24, 1004]])
</code></pre>

<p>If you'd prefer less typing, you can also use:</p>

<pre><code>In [4]: np.c_[x, y]
Out[4]:
array([[   0,    1,    2,    3,    4, 1000],
       [   5,    6,    7,    8,    9, 1001],
       [  10,   11,   12,   13,   14, 1002],
       [  15,   16,   17,   18,   19, 1003],
       [  20,   21,   22,   23,   24, 1004]])
</code></pre>

<p>However, I'd discourage using <code>np.c_</code> for anything other than interactive use, simply due to readability concerns.</p>
"
"2130541","730","<python-2.7><machine-learning><parameter-passing><scikit-learn><cross-validation>","27810855","19","(Python - sklearn) How to pass parameters to the customize ModelTransformer class by gridsearchcv","<p>Below is my pipeline and it seems that I can't pass the parameters to my models by using the ModelTransformer class, which I take it from the link (<a href=""http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"">http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html</a>)</p>

<p>The error message makes sense to me, but I don't know how to fix this. Any idea how to fix this? Thanks.</p>

<pre><code># define a pipeline
pipeline = Pipeline([
('vect', DictVectorizer(sparse=False)),
('scale', preprocessing.MinMaxScaler()),
('ess', FeatureUnion(n_jobs=-1, 
                     transformer_list=[
     ('rfc', ModelTransformer(RandomForestClassifier(n_jobs=-1, random_state=1,  n_estimators=100))),
     ('svc', ModelTransformer(SVC(random_state=1))),],
                     transformer_weights=None)),
('es', EnsembleClassifier1()),
])

# define the parameters for the pipeline
parameters = {
'ess__rfc__n_estimators': (100, 200),
}

# ModelTransformer class. It takes it from the link
(http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html)
class ModelTransformer(TransformerMixin):
    def __init__(self, model):
        self.model = model
    def fit(self, *args, **kwargs):
        self.model.fit(*args, **kwargs)
        return self
    def transform(self, X, **transform_params):
        return DataFrame(self.model.predict(X))

grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, refit=True)
</code></pre>

<p>Error Message:
ValueError: Invalid parameter n_estimators for estimator ModelTransformer.</p>
","2015-01-07 03:08:46","27817446","1190430","5156","21","27817446","<p><code>GridSearchCV</code> has a special naming convention for nested objects. In your case <code>ess__rfc__n_estimators</code> stands for <code>ess.rfc.n_estimators</code>, and, according to the definition of the <code>pipeline</code>, it points to the property <code>n_estimators</code> of </p>

<pre><code>ModelTransformer(RandomForestClassifier(n_jobs=-1, random_state=1,  n_estimators=100)))
</code></pre>

<p>Obviously, <code>ModelTransformer</code> instances don't have such property.</p>

<p>The fix is easy: in order to access underlying object of <code>ModelTransformer</code> one needs to use <code>model</code> field. So, grid parameters become</p>

<pre><code>parameters = {
  'ess__rfc__model__n_estimators': (100, 200),
}
</code></pre>

<p><strong>P.S.</strong> it's not the only problem with your code. In order to use multiple jobs in GridSearchCV, you need to make all objects you're using copy-able. This is achieved by implementing methods <code>get_params</code> and <code>set_params</code>, you can borrow them from <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#sklearn.base.BaseEstimator"" rel=""noreferrer""><code>BaseEstimator</code></a> mixin.</p>
"
"4202221","453","<python><machine-learning><scipy><neural-network>","27815580","2","Use of scipy.optimize.minimize in Neural Network","<p>Trying to use Backpropagation Neural Network for multiclass classification. I have found <a href=""http://danielfrg.com/blog/2013/07/03/basic-neural-network-python/#disqus_thread"" rel=""nofollow"">this code</a> and try to adapt it. It is based on the lections of <a href=""http://sun.stanford.edu/~couvidat/MachineLearning/ex4.pdf"" rel=""nofollow"">Machine Learning in Coursera from Andrew Ng</a>. </p>

<p>I don't understand exactly the implementation of <code>scipy.optimize.minimize</code> function here. It is used just once in the code. Is it iteratively updating the weights of the network? How can I visualize  (plot) it's performance to see when it converges?</p>

<p>Using this function what parameters I can adjust to achieve better performance? I found <a href=""http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Neural_Network_Basics"" rel=""nofollow"">here</a> a list common parameters:</p>

<ul>
<li>Number of neurons in the hidden layer: this is <code>hidden_layer_size=25</code> in my code</li>
<li><strong>Learning rate: can I still adjust that using built-in minimization function?</strong></li>
<li><strong>Momentum:</strong> is that <code>reg_lambda=0</code> in my case? Regularization parameter to avoid overfitting, right?</li>
<li>Epoch: <code>maxiter=500</code></li>
</ul>

<p>Here is my training data (target class is in the last column):</p>

<hr>

<pre><code>65535, 3670, 65535, 3885, -0.73, 1
65535, 3962, 65535, 3556, -0.72, 1
65535, 3573, 65535, 3529, -0.61, 1
3758, 3123, 4117, 3173, -0.21, 0
3906, 3119, 4288, 3135, -0.28, 0
3750, 3073, 4080, 3212, -0.26, 0
65535, 3458, 65535, 3330, -0.85, 2
65535, 3315, 65535, 3306, -0.87, 2
65535, 3950, 65535, 3613, -0.84, 2
65535, 32576, 65535, 19613, -0.35, 3
65535, 16657, 65535, 16618, -0.37, 3
65535, 16657, 65535, 16618, -0.32, 3
</code></pre>

<p>The dependencies are so obvious, I think it should be so easy to classify it...</p>

<p>But results are terrible. I get accuracy of 0.6 to 0.8. This is absolutely inappropriate for my application. I know I need more data normally, but I would be already happy when I could at least fit the training data (without taking into account potential overfitting)</p>

<p>Here is the code:</p>

<pre><code>import numpy as np
from scipy import optimize

from sklearn import cross_validation
from sklearn.metrics import accuracy_score
import math

class NN_1HL(object):

    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):
        self.reg_lambda = reg_lambda
        self.epsilon_init = epsilon_init
        self.hidden_layer_size = hidden_layer_size
        self.activation_func = self.sigmoid
        self.activation_func_prime = self.sigmoid_prime
        self.method = opti_method
        self.maxiter = maxiter

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def sigmoid_prime(self, z):
        sig = self.sigmoid(z)
        return sig * (1 - sig)

    def sumsqr(self, a):
        return np.sum(a ** 2)

    def rand_init(self, l_in, l_out):
        self.epsilon_init = (math.sqrt(6))/(math.sqrt(l_in + l_out))
        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init

    def pack_thetas(self, t1, t2):
        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))

    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):
        t1_start = 0
        t1_end = hidden_layer_size * (input_layer_size + 1)
        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))
        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))
        return t1, t2

    def _forward(self, X, t1, t2):
        m = X.shape[0]
        ones = None
        if len(X.shape) == 1:
            ones = np.array(1).reshape(1,)
        else:
            ones = np.ones(m).reshape(m,1)

        # Input layer
        a1 = np.hstack((ones, X))

        # Hidden Layer
        z2 = np.dot(t1, a1.T)
        a2 = self.activation_func(z2)
        a2 = np.hstack((ones, a2.T))

        # Output layer
        z3 = np.dot(t2, a2.T)
        a3 = self.activation_func(z3)
        return a1, z2, a2, z3, a3

    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        Y = np.eye(num_labels)[y]

        _, _, _, _, h = self._forward(X, t1, t2)
        costPositive = -Y * np.log(h).T
        costNegative = (1 - Y) * np.log(1 - h).T
        cost = costPositive - costNegative
        J = np.sum(cost) / m

        if reg_lambda != 0:
            t1f = t1[:, 1:]
            t2f = t2[:, 1:]
            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))
            J = J + reg
        return J

    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):
        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)

        m = X.shape[0]
        t1f = t1[:, 1:]
        t2f = t2[:, 1:]
        Y = np.eye(num_labels)[y]

        Delta1, Delta2 = 0, 0
        for i, row in enumerate(X):
            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)

            # Backprop
            d3 = a3 - Y[i, :].T
            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)

            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])
            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])

        Theta1_grad = (1 / m) * Delta1
        Theta2_grad = (1 / m) * Delta2

        if reg_lambda != 0:
            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f
            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f

        return self.pack_thetas(Theta1_grad, Theta2_grad)

    def fit(self, X, y):
        num_features = X.shape[0]
        input_layer_size = X.shape[1]
        num_labels = len(set(y))

        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)
        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)
        thetas0 = self.pack_thetas(theta1_0, theta2_0)

        options = {'maxiter': self.maxiter}
        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, 
                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)

        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)

        np.savetxt(""weights_t1.txt"", self.t1, newline=""\n"")
        np.savetxt(""weights_t2.txt"", self.t2, newline=""\n"")

    def predict(self, X):
        return self.predict_proba(X).argmax(0)

    def predict_proba(self, X):
        _, _, _, _, h = self._forward(X, self.t1, self.t2)
        return h


##################
# IR data        #
##################
values = np.loadtxt('infrared_data.txt', delimiter=', ', usecols=[0,1,2,3,4])

targets = np.loadtxt('infrared_data.txt', delimiter=', ', dtype=(int), usecols=[5])

X_train, X_test, y_train, y_test = cross_validation.train_test_split(values, targets, test_size=0.4)
nn = NN_1HL()
nn.fit(values, targets)
print(""Accuracy of classification: ""+str(accuracy_score(y_test, nn.predict(X_test))))
</code></pre>
","2015-01-07 08:52:28","27818014","1190430","5156","1","27818014","<p>In the given code <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize"" rel=""nofollow""><code>scipy.optimize.minimize</code></a> iteratively minimizes function given it's derivative (Jacobi's matrix). According to the documentation, use can specify <code>callback</code> argument to a function that will be called after each iteration — this will let you measure performance, though I'm not sure if it'll let you halt the optimization process.</p>

<p>All parameters you listed are hyperparameters, it's hard to optimize them directly:</p>

<p><em>Number of neurons in the hidden layer</em> is a discrete valued parameters, and, thus, is not optimizable via gradient techniques. Moreover, it affects NeuralNet architecture, so you can't optimize it while training the net. What you can do, though, is to use some higher-level routine to search for possible options, like exhaustive grid search with cross-validation (for example look at <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn.grid_search.GridSearchCV"" rel=""nofollow"">GridSearchCV</a>) or other tools for hyperparameter search (<a href=""https://github.com/hyperopt/hyperopt"" rel=""nofollow"">hyperopt</a>, <a href=""https://github.com/JasperSnoek/spearmint"" rel=""nofollow"">spearmint</a>, <a href=""https://github.com/Yelp/MOE"" rel=""nofollow"">MOE</a>, etc).</p>

<p><em>Learning rate</em> does not seem to be customizable for most of the optimization methods available. But, actually, learning rate in gradient descent is just a Newton's method with Hessian ""approximated"" by <code>1 / eta I</code> — diagonal matrix with inverted learning rates on the major diagonal. So you can try hessian-based methods with this heuristic.</p>

<p><em>Momentum</em> is completely unrelated to regularization. It's an optimization technique, and, since you use scipy for optimization, is unavailable for you.</p>
"
"4202221","453","<machine-learning><artificial-intelligence><normalization>","27819923","4","Do I need to normalize targets for Neural Network?","<p>I use backpropagation neural network for multiclass classification.</p>

<p>My data looks like this</p>

<pre><code>65535, 8710, 55641, 5396, 23.6056640625
65535, 8600, 65535, 5305, 10.0318359375
64539, 8664, 65535, 5305, 11.0232421875 
65535, 8674, 65535, 5257, 21.962109375
32018, 8661, 65535, 5313, 2.8986328125
35569, 8665, 65535, 5289, 2.8494140624999997
23652, 8656, 65535, 5260, 22.4806640625
42031, 8551, 65535, 5239, 2.7298828125
65535, 8573, 65535, 5232, 10.3728515625
</code></pre>

<p>Before I feed it to the network I scale the data to be in the range [0,1]</p>

<p>And the targets are:</p>

<pre><code>[0, 1, 1, 0, 2, 2, 0, 2, 1]
</code></pre>

<p>Do I need to normalize targets to be in the range [0,1]?</p>
","2015-01-07 12:50:36","27820348","1190430","5156","4","27820348","<p>Normalizing targets makes sense only in <a href=""http://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a> problems when your Network is asked to predict a (possibly, vector of) real value(s).</p>

<p>In your case targets are too ""round"" and, apparently, class indicators. Thus, solving regression problem would be incorrect and you need to go with <a href=""http://en.wikipedia.org/wiki/Statistical_classification"" rel=""nofollow"">classification</a> instead. Normalizing targets would be a total disaster in that case: you'd make targets incomparable (since limitations of computer floating arithmetic do not allow us to compare floats for equality) and will not simplify NN's (or any other ML algorthm's) work because numerical values of these classes aren't used at all.</p>
"
"4154133","327","<machine-learning><chess>","27824546","2","Machine Learning applied to chess tutoring software","<p>This may belong in the chess SE community but I am looking at the question from a programming perspective rather than pedagogical or even chess perspective.</p>

<p>I know of several studies and attempts to create chess engines which use some variant of machine learning to <strong>play</strong> chess (most of which are usually studies in the subject, rather than attempts to trump the brute force method, which is so far superior to other methods), but few attempts to apply machine learning to chess pedagogy. </p>

<p>One of the main reasons for a chess coach/tutor is the personalized attention and direction that the tutor provides. Is it then possible to create a chess program which uses machine learning to generate personalized ""lessons"" for the user based on their strengths and weaknesses?</p>

<p>The lessons need not be complex, even generating relevant positions from a database and asking the user to ""solve"" them, then giving a line or variation in response to an answer (correct or incorrect) is a great deal of instruction (for, even without explanations, the variations can often suffice)</p>

<p>The main questions are:</p>

<ol>
<li>How would the software be able to guage the user's skill level? (This is really where the ML algorithm would have to come in)</li>
<li>How can the software determine the difficulty or ""appropriateness"" of a test position? e.g suppose the software determines that the user has difficulty with tactical positions, (an issue most amateur players have) how can the program choose a position (from its database of games, lets suppose) with approriate tactical difficulty?</li>
<li>Finally, how will the software percieve and adapt to improvement by the user?</li>
</ol>

<p>I apologize if this question is to abstract or theoritical for SO, if so, I will move it elsewhere.</p>

<p>Thanks </p>
","2015-01-07 16:57:49","","1832418","2140","1","27824747","<p>I would start by having the chess program dump out statistics to a CSV or JSON file showing:</p>

<blockquote>
  <ol>
  <li>Which pieces were moved and how often</li>
  <li>How many moves it took until a checkmate took place</li>
  <li>How many games over what period of time.</li>
  <li>How many pieces captured over time. etc</li>
  </ol>
</blockquote>

<p>You have a tremendous amount of flexibility over picking you data points of interest or features that will be used to train your ML algorithm. Once you have these data points together and a data file that can be made available to your algorithm, you can begin to train it and see what predictive results you get. Then will need to tweak your experiment until you get results that are indeed useful. </p>

<p>Here is a Python based Random Forrest algorithm along with <a href=""https://www.kaggle.com/c/digit-recognizer/forums/t/2299/getting-started-python-sample-code-random-forest"" rel=""nofollow"">a tutorial</a> to get you started:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier
from numpy import genfromtxt, savetxt

def main():
    #create the training &amp; test sets, skipping the header row with [1:]
    dataset = genfromtxt(open('Data/train.csv','r'), delimiter=',', dtype='f8')[1:]    
    target = [x[0] for x in dataset]
    train = [x[1:] for x in dataset]
    test = genfromtxt(open('Data/test.csv','r'), delimiter=',', dtype='f8')[1:]

    #create and train the random forest
    #multi-core CPUs can use: rf = RandomForestClassifier(n_estimators=100, n_jobs=2)
    rf = RandomForestClassifier(n_estimators=100)
    rf.fit(train, target)

    savetxt('Data/submission2.csv', rf.predict(test), delimiter=',', fmt='%f')

if __name__==""__main__""
</code></pre>

<p>This could be quite interesting when you get it going. Getting the program to begin to anticipate weaknesses in a player would depend on what data you choose to collect. </p>

<p>Good luck.</p>
"
"1115169","2518","<python><pandas><machine-learning><missing-data>","27824954","8","How to handle missing NaNs for machine learning in python","<p><strong>How to handle missing values in datasets before applying machine learning algorithm??.</strong> </p>

<p>I noticed that it is not a smart thing to drop missing NAN values. I usually do interpolate (compute mean) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do.</p>

<p>Here is a very important question. <strong>What is the best way to handle missing values in data set?</strong></p>

<p>For example if you see this dataset, only 30% has original data.</p>

<pre><code>Int64Index: 7049 entries, 0 to 7048
Data columns (total 31 columns):
left_eye_center_x            7039 non-null float64
left_eye_center_y            7039 non-null float64
right_eye_center_x           7036 non-null float64
right_eye_center_y           7036 non-null float64
left_eye_inner_corner_x      2271 non-null float64
left_eye_inner_corner_y      2271 non-null float64
left_eye_outer_corner_x      2267 non-null float64
left_eye_outer_corner_y      2267 non-null float64
right_eye_inner_corner_x     2268 non-null float64
right_eye_inner_corner_y     2268 non-null float64
right_eye_outer_corner_x     2268 non-null float64
right_eye_outer_corner_y     2268 non-null float64
left_eyebrow_inner_end_x     2270 non-null float64
left_eyebrow_inner_end_y     2270 non-null float64
left_eyebrow_outer_end_x     2225 non-null float64
left_eyebrow_outer_end_y     2225 non-null float64
right_eyebrow_inner_end_x    2270 non-null float64
right_eyebrow_inner_end_y    2270 non-null float64
right_eyebrow_outer_end_x    2236 non-null float64
right_eyebrow_outer_end_y    2236 non-null float64
nose_tip_x                   7049 non-null float64
nose_tip_y                   7049 non-null float64
mouth_left_corner_x          2269 non-null float64
mouth_left_corner_y          2269 non-null float64
mouth_right_corner_x         2270 non-null float64
mouth_right_corner_y         2270 non-null float64
mouth_center_top_lip_x       2275 non-null float64
mouth_center_top_lip_y       2275 non-null float64
mouth_center_bottom_lip_x    7016 non-null float64
mouth_center_bottom_lip_y    7016 non-null float64
Image                        7049 non-null object
</code></pre>
","2015-01-07 17:19:27","27825523","2547739","5520","12","27825523","<pre><code>What is the best way to handle missing values in data set?
</code></pre>

<p>There is NO best way, each solution/algorithm has their own pros and cons (and you can even mix some of them together to create your own strategy and tune the related parameters to come up one best satisfy your data, there are many research/papers about this topic).  </p>

<p>For example, <strong>Mean Imputation</strong> is quick and simple, but it would underestimate the variance and the distribution shape is distorted by replacing NaN with the mean value, while <strong>KNN Imputation</strong> might not be ideal in a large data set in terms of time complexity, since it iterate over all the data points and perform calculation for each NaN value, and the assumption is that NaN attribute is correlated with other attributes.   </p>

<pre><code>How to handle missing values in datasets before applying machine learning algorithm??
</code></pre>

<p>In addition to <em>mean imputation</em> you mention, you could also take a look at <em>K-Nearest Neighbor Imputation</em> and <em>Regression Imputation</em>, and refer to the powerful <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html"" rel=""noreferrer"">Imputer</a> class in <a href=""http://scikit-learn.org/stable/"" rel=""noreferrer"">scikit-learn</a> to check existing APIs to use.</p>

<p><strong>KNN Imputation</strong></p>

<p>Calculate the mean of k nearest neighbors of this NaN point.</p>

<p><strong>Regression Imputation</strong></p>

<p>A regression model is estimated to predict observed values of a variable based on other variables, and that model is then used to impute values in cases where that variable is missing.</p>

<p><a href=""http://scikit-learn.org/stable/modules/preprocessing.html#imputation-of-missing-values"" rel=""noreferrer"">Here</a> links to scikit's '<em>Imputation of missing values'</em> section. 
I have also heard of <a href=""http://orange.biolab.si/docs/latest/reference/rst/Orange.feature.imputation.html"" rel=""noreferrer"">Orange</a> library for imputation, but haven't had a chance to use it yet. </p>
"
"1115169","2518","<python><pandas><machine-learning><missing-data>","27824954","8","How to handle missing NaNs for machine learning in python","<p><strong>How to handle missing values in datasets before applying machine learning algorithm??.</strong> </p>

<p>I noticed that it is not a smart thing to drop missing NAN values. I usually do interpolate (compute mean) using pandas and fill it up the data which is kind of works and improves the classification accuracy but may not be the best thing to do.</p>

<p>Here is a very important question. <strong>What is the best way to handle missing values in data set?</strong></p>

<p>For example if you see this dataset, only 30% has original data.</p>

<pre><code>Int64Index: 7049 entries, 0 to 7048
Data columns (total 31 columns):
left_eye_center_x            7039 non-null float64
left_eye_center_y            7039 non-null float64
right_eye_center_x           7036 non-null float64
right_eye_center_y           7036 non-null float64
left_eye_inner_corner_x      2271 non-null float64
left_eye_inner_corner_y      2271 non-null float64
left_eye_outer_corner_x      2267 non-null float64
left_eye_outer_corner_y      2267 non-null float64
right_eye_inner_corner_x     2268 non-null float64
right_eye_inner_corner_y     2268 non-null float64
right_eye_outer_corner_x     2268 non-null float64
right_eye_outer_corner_y     2268 non-null float64
left_eyebrow_inner_end_x     2270 non-null float64
left_eyebrow_inner_end_y     2270 non-null float64
left_eyebrow_outer_end_x     2225 non-null float64
left_eyebrow_outer_end_y     2225 non-null float64
right_eyebrow_inner_end_x    2270 non-null float64
right_eyebrow_inner_end_y    2270 non-null float64
right_eyebrow_outer_end_x    2236 non-null float64
right_eyebrow_outer_end_y    2236 non-null float64
nose_tip_x                   7049 non-null float64
nose_tip_y                   7049 non-null float64
mouth_left_corner_x          2269 non-null float64
mouth_left_corner_y          2269 non-null float64
mouth_right_corner_x         2270 non-null float64
mouth_right_corner_y         2270 non-null float64
mouth_center_top_lip_x       2275 non-null float64
mouth_center_top_lip_y       2275 non-null float64
mouth_center_bottom_lip_x    7016 non-null float64
mouth_center_bottom_lip_y    7016 non-null float64
Image                        7049 non-null object
</code></pre>
","2015-01-07 17:19:27","27825523","481326","410","2","35049578","<p>There's no single best way to deal with missing data. The most rigorous approach is to model the missing values as additional parameters in a probabilistic framework like PyMC. This way you'll get a distribution over possible values, instead of just a single answer. Here's an example of dealing with missing data using PyMC: <a href=""http://stronginference.com/missing-data-imputation.html"" rel=""nofollow"">http://stronginference.com/missing-data-imputation.html</a></p>

<p>If you really want to plug those holes with point estimates, then you're looking to perform ""imputation"". I'd steer away from simple imputation methods like mean-filling since they really butcher the joint distribution of your features. Instead, try something like <a href=""https://web.stanford.edu/~hastie/swData/softImpute/vignette.html"" rel=""nofollow"">softImpute</a> (which tries you infer the missing value via low-rank approximation). The original version of softImpute is written for R but I've made a Python version (along with other methods like kNN imputation) here: <a href=""https://github.com/hammerlab/fancyimpute"" rel=""nofollow"">https://github.com/hammerlab/fancyimpute</a></p>
"
"123991","1134","<machine-learning><simplecv><orange>","27830657","0","Hough Transform SimpleCV Feature Extractor","<p>I'm trying to build a new SimpleCV FeatureExtractor for openCV's Hough Circle Transform but I'm running into an error during my machine learning script's training phase.</p>

<p>I've provided the error below. It is raised by the Orange machine learning library when creating the <code>self.mDataSetOrange</code> variable within SimpleCV's TreeClassifier.py. The size of the dataset does not match Orange's expectation for some reason. I looked into Orange's source code and the found that error is thrown <a href=""https://github.com/biolab/orange/blob/5eaa02a1ae167c7563b10d78ff15539886af6789/source/orange/cls_example.cpp#L43"" rel=""nofollow"">here</a>:</p>

<p>orange/source/orange/cls_example.cpp</p>

<pre><code>int const nvars = dom-&gt;variables-&gt;size() + dom-&gt;classVars-&gt;size();
if (Py_ssize_t(nvars) != PyList_Size(lst)) {
    PyErr_Format(PyExc_IndexError, ""invalid list size (got %i, expected %i items)"",
        PyList_Size(lst), nvars);
    return false;
}
</code></pre>

<p>Obviously, my feature extractor is not extracting the things as required by Orange but I can't pinpoint what the problem could be. I'm pretty new to SimpleCV and Orange so I'd be grateful if someone could point out any mistakes I'm making.</p>

<p>The error:</p>

<pre><code>Traceback (most recent call last):
  File ""MyClassifier.py"", line 113, in &lt;module&gt;
    MyClassifier.run(MyClassifier.TRAIN_RUN_TYPE, trainingPaths)
  File ""MyClassifier.py"", line 39, in run
    self.decisionTree.train(imgPaths, MyClassifier.CLASSES, verbose=True)
  File ""/usr/local/lib/python2.7/dist-packages/SimpleCV-1.3-py2.7.egg/SimpleCV/MachineLearning/TreeClassifier.py"", line 282, in train
    self.mDataSetOrange = orange.ExampleTable(self.mOrangeDomain,self.mDataSetRaw)
IndexError: invalid list size (got 266, expected 263 items) (at example 2)
</code></pre>

<p>HoughTransformFeatureExtractor.py</p>

<pre><code>class HoughTransformFeatureExtractor(FeatureExtractorBase):

    def extract(self, img):
        bitmap = img.getBitmap()
        cvMat = cv.GetMat(bitmap)
        cvImage = numpy.asarray(cvMat)

        height, width = cvImage.shape[:2]
        gray = cv2.cvtColor(cvImage, cv2.COLOR_BGR2GRAY)

        circles = cv2.HoughCircles(gray, cv2.cv.CV_HOUGH_GRADIENT, 2.0, width / 2)
        self.featuresLen = 0

        if circles is not None:
            circleFeatures = circles.ravel().tolist()
            self.featuresLen = len(circleFeatures)

            return circleFeatures
        else:
            return None

    def getFieldNames(self):
        retVal = []
        for i in range(self.featuresLen):
            name = ""Hough""+str(i)
            retVal.append(name)
        return retVal

    def getNumFields(self):
        return self.featuresLen
</code></pre>
","2015-01-07 23:39:39","","123991","1134","0","27831442","<p>So, I figured out my issue. Basically, the problem was with the size of list returned by the <code>extract</code> method. The size of the list varied for each processed image, which is what led to this error. So, here are some examples of the type of lists returned by the <code>extract</code> method:</p>

<pre><code>3 -&gt; [74.0, 46.0, 14.866068840026855]
3 -&gt; [118.0, 20.0, 7.071067810058594]
6 -&gt; [68.0, 8.0, 8.5440034866333, 116.0, 76.0, 13.03840446472168]
3 -&gt; [72.0, 44.0, 8.602325439453125]
9 -&gt; [106.0, 48.0, 15.81138801574707, 20.0, 52.0, 23.409399032592773, 90.0, 122.0, 18.0]
</code></pre>

<p>Once I made sure that the size of the list was consistent, no matter the image, the error went away. Hopefully, this will help anyone having similar issues in the future.</p>
"
"2483127","11369","<matlab><machine-learning><data-mining><projection><pca>","27830783","1","How to find projection matrix for PCA in MATLAB?","<p>I'm trying to reduce the dimensionality of my data with PCA.</p>

<p>So I call <code>[COEFF, SCORE] = princomp(data);</code> According to <a href=""https://stackoverflow.com/a/12689269/2483127"">this answer</a>, I can reconstruct my data with <code>SCORE * COEFF' + Mean</code>, and it works.</p>

<p>But I'm trying to find the projection matrix <code>P</code>, where any given vector <code>x</code> can be transformed to its projection in PCA space.</p>

<p>My intuition tells me that I should be able to project <code>x</code> by : </p>

<pre><code>proj = ((x-m) * inv(C)) + m
</code></pre>

<p>where <code>m</code> is the mean of my data.</p>

<p>so I test this by choosing <code>x</code> as the first observation of my data, and I expect <code>proj</code> should be very close to the first row of <code>SCORE</code>. However this is not the case.</p>

<p>So where am I doing wrong? And how can I find the projection matrix?</p>

<p>Thanks for any help!</p>
","2015-01-07 23:53:05","27830910","2483127","11369","0","27830910","<p>Oops, now I see my mistake.</p>

<p>First of all, COEFF is orthogonal (not sure) so <code>inv(COEFF) == COEFF'</code></p>

<p>and the projection is found by</p>

<pre><code>proj = COEFF' * (x-m)
</code></pre>
"
"3782614","510","<c#><machine-learning>","27837396","-4","Can't understand this part of C# code","<p>This part of code has been taken from <a href=""http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20Introduction.aspx"" rel=""nofollow"">this tutorial</a> regarding using Learners in Infer.NET library. I'm planning to use it for machine learning but unfortunately i can't get this code fragment. Please help me out.</p>

<pre><code>/// &lt;summary&gt;
/// A mapping for the Bayes Point Machine classifier tutorial.
/// &lt;/summary&gt;
public class ClassifierMapping 
    : IClassifierMapping&lt;IList&lt;Vector&gt;, int, IList&lt;string&gt;, string, Vector&gt;
{
    public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
    {
        for (int instance = 0; instance &lt; featureVectors.Count; instance++)
        {
            yield return instance;
        }
    }

    public Vector GetFeatures(int instance, IList&lt;Vector&gt; featureVectors)
    {
        return featureVectors[instance];
    }

    public string GetLabel(
        int instance, IList&lt;Vector&gt; featureVectors, IList&lt;string&gt; labels)
    {
        return labels[instance];
    }

    public IEnumerable&lt;string&gt; GetClassLabels(
        IList&lt;Vector&gt; featureVectors = null, IList&lt;string&gt; labels = null)
    {
        return new[] { ""Female"", ""Male"" };
    }
}
</code></pre>

<p>Thank you in advance!</p>
","2015-01-08 10:04:00","27837774","360211","50853","2","27837774","<p>The only remotely complicated bit is the <code>yield return</code>:</p>

<pre><code>public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
{
    for (int instance = 0; instance &lt; featureVectors.Count; instance++)
    {
        yield return instance;
    }
}
</code></pre>

<p>Which could be replaced with:</p>

<pre><code>public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
{
    var result = new List&lt;int&gt;();
    for (int instance = 0; instance &lt; featureVectors.Count; instance++)
    {
       result.Add(instance);
    }
    return result;
}
</code></pre>
"
"3782614","510","<c#><machine-learning>","27837396","-4","Can't understand this part of C# code","<p>This part of code has been taken from <a href=""http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20Introduction.aspx"" rel=""nofollow"">this tutorial</a> regarding using Learners in Infer.NET library. I'm planning to use it for machine learning but unfortunately i can't get this code fragment. Please help me out.</p>

<pre><code>/// &lt;summary&gt;
/// A mapping for the Bayes Point Machine classifier tutorial.
/// &lt;/summary&gt;
public class ClassifierMapping 
    : IClassifierMapping&lt;IList&lt;Vector&gt;, int, IList&lt;string&gt;, string, Vector&gt;
{
    public IEnumerable&lt;int&gt; GetInstances(IList&lt;Vector&gt; featureVectors)
    {
        for (int instance = 0; instance &lt; featureVectors.Count; instance++)
        {
            yield return instance;
        }
    }

    public Vector GetFeatures(int instance, IList&lt;Vector&gt; featureVectors)
    {
        return featureVectors[instance];
    }

    public string GetLabel(
        int instance, IList&lt;Vector&gt; featureVectors, IList&lt;string&gt; labels)
    {
        return labels[instance];
    }

    public IEnumerable&lt;string&gt; GetClassLabels(
        IList&lt;Vector&gt; featureVectors = null, IList&lt;string&gt; labels = null)
    {
        return new[] { ""Female"", ""Male"" };
    }
}
</code></pre>

<p>Thank you in advance!</p>
","2015-01-08 10:04:00","27837774","3047225","309","0","27839779","<p>This is creating a Mapping class which is implementing <code>IClassifierMapping</code> interface. </p>

<pre><code>http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20API%20-%20Mappings%20-%20Standard%20Data%20Format%20Mapping.aspx
</code></pre>

<p>There are other mapping interfaces as well as can be seen here.</p>

<pre><code>http://research.microsoft.com/en-us/um/cambridge/projects/infernet/docs/Infer.NET%20Learners%20-%20Bayes%20Point%20Machine%20classifiers%20-%20API%20-%20Mappings.aspx
</code></pre>

<p>With this mapping class, you can now create a Bayes Point Machine classifier.</p>

<p>Regarding the complexity of the code, you can look at westons answer. </p>
"
"574187","3745","<machine-learning><scikit-learn><cross-validation>","27846542","2","cross validating a train set where the class variable has a different distribution than the actual population","<p>(noob in ML, be patient) 
I want to test the performance of my scikit-learn SVMLinear classifier. My train-set has a different class distribution than the actual population, but my test-set is a representative, and distributes like the actual population. </p>

<p>I noticed that there's a class-weight parameter, and I want to try giving my classifier the actual population distribution, and see if it helps it perform better.</p>

<p>However - as my train-set distribution is different, so will be my validation set, right? So should I expect an improvement on the validation, or must I use my test-set to see the improvement? And if so - isn't it against the rules to calibrate using the test-set which will lead to burning the test-set or overfitting?</p>

<p>I've thought about bootstrap re-sampling of my train-set: making it distribute the same as the general population, and only then training and validating my model. Is this a good solution?</p>

<p>Thanks!</p>
","2015-01-08 17:56:36","","3646384","557","1","27869099","<p>It seems that you have some good ideas which are mostly worth trying. The answers mostly depend on the application and the size of your train/test set. </p>

<p>It is against the rules to calibrate based on test set and again use the whole test set for evaluation. However, if your test set is large enough, you can always divide your test set to two sets: validation set, and actual test set. Then, your final evaluation will be based on a smaller test set, which might be still acceptable depending on the application. </p>

<p>For your training set that you believe it has different class distribution than the actual population, there might be several things worth trying. Usually the most acceptable approach is to use a classifier that can handle these differences (usually with fewer parameters to avoid over-fitting). There is a whole topic of classification and regression on skewed datasets that you can look through. Other than the classifier, provided that you did not derive the actual population from your test set, the methods below might help too:
1- One of them can be (as you said) bootstrap re-sampling in case that your training set is large enough for that. 
2- Another approach can be generating more training samples by adding some noise to the current samples of the training set. For example if you are classifying images of birds, you can randomly make images darker or brighter, or randomly move them a few pixels to the sides or up and down (select values randomly in a small enough range). This way, you can add to the training set in a way to get the desired distribution. </p>
"
"2419777","46","<java><c#><machine-learning><weka>","27859167","1","Combining multiple saved Classifier in weka","<p>Have huge distributed datasets which are trained to produce classifiers.All the datasets have identical attributes and the training is done using a single algorithm J48.
The problem I am facing is as to how would  combine these classifiers to have a single classifier which can be used for testing and predicting data.
I am using weka tool for the code.Have converted the weka jar to dll.Using C# language.
Any help in C# or Java would be of great help.
If any additional information is needed you are free to ask.
Thanks </p>
","2015-01-09 11:06:09","","968064","2353","0","28175002","<p>I don't think it is possible if you create N classifiers on N training sets and then combine N classifiers to generate a single one. Because first, the data are different; second, so the models will be different. Instead, what I would do is if I were happy with the N results, I would combine all N datasets and develop a single model from it to test and predict unseen data.</p>
"
"2419777","46","<java><c#><machine-learning><weka>","27859167","1","Combining multiple saved Classifier in weka","<p>Have huge distributed datasets which are trained to produce classifiers.All the datasets have identical attributes and the training is done using a single algorithm J48.
The problem I am facing is as to how would  combine these classifiers to have a single classifier which can be used for testing and predicting data.
I am using weka tool for the code.Have converted the weka jar to dll.Using C# language.
Any help in C# or Java would be of great help.
If any additional information is needed you are free to ask.
Thanks </p>
","2015-01-09 11:06:09","","3562232","483","1","32564591","<p>It is perfectly possible to do what you are asking for. You could build N different classifiers from N different but compatible datasets and combine their outputs to form a new dataset of higher order. Its a hierarchical way of combining classifiers and there is a great variety in ways of doing that. Its called 'ensembling' or 'classifier ensemble'. There are a large number of technical articles detailing how to do it.</p>

<p>One approach would be:
1. Train/get N different classifiers.
2. Build a new dataset with its probability output for a known set of instances, one instance per row, the set-of-output-probalities per set of columns. And the right/known class.
3. Throw away the old attributes and retain only the output probs calculated and known class.
4. Train a new model/classifier with this higher order dataset (don't need to use the whole data, only a moderate subsample).
5. For every new instance, get lower level probabilities (using N classifiers), as previously done, and apply higher level classifier over these newly constructed instance.</p>

<p>Hope to have helped.</p>
"
"1452759","6578","<python><machine-learning><scikit-learn>","27860302","2","In Sklearn machine learning, is there any way to classify text without target labels?","<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>

<p>For example:</p>

<pre><code>text = [[""request approval for access"", ""request approval to enter premises"", ""Laptop not working""], [""completed bw table loading""]]
</code></pre>

<p>So can I get categories like:</p>

<pre><code>category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""Laptop working""], [""bw table""]]
</code></pre>

<p>where </p>

<pre><code>      0 = approval request
      2 = laptop working
      1 = bw table
</code></pre>

<p>Basically the above would imply that there is no labelled training data or target labels. </p>
","2015-01-09 12:11:43","27867351","1330293","33641","1","27861026","<p>You can try a clustering method but there is no guarantee that the clusters you get will correspond to the categories you want, because you haven't clearly explained the algorithm what you want.</p>

<p>What I would do is manually label some data (how long can it take to label 300 samples?) and train on that, so that your algo can learn the words the are correlated with each class. </p>

<p>If this is impossible then your best bet is to calculate a cosine similarity between one sample and each class description, rank them, and then assign the closest class. But in my opinion by the time you finish to code this, you could have manually labeled some samples and trained a standard algo with a much better precision.</p>
"
"1452759","6578","<python><machine-learning><scikit-learn>","27860302","2","In Sklearn machine learning, is there any way to classify text without target labels?","<p>I was wondering if there was any way to classify text data into different groups/categories based on the words in the text using a combination of Python and Sklearn Machine Learning?</p>

<p>For example:</p>

<pre><code>text = [[""request approval for access"", ""request approval to enter premises"", ""Laptop not working""], [""completed bw table loading""]]
</code></pre>

<p>So can I get categories like:</p>

<pre><code>category_label = [[0,0,2], [1]]
categories = [[""approval request"", ""approval request"", ""Laptop working""], [""bw table""]]
</code></pre>

<p>where </p>

<pre><code>      0 = approval request
      2 = laptop working
      1 = bw table
</code></pre>

<p>Basically the above would imply that there is no labelled training data or target labels. </p>
","2015-01-09 12:11:43","27867351","1615070","800","0","27862639","<p>@user1452759</p>

<p>Your problem is more specific than general machine learning, and you should use package NLTK instead of sklearn. Look at classifying text with nltk <a href=""http://www.nltk.org/book/ch06.html"" rel=""nofollow"">http://www.nltk.org/book/ch06.html</a></p>
"
"2572645","4438","<machine-learning><nlp><word2vec>","27860652","97","word2vec: negative sampling (in layman term)?","<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
","2015-01-09 12:31:25","27864657","419338","13672","171","27864657","<p>The idea of <code>word2vec</code> is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. In equation (3) of the paper you link to, ignore the exponentiation for a moment. You have </p>

<pre><code>      v_c * v_w
 -------------------
   sum(v_c1 * v_w)
</code></pre>

<p>The numerator is basically the similarity between words <code>c</code> (the context) and <code>w</code> (the target) word. The denominator computes the similarity of all other contexts <code>c1</code> and the target word <code>w</code>. Maximising this ratio ensures words that appear closer together in text have more similar vectors than words that do not. However, computing this can be very slow, because there are many contexts <code>c1</code>. Negative sampling is one of the ways of addressing this problem- just select a couple of contexts <code>c1</code> at random. The end result is that if <code>cat</code> appears in the context of <code>food</code>, then the vector of <code>food</code> is more similar to the vector of <code>cat</code> (as measures by their dot product) than the vectors of <strong>several other randomly chosen words</strong> (e.g. <code>democracy</code>, <code>greed</code>, <code>Freddy</code>), instead of <strong>all other words in language</strong>. This makes <code>word2vec</code> much much faster to train.</p>
"
"2572645","4438","<machine-learning><nlp><word2vec>","27860652","97","word2vec: negative sampling (in layman term)?","<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
","2015-01-09 12:31:25","27864657","1462770","13601","41","41319421","<p>Computing <strong><em>Softmax</em></strong> (Function to determine which words are similar to the current target word) is expensive since requires summing over all words in <strong>V</strong> (denominator), which is generally very large.</p>

<p><a href=""https://i.stack.imgur.com/Akfej.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Akfej.png"" alt=""enter image description here""></a></p>

<p><em>What can be done?</em></p>

<p>Different strategies have been proposed to <strong>approximate</strong> the softmax. These approaches can be grouped into <strong>softmax-based</strong> and <strong>sampling-based</strong> approaches. <strong><em>Softmax-based</em></strong> approaches are methods that keep the softmax layer intact, but modify its architecture to improve its efficiency (e.g hierarchical softmax). <strong><em>Sampling-based</em></strong> approaches on the other hand completely do away with the softmax layer and instead optimise some other loss function that approximates the softmax (They do this by approximating the normalization in the denominator of the softmax with some other loss that is cheap to compute like negative sampling).</p>

<p>The loss function in Word2vec is something like:</p>

<p><a href=""https://i.stack.imgur.com/4s4f6.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/4s4f6.png"" alt=""enter image description here""></a></p>

<p>Which logarithm can decompose into:</p>

<p><a href=""https://i.stack.imgur.com/6RDai.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/6RDai.png"" alt=""enter image description here""></a></p>

<p>With some mathematic and gradient formula (See more details at <a href=""http://sebastianruder.com/word-embeddings-softmax/"" rel=""noreferrer"">6</a>) it converted to:</p>

<p><a href=""https://i.stack.imgur.com/fua4s.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/fua4s.png"" alt=""enter image description here""></a></p>

<p>As you see it converted to binary classification task (y=1 positive class, y=0 negative class). As we need labels to perform our binary classification task, we designate all context words <em>c</em> as true labels (y=1, positive sample), and <em>k</em> randomly selected from corpora as false labels (y=0, negative sample).</p>

<hr>

<p>Look at the following paragraph. Assume our target word is ""<strong>Word2vec</strong>"". With window of 3, our context words are: <code>The</code>, <code>widely</code>, <code>popular</code>, <code>algorithm</code>, <code>was</code>, <code>developed</code>. These context words consider as positive labels. We also need some negative labels. We randomly pick some words from corpus (<code>produce</code>, <code>software</code>, <code>Collobert</code>, <code>margin-based</code>, <code>probabilistic</code>) and consider them as negative samples. This technique that we picked some randomly example from corpus is called negative sampling.  </p>

<p><a href=""https://i.stack.imgur.com/nnnQX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/nnnQX.png"" alt=""enter image description here""></a></p>

<p><strong>Reference</strong> :</p>

<ul>
<li>(1) C. Dyer, <em>""Notes on Noise Contrastive Estimation and Negative Sampling""</em>, 2014</li>
<li>(2) <a href=""http://sebastianruder.com/word-embeddings-softmax/"" rel=""noreferrer"">http://sebastianruder.com/word-embeddings-softmax/</a></li>
</ul>
"
"2572645","4438","<machine-learning><nlp><word2vec>","27860652","97","word2vec: negative sampling (in layman term)?","<p>I'm reading the paper below and I have some trouble , understanding the concept of negative sampling.</p>

<p><a href=""http://arxiv.org/pdf/1402.3722v1.pdf"">http://arxiv.org/pdf/1402.3722v1.pdf</a></p>

<p>Can anyone help , please?</p>
","2015-01-09 12:31:25","27864657","7552761","1957","24","56401065","<p>I wrote an tutorial article about negative sampling <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling"" rel=""noreferrer"">here</a>.</p>

<p><strong>Why do we use negative sampling?</strong> -> to reduce computational cost</p>

<p>The cost function for vanilla Skip-Gram (SG) and Skip-Gram negative sampling (SGNS) looks like this:</p>

<p><a href=""https://i.stack.imgur.com/8bUdX.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/8bUdX.png"" alt=""enter image description here""></a></p>

<p>Note that <code>T</code> is the number of all vocabs. It is equivalent to <code>V</code>. In the other words, <code>T</code> = <code>V</code>.</p>

<p>The probability distribution <code>p(w_t+j|w_t)</code> in SG is computed for all <code>V</code> vocabs in the corpus with:</p>

<p><a href=""https://i.stack.imgur.com/3Lc8N.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/3Lc8N.png"" alt=""enter image description here""></a></p>

<p><code>V</code> can easily exceed tens of thousand when training Skip-Gram model. The probability needs to be computed <code>V</code> times, making it computationally expensive. Furthermore, the normalization factor in the denominator requires extra <code>V</code> computations. </p>

<p>On the other hand, the probability distribution in SGNS is computed with:</p>

<p><a href=""https://i.stack.imgur.com/Tl4ST.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Tl4ST.png"" alt=""enter image description here""></a></p>

<p><code>c_pos</code> is a word vector for positive word, and <code>W_neg</code> is word vectors for all <code>K</code> negative samples in the output weight matrix. With SGNS, the probability needs to be computed only <code>K + 1</code> times, where <code>K</code> is typically between 5 ~ 20. Furthermore, no extra iterations are necessary to compute the normalization factor in the denominator. </p>

<p>With SGNS, only a fraction of weights are updated for each training sample, whereas SG updates all millions of weights for each training sample.</p>

<p><a href=""https://i.stack.imgur.com/0bPat.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/0bPat.png"" alt=""enter image description here""></a></p>

<p><strong>How does SGNS achieve this?</strong> -> by transforming multi-classification task into binary classification task.</p>

<p>With SGNS, word vectors are no longer learned by predicting context words of a center word. It learns to differentiate the actual context words (positive) from randomly drawn words (negative) from the noise distribution.</p>

<p><a href=""https://i.stack.imgur.com/qbZaH.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/qbZaH.png"" alt=""enter image description here""></a></p>

<p>In real life, you don't usually observe <code>regression</code> with random words like <code>Gangnam-Style</code>, or <code>pimples</code>. The idea is that if the model can distinguish between the likely (positive) pairs vs unlikely (negative) pairs, good word vectors will be learned.</p>

<p><a href=""https://i.stack.imgur.com/7V7EO.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/7V7EO.png"" alt=""enter image description here""></a></p>

<p>In the above figure, current positive word-context pair is (<code>drilling</code>, <code>engineer</code>). <code>K=5</code> negative samples are <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#neg_drawn"" rel=""noreferrer"">randomly drawn</a> from the <a href=""https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#noise_dist"" rel=""noreferrer"">noise distribution</a>: <code>minimized</code>, <code>primary</code>, <code>concerns</code>, <code>led</code>, <code>page</code>. As the model iterates through the training samples, weights are optimized so that the probability for positive pair will output <code>p(D=1|w,c_pos)≈1</code>, and probability for negative pairs will output <code>p(D=1|w,c_neg)≈0</code>. </p>
"
"4437095","11","<machine-learning><neural-network><classification><svm><decision-tree>","27862165","1","online/incremental learning for classifiers","<p>I understand that in online/incremental learning it is possible that SVM or NN learn incrementally, as the new data becomes available over time. What if instead of new cases, just new features/variables for the existing cases become available over time. Is there any technique that can handle this kind of training for classifiers/predictions?  </p>
","2015-01-09 13:57:21","","4415339","103","1","27864304","<p>In the case of neural networks I would go with this method:</p>

<p>Take the already trained network. Add new input neurons for the new features. Optionally add new neurons to the hidden layer(s). Initialize the weights for the new connections with zeros or random values. Retrain the network.</p>

<p>It should probably be faster than training a new network from scratch.</p>
"
"3474956","7410","<python><machine-learning>","27852199","-1","Python Machine Learning Algorithm to Recognize Known Events","<p>I have two sets of data. These data are logged voltages of two points A and B in a circuit. Voltage A is the main component of the circuit, and B is a sub-circuit. Every positive voltage in B is (1) considered a B event and (2) known to be composite of A. I have included sample data where there is a B voltage event, <code>4,4,0,0,4,4</code>. A real training data set would have many more available data.</p>

<p>How can I train a Python machine learning algorithm to recognize B events given only A data? </p>

<p>Example data:</p>

<pre><code>V(A), V(B)
0, 0
2, 0
5, 4
3, 4
1, 0
3, 4
4, 4
1, 0
0, 0
2, 0
5, 0
7, 0
2, 0
5, 4
9, 4
3, 0
5, 0
4, 4
6, 4
3, 0
2, 0
</code></pre>
","2015-01-09 00:47:39","27852335","1330293","33641","1","27852335","<p>An idea:</p>

<pre><code>from sklearn.ensemble import RandomForestClassifier

n = 5
X = [df.A.iloc[i:i+n] for i in df.index[:-n+1]] 
labels = (df.B &gt; 0)[n-1:]

model = RandomForestClassifier()
model.fit(X, labels)
model.predict(X)
</code></pre>

<p>What this does is, it takes the previous <code>n</code> observations as predictors for the 'B' value. On this small data set it achieves 0.94 accuracy (could be overfitting).</p>

<p>EDIT: Corrected a small alignment error.</p>
"
"3073054","73","<python><numpy><machine-learning><scikit-learn>","27856705","2","Categorical data transformation in Scikit-Learn","<p>I have a 40 million x 22 numpy array of integer data for a classification task.
Most of the features are categorical data that use different integer values to represent different categories. For example, in the column ""Color"": 0 means blue, 1 means red and so on. I have preprocessed the data using LabelEncoder. </p>

<ol>
<li>Does it make sense to fit those data into any classification model in SK-learn? I tried to fit the data into Random Forest model but got extremely bad accuracy. I also tried One Hot Encoding to transform the data into dummy variables, but my computer can only deal with a sparse matrix after using One Hot Encoding, the problem is that Random Forest can only take a dense matrix, which will exceed my computer's memory. </li>
<li>What's the correct strategy to deal with categorical data in SK-learn?</li>
</ol>
","2015-01-09 08:44:23","27857428","1190430","5156","1","27857428","<p><code>LabelEncoder</code> is useless in your case, since output numbers do not make any sense as numbers (i.e. it's meaningless to perform arithmetic operations on them). <code>OneHotEncoder</code> is essential when dealing with categorical data.</p>

<p>Recently sklearn <a href=""https://github.com/scikit-learn/scikit-learn/pull/3173"" rel=""nofollow"">got support for sparse input</a> in Random Forests and Decision Trees, so you might want to check out the latest version. Also, other methods like <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"" rel=""nofollow"">LogisticRegression</a> support sparse data.</p>

<p>Moreover, I don't think you need to use all 40M of examples to get a decent accuracy. It should be enough to randomly sample, say, 100k of them (this number depends on number of features after OneHotEncoding, their variability, and number of target classes).</p>
"
"4309200","240","<matlab><audio><machine-learning><mp3><large-files>","27878190","0","Read and represent mp3 files using memmapfile in matlab","<p>I have to analyze bio acoustic audiofiles using matlab. Eventually I want to be able to find anomalies in the audio. That's the reason I need to find a way to represent the audio in a way I can extract and compare features.  I'm dealing with mp3 files up to 150 mb. These files are too large for matlab to read in to it's memory. Therefore I want to use the memmapfile() function. I used the following code and a small mp3 file to find out how it actually works.    </p>

<pre><code>[testR, ~] = audioread('test.mp3');
testM = memmapfile('test.mp3');
disp(testM.Data);
disp(testR);   
</code></pre>

<p>The actual values of the testM.Data and testR are different. Audioread() returns a 7483391 x 2 matrix and memmapfile() a 4113874 x 1 matrix.
I'm not really sure how memmapfile() works, I expected this to be equal to each other. Is there a way to read mp3 files in the same format audioread() does using memmapfile()? And what does memmapfile actually return in case of an audio file? Maybe it's also usable in the vector format in the case of anomaly detection?</p>

<p>Thanks in advance!</p>

<p>NOTE: The original files were in wav IMA ADPCM format with sizes from 1.5 up to 2.5 gb. Since Matlab can't deal with that format and the size of the files I converted them to 8bit mp3 files. </p>
","2015-01-10 16:04:11","","4312376","1","0","27882518","<p>I think that the problem is mammapfile by default read data in uint8 format, while audioread function read data in another way.
How you can see <a href=""http://uk.mathworks.com/help/matlab/ref/memmapfile.html"" rel=""nofollow"">here</a> you can specify the format of data when you read it with memmapfile, so try to ""play"" with different values. From the <a href=""http://uk.mathworks.com/help/matlab/ref/audioread.html"" rel=""nofollow"">documentation</a> I read that you can read data in double format, so try to modify the memmapfile data format and audioread data format.<br>
Last thing, memmapfile always organize the data in matrix like ""somenumbers x 1"", so if you want the original one you need to use something like reshape.
Anyway if you work with big data I suggest you to try with something different instead memmapfile, because it is very very slow </p>
"
"4436207","9","<java><machine-learning><classification><weka><prediction>","27857486","0","using the saved model for predicting using weka (Eclipse+Java)","<p>I was confused with the arguments of the lines ""Instances originalTrain="" can anyone please help me to correct this error since I was new to this weka. We are creating a disease prediction system using weka in java.</p>

<pre><code>import weka.classifiers.Classifier;
import weka.core.Instances;

public class Main {

    public static void main(String[] args) throws Exception
    {
        String rootPath=""/some/where/""; 
        Instances originalTrain= //instances here (don't know to complete this statement)

        //load model
        Classifier cls = (Classifier) weka.core.SerializationHelper.read(rootPath+""tree.model"");

        //predict instance class values
        Instances originalTrain= //load or create Instances to predict (This statement too)

        //which instance to predict class value
        int s1=0;

        //perform your prediction
        double value=cls.classifyInstance(originalTrain.instance(s1));

        //get the prediction percentage or distribution
        double[] percentage=cls.distributionForInstance(originalTrain.instance(s1));

        //get the name of the class value
        String prediction=originalTrain.classAttribute().value((int)value); 

        System.out.println(""The predicted value of instance ""+
                            Integer.toString(s1)+
                            "": ""+prediction); 

        //Format the distribution
        String distribution="""";
        for(int i=0; i &lt;percentage.length; i=i+1)
        {
            if(i==value)
            {
                distribution=distribution+""*""+Double.toString(percentage[i])+"","";
            }
            else
            {
                distribution=distribution+Double.toString(percentage[i])+"","";
            }
        }
        distribution=distribution.substring(0, distribution.length()-1);

        System.out.println(""Distribution:""+ distribution);
    }

}
</code></pre>
","2015-01-09 09:31:41","27861498","628499","2731","0","27861498","<p>For completeness, the code snippet in the question originates from <a href=""https://stackoverflow.com/questions/21674522/get-prediction-percentage-in-weka-using-own-java-code-and-a-model/21678307#21678307"">Get prediction percentage in WEKA using own Java code and a model</a>.</p>

<p>originalTrain should be your training instances.  There are two ways that I know to add instances to originalTrain.</p>

<ol>
<li><p>This method loads data from an .arff file and is based on instructions found <a href=""http://weka.wikispaces.com/Use+WEKA+in+your+Java+code"" rel=""nofollow noreferrer"">here</a>.<br>
<pre><code>// rootPath should be where the .arff file is held
// filename should hold the complete name of the .arff file
public static Instances instanceData(String rootPath, String filename) throws Exception
{ 
  // initialize source 
  DataSource source = null;
  Instances data = null;
  source = new DataSource(rootPath + filename);
  data = source.getDataSet();<br>
  // set the class to the last attribute of the data (may need to tweak) 
  if (data.classIndex() == -1)
   data.setClassIndex(data.numAttributes() -1 );
  return data;
}
</pre></code></p></li>
<li><p>You can create and add instance manually as described in this answer <a href=""https://stackoverflow.com/questions/20093977/define-input-data-for-clustering-using-weka-api/20109536#20109536"">Define input data for clustering using WEKA API</a> .  </p></li>
</ol>
"
"4415339","103","<machine-learning><genetic-algorithm><evolutionary-algorithm><genetic-programming>","27875980","4","How can we implement loop constructs in genetic programming?","<p>I've been playing around with genetic programming for some time and started wondering how to implement looping constructs.</p>
<p>In the case of for loops I can think of 3 parameters:</p>
<ul>
<li><strong>start</strong>: starting value of counter</li>
<li><strong>end</strong>: counter upper limit</li>
<li><strong>expression</strong>:  the expression to execute while counter &lt; end</li>
</ul>
<p>Now the tricky part is the <em>expression</em> because it generates the same value in every iteration unless <em>counter</em> is somehow injected into it. So I could allow the symbol for <em>counter</em> to be present in the expressions but then how do I prevent it from appearing outside of for loops?</p>
<p>Another problem is using the result of the expression. I could have a for loop which sums the results, another one that multiplies them together but that's limiting and doesn't seem right. I would like a general solution, not one for every operator.</p>
<p>So does anyone know a good method to implement loops in genetic programming?</p>
","2015-01-10 11:58:07","27877867","461202","6561","1","27877867","<p>Well, that's tricky. Genetic programming (the original Koza-style GP) is best suited for functional-style programming, i.e. there is no internal execution state and every node is a function that returns (and maybe takes) values, like lisp. That is a problem when the node is some loop - it is not clear what the node should return.</p>
<p>You could also design your loop node as a binary node. One parameter is a boolean expression that will be called before every loop and if true is returned, the loop will be executed. The second parameter would be the loop expression.</p>
<p>The problem you already mentioned, that there is no way of changing the loop expression. You can solve this by introducing a concept of some internal state or variables. But that leaves you with another problems like the need to define the number of variables. A variable can be realized e.g. by a tuple of functions - a setter (one argument, no return value, or it can return the argument) and getter (no arguments, returns the value of the variable).</p>
<p>Regarding the way of handling the loop result processing, you could step from GP to strongly typed GP or STGP for short. It is essentialy a GP with types. Your loop could then be effectively a function that returns a list of values (e.g. numbers) and you could have other functions that take such lists and calculate other values...</p>
<p>There is another GP algorithm (my favourite), called Grammatical Evolution (or GE) which uses context-free grammar to generate the programs. It can be used to encode type information like in STGP. You could also define the grammar in a way that classical c-like for and while loops can be generated. Some extensions to it, like Dynamically Defined Functions, could be used to implement variables dynamically.</p>
<p>If there is anything unclear, just comment on the answer and I'll try to clarify it.</p>
"
"4415339","103","<machine-learning><genetic-algorithm><evolutionary-algorithm><genetic-programming>","27875980","4","How can we implement loop constructs in genetic programming?","<p>I've been playing around with genetic programming for some time and started wondering how to implement looping constructs.</p>
<p>In the case of for loops I can think of 3 parameters:</p>
<ul>
<li><strong>start</strong>: starting value of counter</li>
<li><strong>end</strong>: counter upper limit</li>
<li><strong>expression</strong>:  the expression to execute while counter &lt; end</li>
</ul>
<p>Now the tricky part is the <em>expression</em> because it generates the same value in every iteration unless <em>counter</em> is somehow injected into it. So I could allow the symbol for <em>counter</em> to be present in the expressions but then how do I prevent it from appearing outside of for loops?</p>
<p>Another problem is using the result of the expression. I could have a for loop which sums the results, another one that multiplies them together but that's limiting and doesn't seem right. I would like a general solution, not one for every operator.</p>
<p>So does anyone know a good method to implement loops in genetic programming?</p>
","2015-01-10 11:58:07","27877867","2036035","3519","1","47723172","<p>The issue with zegkjan's answer is that there is more than one way to skin a cat. </p>

<p>Theres actually a simpler, and at times, better solution to creating GP datastructures than koza trees, instead using stacks. </p>

<p>This method is called <a href=""https://pdfs.semanticscholar.org/d992/1698680ca2327d7c242ebdb530707f170a4e.pdf"" rel=""nofollow noreferrer"">Stack Based Genetic Programming</a>, which is quite old (1993).  This method of genetic programming removes trees entirely, you have a instruction list, and a data stack (where your functional and terminal set remain the same).   You iterate through your instruction list, pushing values to the data stack, and pulling values to consume them, and returning a new value/values to the stack given your instruction.  For example, consider the following genetic program.</p>

<pre><code>0: PUSH TERMINAL X
1: PUSH TERMINAL X
2: MULTIPLY (A,B)
</code></pre>

<p>Iterating through this program will give you:</p>

<pre><code>step1: DATASTACK X
step2: DATASTACK X, X
step3: DATASTACK X^2
</code></pre>

<p>Once you have executed all program list statements, you then just take off the number of elements from the stack that you care about (you can get multiple values out of the GP program this way).   This ends up being a fast and extremely flexible method (memory locality, number of parameters doesn't matter, nor number of elements returned) you can implement fairly quickly.</p>

<p>To loop in this method, you can create a separate stack, an execution stack, where new special operators are used to push and pop multiple statements from the execution stack at once to be executed afterwards. </p>

<p>Additionally you can simply include a jump statement to move backwards in your program list, make a loop statement, or have a separate stack holding loop information.  With this a genetic program could theoretically develop its own for loop. </p>

<pre><code>0: PUSH TERMINAL X
1: START_LOOP 2
2: PUSH TERMINAL X
3: MULTIPLY (A, B)
4: DECREMENT_LOOP_NOT_ZERO

step1: DATASTACK X
       LOOPSTACK 
step2: DATASTACK X
       LOOPSTACK [1,2]
step3: DATASTACK X, X
       LOOPSTACK [1,2]
step4: DATASTACK X^2
       LOOPSTACK [1,2]
step5: DATASTACK X^2
       LOOPSTACK [1,1]
step6: DATASTACK X^2, X
       LOOPSTACK [1,1]
step7: DATASTACK X^3
       LOOPSTACK [1,1]
step8: DATASTACK X^3
       LOOPSTACK [1,0]
</code></pre>

<p>Note however, with any method, it may be difficult for a GP program to actually evolve a member that has a loop, and even if it does, its likely that such a mechanism would result in a poor fitness evaluation at the start, likely removing it from the population any way.   To fix this type of problem (potentially good innovations dying early due to poor early fitness), you'll need to include the concepts of <a href=""https://en.wikipedia.org/wiki/Deme_(biology)"" rel=""nofollow noreferrer"">demes</a> in your genetic program to isolate genetically disparate populations. </p>
"
"4379528","43","<r><machine-learning><statistics><svm>","27864446","3","An Error from SVM in R","<p>I am new to R and try to retrieve data from a text, and then apply it in SVM for classification. Here is the code:</p>

<pre><code>train&lt;-read.table(""training.txt"")
train[which(train==""?"",arr.ind=TRUE)]&lt;-NA
train=unique(train)
y=train[,length(train)]

classifier&lt;-svm(y~.,data=train[,-length(train)],scale=F)
classifier&lt;-svm(x=train[,-length(train)],y=factor(y),scale=F)
</code></pre>

<p>I try the 2 different ways to invoke svm, for the 1st one <code>(svm(y~.,data=train[,-length(train)],scale=F))</code> seems ok, but the 2nd one has problems, it reported:</p>

<pre><code>Error in svm.default(x = train[, length(train)], y = factor(y), scale = F) : 
  NA/NaN/Inf in foreign function call (arg 1)
In addition: Warning message:
In svm.default(x = train[, length(train)], y = factor(y), scale = F) :
  NAs introduced by coercion
</code></pre>

<p>Here is a sample of the <code>training.txt</code>, the last column is the target</p>

<pre><code>39,State-gov,77516,Bachelors,13,Never-married,Adm-clerical,Not-in-family,White,Male,2174,0,40,United-States,0
50,Self-emp-not-inc,83311,Bachelors,13,Married-civ-spouse,Exec-managerial,Husband,White,Male,0,0,13,United-States,0
38,Private,215646,HS-grad,9,Divorced,Handlers-cleaners,Not-in-family,White,Male,0,0,40,United-States,0
53,Private,234721,11th,7,Married-civ-spouse,Handlers-cleaners,Husband,Black,Male,0,0,40,United-States,0
28,Private,338409,Bachelors,13,Married-civ-spouse,Prof-specialty,Wife,Black,Female,0,0,40,Cuba,0
37,Private,284582,Masters,14,Married-civ-spouse,Exec-managerial,Wife,White,Female,0,0,40,United-States,0
49,Private,160187,9th,5,Married-spouse-absent,Other-service,Not-in-family,Black,Female,0,0,16,Jamaica,0
52,Self-emp-not-inc,209642,HS-grad,9,Married-civ-spouse,Exec-managerial,Husband,White,Male,0,0,45,United-States,1
31,Private,45781,Masters,14,Never-married,Prof-specialty,Not-in-family,White,Female,14084,0,50,United-States,1
42,Private,159449,Bachelors,13,Married-civ-spouse,Exec-managerial,Husband,White,Male,5178,0,40,United-States,1
37,Private,280464,Some-college,10,Married-civ-spouse,Exec-managerial,Husband,Black,Male,0,0,80,United-States,1
30,State-gov,141297,Bachelors,13,Married-civ-spouse,Prof-specialty,Husband,Asian-Pac-Islander,Male,0,0,40,India,1
23,Private,122272,Bachelors,13,Never-married,Adm-clerical,Own-child,White,Female,0,0,30,United-States,0
32,Private,205019,Assoc-acdm,12,Never-married,Sales,Not-in-family,Black,Male,0,0,50,United-States,0
40,Private,121772,Assoc-voc,11,Married-civ-spouse,Craft-repair,Husband,Asian-Pac-Islander,Male,0,0,40,NA,1
</code></pre>

<p>Any idea about it? thanks in advance!</p>
","2015-01-09 15:59:45","27864621","4130044","33779","5","27864621","<p>From documentation:</p>

<p>For the <code>x</code> argument:</p>

<pre><code>a data matrix, a vector, or a sparse matrix (object of class Matrix
provided by the Matrix package,or of class matrix.csr provided by the
SparseM package, or of class simple_triplet_matrix provided by the slam package).
</code></pre>

<p>For the <code>y</code> argument:</p>

<pre><code>a response vector with one label for each row/component of x. Can be
either a factor (for classification tasks) or a numeric vector (for regression).
</code></pre>

<p>When you type: <code>x=train[,-length(train)]</code> in the second function you are practically using a <code>data.frame</code> which is not supported and it crashes.</p>

<p>The <code>svm</code> function works with a numeric matrix <strong>only</strong></p>

<pre><code>library(e1071)
train[which(train==""?"",arr.ind=TRUE)]&lt;-NA
train=unique(train)
y=factor(train[,length(train)])
train &lt;- data.frame(lapply(train,as.numeric)) #convert to numeric. factors are integer fields anyway behind the scenes.

train &lt;- as.matrix(train[-length(train)])

classifier&lt;-svm(x= train ,y=y,scale=F)
</code></pre>

<p>Output:</p>

<pre><code>&gt; summary(classifier)

Call:
svm.default(x = train, y = y, scale = F)


Parameters:
   SVM-Type:  C-classification 
 SVM-Kernel:  radial 
       cost:  1 
      gamma:  0.07142857 

Number of Support Vectors:  14

 ( 9 5 )


Number of Classes:  2 

Levels: 
 0 1
</code></pre>
"
"1330926","1803","<machine-learning><svm><libsvm>","27883413","0","The meaning of the output from grid.py in libsvm","<p>I'm a newbie in SVM, and have several questions regarding a tool in <a href=""https://github.com/cjlin1/libsvm"" rel=""nofollow"">libsvm</a>.</p>

<p>There's tools/grid.py which tools/README explains as ""parameter selection tool for C-SVM classification using  47 the RBF (radial basis function) kernel"".</p>

<p>I have 2 questions regarding this tool. </p>

<ol>
<li>What this tool does is: given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters by performing grid search. Am I correct?</li>
</ol>

<p>e.g. Given a dataset like following, whose the label is only dependent on param1,</p>

<pre><code>label, param1, param2, param3
0    , 0     , 61    , 2     
0    , 0     , 92    , 6
1    , 1     , 10    , 32
1    , 1     , 83    , 10
</code></pre>

<p>If we apply grid.py to this dataset, does it tell me that most ""efficient"" (in the way that it precisely identifies the class of a test data) and ""minimum"" (in the way that only that no trivial parameters are included) parameter is param1.</p>

<ol start=""2"">
<li>If the answer of the question above is YES, how can I know which parameters are efficient and minimum? I see some output files but doens't make sense for me. If it's NO, are there any de facto standard method for doing what I want?</li>
</ol>
","2015-01-11 02:34:21","27982247","1330926","1803","0","27982247","<p>Probably found the answer.</p>

<hr>

<h3>Question 1. What this tool does is: given sets of label/feature_parameters, chooses the most ""efficient"" and ""minimum"" feature_parameters by performing grid search. Am I correct?</h3>

<p>The answer is No.
grid.py performs <a href=""http://scikit-learn.org/stable/modules/grid_search.html"" rel=""nofollow"">grid search</a> and estimates the best <code>cost</code> and <code>gamma</code> value. So it helps making SVM ""efficient"" anyway, but not doesn't help finding minimum set of features (well, there's no ""absolute minimum"" probably, because accuracy and the number of features is probably in proportion).</p>

<h3>Question 2. If the answer of the question above is YES, how can I know which parameters are efficient and minimum? I see some output files but doesn't make sense for me. If it's NO, are there any de facto standard method for doing what I want?</h3>

<p>There probably isn't any de facto standard method. Perhaps when deciding feature parameters, we should first take approach from domain's perspective, and then adjust them in mathematical approach. (e.g. if you're making SVM to descriminate malwares and benign apps, you should think of the behaviour and tendency of malwares, and decide the ""parameter candidates"" first, then apply mathematical approach like calculating average and deviation of each features.)</p>
"
"4273266","4755","<machine-learning><scikit-learn>","27884601","1","Error when I pass a different form of K in knearest neighbours(Sci kit learn)","<p>When running k nearest neighbors in scikit learn, When I set k as 21 I get value error. but when I set k as <code>k=np.arange(20) +1</code> I dont get an error, so what is the difference between these two? </p>

<pre><code>k = np.arange(21)

parameters = {'n_neighbors': k}
knn = sklearn.neighbors.KNeighborsClassifier()

clf = sklearn.grid_search.GridSearchCV(knn, parameters, cv=10)
clf.fit(X_train, Y_train)

ValueError: Invalid shape in axis 1: 0.
</code></pre>

<p>Also can some one explain me what is the </p>

<pre><code>a = clf.grid_scores_
scores = [b.cv_validation_scores for b in a]
</code></pre>

<p>Out put of scores is as follows :</p>

<pre><code>array([ 1.        ,  0.90909091,  1.        ,  0.72727273,  0.9       ,
         1.        ,  1.        ,  1.        ,  1.        ,  0.88888889]),
</code></pre>

<p><code>clf.grid_scores_</code> is accuracy of the classifier but what is that <code>cv_validation</code> scores?? </p>
","2015-01-11 06:31:35","27885788","1190430","5156","1","27885788","<p><code>GridSearchCV</code> expects a list of values for each parameter to search over. If you want to set just one value for the search, put it into list:</p>

<pre><code>parameters = {'n_neighbors': [21]} # ok
parameters = {'n_neighbors': np.range(21)} # error, the first value is 0
parameters = {'n_neighbors': np.range(21) + 1} # ok
</code></pre>

<p>Notice that each value in that list should be valid value for that parameter. For example, using <code>np.range(p)</code> as a list of parameters for <code>n_neighbors</code> is wrong since it has <code>0</code> which is invalid value for number of neighbors.</p>

<p>According to the <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html#sklearn-grid-search-gridsearchcv"" rel=""nofollow"">documentation</a>,</p>

<blockquote>
  <p><code>cv_validation_scores</code> [is] the list of scores for each fold</p>
</blockquote>

<p>Thus <code>grid_scores_</code> must be averages of corresponding <code>cv_validation_scores</code>.</p>
"
"2085886","319","<python><arrays><numpy><matrix><machine-learning>","27884413","1","Numpy: select value at a particular row for each column of a matrix","<p>I have a 2D matrix <code>X = ((a11, a12, .. a1n), (a21 .. a2n) .. (am1, .. amn))</code> and a 1D vector <code>y = [y1, ..., yn]</code> each <code>yi</code> is between <code>1</code> and <code>m</code>. For each column <code>i</code> of <code>X</code> I want to pick out the element at row <code>yi</code>. That is, I want to pick out the vector <code>z = (a_(y1 1), ... a_(yn n))</code>. </p>

<p>Is there a vectorized way to do this?</p>
","2015-01-11 05:56:12","27884515","1078084","67010","1","27884515","<p>How about this: </p>

<pre><code>In [39]: x = np.arange(12).reshape(4,3)

In [40]: y = np.array([0,3,2])

In [41]: x[y[None, :], np.arange(len(y))[None,:]][0]
Out[41]: array([ 0, 10,  8])

In [42]: x
Out[42]: 
array([[ 0,  1,  2],
       [ 3,  4,  5],
       [ 6,  7,  8],
       [ 9, 10, 11]])
</code></pre>
"
"2085886","319","<python><arrays><numpy><matrix><machine-learning>","27884413","1","Numpy: select value at a particular row for each column of a matrix","<p>I have a 2D matrix <code>X = ((a11, a12, .. a1n), (a21 .. a2n) .. (am1, .. amn))</code> and a 1D vector <code>y = [y1, ..., yn]</code> each <code>yi</code> is between <code>1</code> and <code>m</code>. For each column <code>i</code> of <code>X</code> I want to pick out the element at row <code>yi</code>. That is, I want to pick out the vector <code>z = (a_(y1 1), ... a_(yn n))</code>. </p>

<p>Is there a vectorized way to do this?</p>
","2015-01-11 05:56:12","27884515","3923281","130163","1","27886919","<p>As an alternative solution, <code>np.choose</code> is useful for making the selections.</p>

<pre><code>&gt;&gt;&gt; x = np.arange(16).reshape(4,4)
</code></pre>

<p>So <code>x</code> looks like this:</p>

<pre><code>array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15]])
</code></pre>

<p>Now the selection of the value at a particular row <code>y</code> in each column can be done like this:</p>

<pre><code>&gt;&gt;&gt; y = np.array([3, 0, 2, 1])
&gt;&gt;&gt; np.choose(y, x)
array([12, 1, 10,  7])
</code></pre>
"
"3423045","968","<opencv><math><image-processing><machine-learning><svm>","27898181","1","Correct way to do Min-Max normalization","<p>I am implementing alphabet classification using opencv svm. 
I have doubt in normalizing feature vector. 
I have two ways of normalizing feature vector, 
I need to find which is logically correct normalization method ??</p>

<p><strong>Method 1</strong></p>

<p>Suppose I have 3 feature vector as follows</p>

<pre><code>[2,  3,  8, 5 ] -&gt; image 1
[3,  5,  2, 5 ] -&gt; image 2
[9,  3,  8, 5 ] -&gt; image 3
</code></pre>

<p>And each value in feature vector is obtained by convolving the pixel with a kernal.</p>

<p>Currently I am finding maximum and minimum value of the each column and doing normalization based on that.</p>

<p>In the above case first column is <code>[2, 3, 9]</code></p>

<pre><code>min = 2
max = 9
</code></pre>

<p>and normalization of first column is done based on that. Likewise all other columns are normalized</p>

<p><strong>Method 2</strong></p>

<p>If the kernal is as follows </p>

<pre><code>[-1   0  1]
[-1   0  1]
[-1   0  1]
</code></pre>

<p>then maximum and minimum value can obtained by convolving with above kernel is as follows (8 bit image- Intensity range: 0-255) </p>

<pre><code>max val = 765
min val = -765
</code></pre>

<p>And normalize every value with above max min ?</p>

<p>Which is logically correct way to do normalization (method-1 or method-2) ?</p>
","2015-01-12 08:50:26","27911434","3214668","650","1","27911434","<p>The standard way to do it is method-1 (see the answer to <a href=""https://stackoverflow.com/questions/15436367/svm-scaling-input-values"">this question</a>). I also recommend you to read <a href=""http://www.csie.ntu.edu.tw/~cjlin/papers/guide/guide.pdf"" rel=""nofollow noreferrer"">this paper</a> for a good reference about svm training.</p>

<p>However, in you case, the range of all features computed with the same kernel will be similar , and method-1 may hurt more than it helps (for example by increasing noise of almost constant features).</p>

<p>So my advice would be : test both methods, and evalute performances to see what works best in your case.</p>
"
"3141533","475","<python><machine-learning><nltk>","27897591","1","Python NLTK Naive Bayes Classifier: What is the underlying computation that this classifier uses to classifiy input?","<p>I use the Naive Bayes classifier in Python NLTK to compute the probability distribution for the following example:</p>

<pre><code>import nltk

def main():
    train = [(dict(feature=1), 'class_x'), (dict(feature=0), 'class_x'),   (dict(feature=0), 'class_y'), (dict(feature=0), 'class_y')]

    test = [dict(feature=1)]

    classifier = nltk.classify.NaiveBayesClassifier.train(train)

    print(""classes available: "", sorted(classifier.labels()))

    print (""input assigned to: "", classifier.classify_many(test))

    for pdist in classifier.prob_classify_many(test):
        print (""probability distribution: "")
        print ('%.4f %.4f' % (pdist.prob('class_x'), pdist.prob('class_y')))

if __name__ == '__main__':
    main()
</code></pre>

<p>There are two classes (class_x and class_y) in the training dataset. Two inputs are given to each of the classes. For class_x, the first input feature has a value of 1, and the second a value of 0. For class_y, both input features have a value of 0. The test dataset is made up of one input, with a value of 1.</p>

<p>When I run the code, the output is:</p>

<pre><code>classes available:  ['class_x', 'class_y']
input assigned to:  ['class_x']
0.7500 0.2500
</code></pre>

<p>To get the probabilities, or likelihoods, for each class, the classifier should multiply the prior of the class (in this case, 0.5) by the probabilities of each of the features in the class. Smoothing should be considered. </p>

<p>I usually use a formula similar to this (or a similar variant):</p>

<p>P(feature|class) = prior of class * frequency of feature in class <strong>+1</strong> / total features in class + <strong>Vocabulary size</strong>. Smoothing can vary and slightly changes the outcome. </p>

<p>In the example code above, how exactly does the classifier compute the probability distribution? What is the formula used? </p>

<p>I checked <a href=""http://www.nltk.org/howto/classify.html"" rel=""nofollow"">here</a> and <a href=""http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/"" rel=""nofollow"">here</a>, but could not get any information as to exactly how the computation is done. </p>

<p>Thanks in advance.</p>
","2015-01-12 08:05:23","30143801","1277335","2263","2","30143801","<p>From the source code</p>

<p><a href=""https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L9yo"" rel=""nofollow"">https://github.com/nltk/nltk/blob/develop/nltk/classify/naivebayes.py#L9yo</a></p>

<pre><code>|                       P(label) * P(features|label)
|  P(label|features) = ------------------------------
|                              P(features)
</code></pre>
"
"447458","3849","<python><machine-learning>","27909622","2","Predicting time series data with python","<p>I'm starting with machine learning and so far have only tested scikit-learn but I couldn't find the right algorithm or an example similar to my problem.</p>

<p>I have a time series showing where an event happened. The location of the event is identified with an integer between 1 and 25 ( including ). At a certain date, an event cannot happen at the same place twice and it always happens in 5 places.</p>

<p>My data looks like this:</p>

<pre><code>2015-01-01,1,3,5,8,9,10
2015-01-03,23,16,3,5,9
2015-01-05,22,16,6,13,11
</code></pre>

<p>The first column is the date and the others are the places. Dates aren't included if nothing happened.</p>

<p>Do you have any recommendations on which algorithm should I take a look to try to predict the numbers ( places ) in the next time series?</p>

<p>An algorithm that is available in a Python library like scikit-learn would be perfect!</p>
","2015-01-12 19:48:20","27909790","1330293","33641","1","27909790","<p>One idea would to treat it as a multi-class problem. You can imagine this as your target <code>y</code> having 25 rows (actually 24 but forget about it for now) where each column is 1 or 0 representing wether the event happened or not.</p>

<p>As predictors for your <code>X</code> you can chose some lagged average or the last lets say <code>3</code> observations. See <a href=""https://stackoverflow.com/questions/27852199/python-machine-learning-algorithm-to-recognize-known-events/27852335#27852335"">this question</a> for more details.</p>

<p>Some code:</p>

<pre><code>from io import StringIO
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import MultiLabelBinarizer

mlb = MultiLabelBinarizer()

s=""""""
2015-01-01,1,2,3
2015-01-03,1,2,4
2015-01-05,1,2,4
2015-01-07,1,4,3
""""""
df = pd.read_csv(StringIO(s), index_col=0, parse_dates=True, header=None)

mlb = MultiLabelBinarizer()
labels = mlb.fit_transform(df.values)
labels
[[1 1 1 0]
 [1 1 0 1]
 [1 1 0 1]
 [1 0 1 1]]
</code></pre>

<p>We have 4 classes and 4 examples so we get a 4x4 matrix. Columns represent classes/locations and rows are events.</p>

<p>Now we will use the first 3 observations to predict the fourth one:</p>

<pre><code>X = labels[:-1]   
[[1 1 1 0]
 [1 1 0 1]
 [1 1 0 1]]
</code></pre>

<p>We get 4 classes and 3 observations. We need to make it a vector because this is only a sample:</p>

<pre><code>&gt;&gt;&gt; X.flatten()
[1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1]
</code></pre>

<p>Each column here is a feature/predictor that can be interpreted in the following way: A 1 in the first column means that class one was present 3 days a go. A 0 in the 7th column means that class 3 was not present in 2 days ago, and so on.</p>

<p>So now we have one sample/event (one row of the final <code>X</code> matrix) and the corresponding label(one row of the target <code>y</code>):</p>

<pre><code>&gt;&gt;&gt; labels[-1]
[1 0 1 1]
</code></pre>

<p>If you follow this procedure you will be able to get a training set that can be fed to a classifier.</p>
"
"4140027","3975","<machine-learning><scikit-learn><libsvm>","27912872","31","What is the difference between SVC and SVM in scikit-learn?","<p>From the <a href=""http://scikit-learn.org/stable/modules/svm.html"">documentation</a> scikit-learn implements SVC, NuSVC and LinearSVC which are classes capable of performing multi-class classification on a dataset. By the other hand I also read about that scikit learn also uses libsvm for support vector machine algorithm. I'm a bit confused about what's the difference between SVC and libsvm versions, by now I guess the difference is that SVC is the support vector machine algorithm fot the multiclass problem and libsvm is for the binary class problem. Could anybody help me to understad the difference between this?.</p>
","2015-01-12 23:59:34","27913126","1330293","33641","29","27913126","<p>They are just different implementations of the same algorithm. The SVM module (SVC, NuSVC, etc) is a wrapper around the <a href=""http://www.csie.ntu.edu.tw/~cjlin/libsvm/"" rel=""noreferrer"">libsvm</a> library and supports different kernels while <code>LinearSVC</code> is based on <a href=""http://www.csie.ntu.edu.tw/~cjlin/liblinear/"" rel=""noreferrer"">liblinear</a> and only supports a linear kernel. So:</p>

<pre><code>SVC(kernel = 'linear')
</code></pre>

<p>is in theory ""equivalent"" to:</p>

<pre><code>LinearSVC()
</code></pre>

<p>Because the implementations are different in practice you will get different results, the most important ones being that LinearSVC only supports a linear kernel, is faster and can scale a lot better.</p>
"
"4140027","3975","<machine-learning><scikit-learn><libsvm>","27912872","31","What is the difference between SVC and SVM in scikit-learn?","<p>From the <a href=""http://scikit-learn.org/stable/modules/svm.html"">documentation</a> scikit-learn implements SVC, NuSVC and LinearSVC which are classes capable of performing multi-class classification on a dataset. By the other hand I also read about that scikit learn also uses libsvm for support vector machine algorithm. I'm a bit confused about what's the difference between SVC and libsvm versions, by now I guess the difference is that SVC is the support vector machine algorithm fot the multiclass problem and libsvm is for the binary class problem. Could anybody help me to understad the difference between this?.</p>
","2015-01-12 23:59:34","27913126","12295283","39","1","64274403","<p>This is a snapshot from the book - <strong>Hands-on Machine Learning</strong></p>
<p><img src=""https://i.stack.imgur.com/BtPP2.png"" alt=""This is a snapshot from the book - Hands-on Machine Learning"" /></p>
<p>I hope you may find it useful.</p>
"
"4334098","109","<python><machine-learning><scikit-learn>","27924292","1","Python scikit-learn Predictionfail","<p>I'm new to Python and Machine Learning. I try to implement a simple Machine Learning script to predict the Topic of a Text, e.g. Texts about Barack Obama should be Mapped to Politicians.</p>

<p>I think i make the right moves to do that, but im not 100% sure so i ask you guys.</p>

<p>First of all here is my little script:</p>

<pre><code>#imports
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.naive_bayes import MultinomialNB
#dictionary for mapping the targets
categories_dict = {'0' : 'politiker','1' : 'nonprofit org'}

import glob
#get filenames from docs
filepaths = glob.glob('Data/*.txt')
print(filepaths)

docs = []

for path in filepaths:
doc = open(path,'r')
docs.append(doc.read())
#print docs


count_vect = CountVectorizer()
#train Data
X_train_count = count_vect.fit_transform(docs)
#print X_train_count.shape

#tfidf transformation (occurences to frequencys)
tfdif_transform = TfidfTransformer()
X_train_tfidf = tfdif_transform.fit_transform(X_train_count)

#get the categories you want to predict in a set, these must be in the order the train        docs are!
categories = ['0','0','0','1','1']
clf = MultinomialNB().fit(X_train_tfidf,categories)

#try to predict
to_predict = ['Barack Obama is the President of the United States','Greenpeace']

#transform(not fit_transform) the new data you want to predict
X_pred_counts = count_vect.transform(to_predict)
X_pred_tfidf = tfdif_transform.transform(X_pred_counts)
print X_pred_tfidf

#predict
predicted = clf.predict(X_pred_tfidf)

for doc,category in zip(to_predict,predicted):
    print('%r =&gt; %s' %(doc,categories_dict[category]))
</code></pre>

<p>Im sure about the general Workflow that is required to use this, but im not sure how i map the categories to the docs i use to train the classifier. I know that they must be in correct order and i think i got that but it doesn't output the right category.</p>

<p>Is that because my Documents i use to Train the Classifier are bad, or do i make a certain mistake im not aware of?</p>

<p>He predicts that both new Texts are about Target 0 (Politicians)</p>

<p>Thanks in advance.</p>
","2015-01-13 14:22:50","27925025","1330293","33641","1","27925025","<p>It looks like the model hyper parameters are not rightly tuned. It is difficult to make conclusions with so little data but if you use:</p>

<pre><code>model = MultinomialNB(0.5).fit(X, y)
# or
model = LogisticRegression().fit(X, y)
</code></pre>

<p>you will get the expected results, at least for words like ""Greenpeace"", ""Obama"", ""President"" which are so obviously correlated with its corresponding class. I took a quick look at the coefficients of the model and it seems to be doing the right thing.</p>

<p>For a more sophisticated approach to topic modeling I recommend you take a look at <a href=""https://radimrehurek.com/gensim/tutorial.html"" rel=""nofollow"">gensim</a>.</p>
"
"961627","10969","<python><machine-learning><computer-vision><classification><logarithm>","27926804","1","Classifier scores normalized for comparison, using log scale","<p>What's a good method to compare the scored results of different classifiers?</p>

<p>For example, let's say I have a classifier that deals with 12 different classes of fruit. 
Out of these 12 classes, 4 of them represent different views of an apple, 4 represent views of a banana, and 4 represent a pineapple. This is a multi-class classifier, and given an image, it assigns a score to all of the possible 12 fruit classes that the image might belong to. The class that got the highest score is selected as THE class of the image.</p>

<p>Now in addition to that - I have 3 individual classifiers, one for apples, one for bananas, and one for pineapples. Each classifier deals with 4 different views of an individual fruit.</p>

<p>I want to compare whether using the single 12-class classifier gives better results than using a combination of the individual 4-class classifiers.</p>

<p>When I run the 12-class classifier on images of apples, the results are indeed less accurate than the results of running the individual apple classifier, and the same goes for bananas and pineapples.</p>

<p>What I want to do now is build a combination of the 3 classifiers. So my program will run all 3 classifiers on a single image and tell me what the most likely class is. </p>

<p>The problem is - how can I normalize the scores across the different classifiers, so that I can make a comparison among the the 3 classifiers' classes and choose the class with the highest score? Although the method used to train the individual classifiers was the same, I doubt I can directly compare their scores without some kind of normalization.</p>

<p>Would it be practical to simply convert all the scores to the log scale and then compare them?</p>
","2015-01-13 16:26:43","","628538","2049","1","27937924","<p>A popular method to normalize classifiers is done by Platt-Scaling, which is implemented in libSVM. Its a straight forward normalization as described in the following link.</p>

<p><a href=""http://en.wikipedia.org/wiki/Platt_scaling"" rel=""nofollow"">http://en.wikipedia.org/wiki/Platt_scaling</a></p>
"
"492372","24812","<machine-learning><nlp><topic-modeling><text-analysis><mallet>","27927556","5","What do the parameters of the csvIterator mean in Mallet?","<p>I am using mallet topic modelling sample code and though it runs fine, I would like to know what the parameters of this statement actually mean?</p>

<pre><code>instances.addThruPipe(new CsvIterator(new FileReader(dataFile),
                                      ""(\\w+)\\s+(\\w+)\\s+(.*)"",
                                      3, 2, 1)  // (data, target, name) field indices                    
                     );
</code></pre>
","2015-01-13 17:04:25","27929358","419338","13672","8","27929358","<p>From the <a href=""http://mallet.cs.umass.edu/api/"" rel=""nofollow noreferrer"">documentation</a>:</p>

<blockquote>
  <p>This iterator, perhaps more properly called a Line Pattern Iterator, reads through a file and returns one instance per line, based on a regular expression.</p>
  
  <p>If you have data of the form</p>
  
  <p>[name]  [label]  [data]</p>
</blockquote>

<p>The call you are interested in is</p>

<pre><code>CsvIterator(java.io.Reader input, java.lang.String lineRegex, 
            int dataGroup, int targetGroup, int uriGroup) 
</code></pre>

<p>The first parameter is how data is read in, like a file reader or a string reader. The second parameter is the regex that is used to extract data from each line that's read from the reader. In your example, you've got <code>(\\w+)\\s+(\\w+)\\s+(.*)</code> which translates to:</p>

<ul>
<li>1 or more alphanumeric characters (capture group, this is the name of the instance), followed by</li>
<li>1 or more whitespace character (tab, space, ..), followed by</li>
<li>1 or more alphanumeric characters (capture group, this is the label/target), followed by</li>
<li>1 or more whitespace character (tab, space, ..), followed by</li>
<li>0 or more characters (this is the data)</li>
</ul>

<p>The numbers <code>3, 2, 1</code> indicate the data comes last, the target comes second, and the name comes first. The regex basically ensures the format of each line is as stated in the documentation:</p>

<pre><code>test1 spam Wanna buy viagra?
test2 not-spam Hello, are you busy on Sunday?
</code></pre>

<p><code>CsvIterator</code> is a terrible name, because it is not actually comma-separated values that this class reads in, it is whitespace-separated (space, tab, ...) values. </p>
"
"492372","24812","<machine-learning><nlp><topic-modeling><text-analysis><mallet>","27927556","5","What do the parameters of the csvIterator mean in Mallet?","<p>I am using mallet topic modelling sample code and though it runs fine, I would like to know what the parameters of this statement actually mean?</p>

<pre><code>instances.addThruPipe(new CsvIterator(new FileReader(dataFile),
                                      ""(\\w+)\\s+(\\w+)\\s+(.*)"",
                                      3, 2, 1)  // (data, target, name) field indices                    
                     );
</code></pre>
","2015-01-13 17:04:25","27929358","4413371","310","0","39545089","<p>Explanation given in above answer is way too good.</p>

<p>However one point is missing. Sequence of regular expression(regex) for each one of the data, label and name fields of input instances in Line regex needs to be in correspondence to the way instances are provided in input file i.e. if say you are providing name as 1st field , data as second field and label as 3rd field in your input file then you have to provide regex of name first followed by regex of data and then at last regex of label. Example is as shown below :</p>

<p>Input instance : Mail67(tab space)TCC problems. Hi there, For some reason no administrators in the Old Master Paintings department have been able to get information from TCC. It appears to be going through on JDE, but nothing appears when searched on TCC. Any help or guidance you can offer to f....(tab space)Inc</p>

<p>CsvIterator Parameters: CsvIterator(new FileReader(Path to file), ""(\w+)\t(.*)\t(\w+)"",2,3,1)</p>
"
"4285011","167","<math><machine-learning><probability><bayesian>","27927700","-3","Probability: the one true fish","<p>I have exams in Machine Learning coming up and I need help answering this question.</p>

<blockquote>
  <p>There are a million identical fish in a lake, one of which has
  swallowed the One True Ring. You must get it back! After months of
  effort, you catch another random fish and pass your metal detector
  over it, and the detector beeps! It is the best metal detector money
  can buy, and has a very low error rate: it fails to beep when near the
  ring only one in a billion times, and it beeps incorrectly only one in
  ten thousand times. What is the probability that, at long last, you’ve
  found your precious ring?</p>
</blockquote>

<p>This is my answer I worked out:</p>

<p><img src=""https://i.stack.imgur.com/bztZM.gif"" alt=""enter image description here""></p>

<p>Is this the right way to work out this type of question and is that somewhat the correct answer?</p>
","2015-01-13 17:13:13","27928164","3777428","5321","1","27928164","<p>What you want is the probability of having the right fish given that the detector beeps, which is <code>P(A|B)</code>.</p>

<p>The <code>P(B|A) = 9999/10000</code> is the probability of the detector beeping given you have the right fish. However, we don't know if the fish you have is the right one. All you know is that the detector beeps, and you can't tell if it's a true positive with probability <code>P(B|A)</code> or a false positive with probability <code>P(B|not A)</code>.</p>

<p>Bayes' theorem lets you switch between <code>P(B|A)</code> and <code>P(A|B)</code>, so the other information isn't useless fluff. You do in need it all to solve the problem.</p>
"
"961627","10969","<python><numpy><machine-learning><scikit-learn><confusion-matrix>","27944762","1","Invalid index to scalar variable in scikit confusion matrix","<p>I'm using a confusion matrix that is working just fine until I get to a specific part of my numpy arrays.</p>

<p>The ground truth results are stored in an array called <code>y_test</code>, while the classfiers' results are stored in <code>r</code>.</p>

<p>When I use the confusion matrix for the whole set of results, there are no problems. </p>

<p>But I want to divide the results from my experiment.
I have 3 specific classifiers' results which are stored in arrays called <code>c</code>, <code>b</code> and <code>t</code>.</p>

<p>Now I want to compare the results of these 3 specific classifiers against some specific indices of the overall results. For example, I want to highlight the confusion matrix for the results for classifier <code>C</code> specifically from indices 91 to 180 of the overall results.</p>

<p>For classifier <code>B</code> I want to see the confusion matrix of the results from indices 1 to 90.
And so on.</p>

<p>This is my code, below. For the first 2 confusion matrices, there are no problems. They show up fine.</p>

<pre><code>cm_c = confusion_matrix(y_test[91:80],c[91:80])
plt.matshow(cm_c)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

cm_b = confusion_matrix(y_test[1:90],b[1:90])
plt.matshow(cm_b)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()

cm_t = confusion_matrix(y_test[228:317,t[228:317])
plt.matshow(cm_t)
plt.title('Confusion matrix')
plt.colorbar()
plt.ylabel('True label')
plt.xlabel('Predicted label')
plt.show()
</code></pre>

<p>But for the last set of results above (using the results array from classifier <code>T</code>), I get the following error:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317], t[228:317])
IndexError: invalid index to scalar variable
</code></pre>

<p>I don't know what's wrong.</p>
","2015-01-14 14:02:24","27945255","1330293","33641","1","27945255","<p>In your line:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317,t[228:317])
</code></pre>

<p>you are missing a bracket. It should be:</p>

<pre><code>cm_t = confusion_matrix(y_test[228:317],t[228:317])
</code></pre>
"
"3214477","143","<machine-learning><scikit-learn><classification>","27936778","1","Classifier is giving different results every time it is trained even though the training and test data is the same","<p>The training and test data are pickled and loaded. But the results (accuracy and f-measure) seem to vary every time even though the classifier is trained with the same training data and tested with the same test data. How is this happening? The classifier I'm referring to is Extreme Learning Machine.</p>
","2015-01-14 06:08:14","27938100","628538","2049","2","27938100","<p>Depends on the classification algorithm you chose. If you chose something like random forests, each time a new model would be learnt, so such an observation is possible.</p>
"
"3214477","143","<machine-learning><scikit-learn><classification>","27936778","1","Classifier is giving different results every time it is trained even though the training and test data is the same","<p>The training and test data are pickled and loaded. But the results (accuracy and f-measure) seem to vary every time even though the classifier is trained with the same training data and tested with the same test data. How is this happening? The classifier I'm referring to is Extreme Learning Machine.</p>
","2015-01-14 06:08:14","27938100","486262","2365","0","27967404","<p>How different are the weights of the output? Do you get different parameters?</p>

<p>Bear in mind many algorithms start with different random seeds, which make them behave different for different sets of experiments.</p>

<p>How different are the outputs? Are they wildly different? If that is the case, perhaps you need to let it run longer.</p>

<p>As someone said, you have to use cross validation to find the best set of parameters and to do that you need to rotate your training and dataset.</p>
"
"4178757","157","<machine-learning><computer-vision><feature-extraction>","27955695","2","What's the difference between standardization and global contrast normalization? (Image preprocessing)","<p>I'm trying to understand the article located <a href=""https://caglarift6266.wordpress.com/"" rel=""nofollow"">here</a></p>

<p>I have 2 questions:</p>

<ol>
<li><p>What's the difference between standardization and global contrast normalization?
To the best of my understanding, I think they mean the same thing in which we subtracts each pixel of an image with the global mean and divides by the global standard deviation.</p></li>
<li><p>What's the purpose of them in machine learning or feature extraction topic?</p></li>
</ol>

<p>Thx</p>
","2015-01-15 02:01:56","28054241","2705625","468","0","28054241","<p>1.) It is not completely clear from the post you shared, but it looks like standardization is normalizing each image column separately (while global contrast normalization does this over all pixels in the image). My guess is that both use the Standardize function from this <a href=""http://deeplearning.net/software/pylearn2/library/datasets.html"" rel=""nofollow"">pylearn2 documentation</a> but the first step has the two global flags set to false, while the second step sets them to true.</p>

<p>2.) Normalization is a fairly standard step in machine learning/feature extraction. It basically attempts to convert data into a common input space, and can at least partially deal with external sources of data variation like illumination levels. A good start would be to read the Wikipedia article on <a href=""http://en.wikipedia.org/wiki/Feature_scaling"" rel=""nofollow"">feature scaling</a>.</p>
"
"3370773","395","<google-analytics><machine-learning><google-analytics-api>","27951742","0","Google-analytics framework for predictive analysis","<p>I'm trying to use the google-analytics framework to create predictive analysis tools. For example I would like to cluster my webpage visitors, etc.
In general, is there any list of machine learning algorithms implemented by this framework? for example: regression, clustering, classification, feature selection, etc.
Thank you for any help</p>
","2015-01-14 20:21:59","","2113279","717","0","28252234","<p>When you say GA framework I'll assume you're referring to the set of Google Analytics APIs listed <a href=""https://developers.google.com/analytics/"" rel=""nofollow"">here</a>. The framework by itself doesn't provide machine learning capabilities. It merely provides access to the processed and aggregated GA data stored in Google's servers. You can use the API and feed the data to a machine learning application/system/program that does all of the stuff you mentioned.</p>
"
"707887","141","<image><machine-learning><apache-spark><distributed-computing>","27959831","2","How does Spark read 100K images effeciently?","<p>Currently, I'm programming something on image classification with Spark. I need to read all the images into memory as RDD and my method is as following:  </p>

<pre><code>val images = spark.wholeTextFiles(""hdfs://imag-dir/"")  
</code></pre>

<p><strong>imag-dir</strong> is the target image storing directory on hdfs. With this method, all the images will be   loaded into memory and every image will be organized as ""image name, image content"" pair. However, I find this process is time consuming, is there any better way to load large image data set into spark? </p>
","2015-01-15 08:59:57","27960634","47978","11485","4","27960634","<p>I suspect that may be because you have a lot of <em>small</em> files on HDFS, which is a problem as such (the 'small files problem'). <a href=""http://blog.cloudera.com/blog/2009/02/the-small-files-problem/"" rel=""nofollow"">Here</a> you'll find a few suggestions in addressing the issue. </p>

<p>You may also want to set the number of partitions (the <code>minpartitions</code> argument of <code>wholetextFiles</code>) to a reasonable number : at least 2x the number of cores in your cluster (look <a href=""https://www.youtube.com/watch?v=dmL0N3qfSc8"" rel=""nofollow"">there</a> for details).</p>

<p>But in sum, apart from the 2 ideas above, the way you're loading those is ok and <em>not</em> where your problem lies (assuming <code>spark</code> is your Spark context).</p>
"
"4178757","157","<python><machine-learning><theano>","27948324","3","Implementing LeCun Local Contrast Normalization with Theano","<p>I'm trying to use the code that I found to implement the LeCun Local Contrast Normalization but I get incorrect result. The code is in Python and uses the <code>theano</code> library.</p>

<pre><code>def lecun_lcn(input, img_shape, kernel_shape, threshold=1e-4):
    """"""
    Yann LeCun's local contrast normalization
    Orginal code in Theano by: Guillaume Desjardins
    """"""
    input = input.reshape(input.shape[0], 1, img_shape[0], img_shape[1])
    X = T.matrix(dtype=theano.config.floatX)
    X = X.reshape(input.shape)

    filter_shape = (1, 1, kernel_shape, kernel_shape)
    filters = gaussian_filter(kernel_shape).reshape(filter_shape)

    convout = conv.conv2d(input=X,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    # For each pixel, remove mean of 9x9 neighborhood

    mid = int(np.floor(kernel_shape / 2.))
    centered_X = X - convout[:, :, mid:-mid, mid:-mid]
    # Scale down norm of 9x9 patch if norm is bigger than 1
    sum_sqr_XX = conv.conv2d(input=centered_X ** 2,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    denom = T.sqrt(sum_sqr_XX[:, :, mid:-mid, mid:-mid])
    per_img_mean = denom.mean(axis=[1, 2])
    divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
    divisor = T.maximum(divisor, threshold)

    new_X = centered_X / divisor
    new_X = new_X.dimshuffle(0, 2, 3, 1)
    new_X = new_X.flatten(ndim=3)

    f = theano.function([X], new_X)
    return f(input)
</code></pre>

<p>Here is the testing code:</p>

<pre><code>x_img_origin = plt.imread(""..//data//Lenna.png"")
x_img = plt.imread(""..//data//Lenna.png"")
x_img_real_result = plt.imread(""..//data//Lenna_Processed.png"")

x_img = x_img.reshape(1, x_img.shape[0], x_img.shape[1], x_img.shape[2])
for d in range(3):
    x_img[:, :, :, d] = tools.lecun_lcn(x_img[:, :, :, d], (x_img.shape[1], x_img.shape[2]), 9)
x_img = x_img[0]

pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(x_img_origin)
pylab.gray()
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(x_img)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(x_img_real_result)
pylab.show()
</code></pre>

<p>Here is the result: </p>

<p><img src=""https://i.stack.imgur.com/UnCq9.jpg"" alt=""Example image processing results""></p>

<p><em>(left to right: origin, my result, the expected result)</em> </p>

<p>Could someone tell me what I did wrong with the code?</p>
","2015-01-14 16:54:47","","4810149","11","1","29746766","<p>I think these two lines may have some mistakes on the matrix axes:</p>

<pre><code>per_img_mean = denom.mean(axis=[1, 2])
divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
</code></pre>

<p>and it should be rewritten as:</p>

<pre><code>per_img_mean = denom.mean(axis=[2, 3])
divisor = T.largest(per_img_mean.dimshuffle(0, 1, 'x', 'x'), denom)
</code></pre>
"
"4178757","157","<python><machine-learning><theano>","27948324","3","Implementing LeCun Local Contrast Normalization with Theano","<p>I'm trying to use the code that I found to implement the LeCun Local Contrast Normalization but I get incorrect result. The code is in Python and uses the <code>theano</code> library.</p>

<pre><code>def lecun_lcn(input, img_shape, kernel_shape, threshold=1e-4):
    """"""
    Yann LeCun's local contrast normalization
    Orginal code in Theano by: Guillaume Desjardins
    """"""
    input = input.reshape(input.shape[0], 1, img_shape[0], img_shape[1])
    X = T.matrix(dtype=theano.config.floatX)
    X = X.reshape(input.shape)

    filter_shape = (1, 1, kernel_shape, kernel_shape)
    filters = gaussian_filter(kernel_shape).reshape(filter_shape)

    convout = conv.conv2d(input=X,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    # For each pixel, remove mean of 9x9 neighborhood

    mid = int(np.floor(kernel_shape / 2.))
    centered_X = X - convout[:, :, mid:-mid, mid:-mid]
    # Scale down norm of 9x9 patch if norm is bigger than 1
    sum_sqr_XX = conv.conv2d(input=centered_X ** 2,
                             filters=filters,
                             image_shape=(input.shape[0], 1, img_shape[0], img_shape[1]),
                             filter_shape=filter_shape,
                             border_mode='full')

    denom = T.sqrt(sum_sqr_XX[:, :, mid:-mid, mid:-mid])
    per_img_mean = denom.mean(axis=[1, 2])
    divisor = T.largest(per_img_mean.dimshuffle(0, 'x', 'x', 1), denom)
    divisor = T.maximum(divisor, threshold)

    new_X = centered_X / divisor
    new_X = new_X.dimshuffle(0, 2, 3, 1)
    new_X = new_X.flatten(ndim=3)

    f = theano.function([X], new_X)
    return f(input)
</code></pre>

<p>Here is the testing code:</p>

<pre><code>x_img_origin = plt.imread(""..//data//Lenna.png"")
x_img = plt.imread(""..//data//Lenna.png"")
x_img_real_result = plt.imread(""..//data//Lenna_Processed.png"")

x_img = x_img.reshape(1, x_img.shape[0], x_img.shape[1], x_img.shape[2])
for d in range(3):
    x_img[:, :, :, d] = tools.lecun_lcn(x_img[:, :, :, d], (x_img.shape[1], x_img.shape[2]), 9)
x_img = x_img[0]

pylab.subplot(1, 3, 1); pylab.axis('off'); pylab.imshow(x_img_origin)
pylab.gray()
pylab.subplot(1, 3, 2); pylab.axis('off'); pylab.imshow(x_img)
pylab.subplot(1, 3, 3); pylab.axis('off'); pylab.imshow(x_img_real_result)
pylab.show()
</code></pre>

<p>Here is the result: </p>

<p><img src=""https://i.stack.imgur.com/UnCq9.jpg"" alt=""Example image processing results""></p>

<p><em>(left to right: origin, my result, the expected result)</em> </p>

<p>Could someone tell me what I did wrong with the code?</p>
","2015-01-14 16:54:47","","5896264","91","6","35259710","<p>Here is how I implemented local contrast normalization as reported in Jarrett et al (<a href=""http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf"">http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf</a>). You can use it as a separate layer.</p>

<p>I tested it on the code from the LeNet tutorial of theano in which I applied LCN to the input and to each convolutional layer which yields slightly better results.</p>

<p>You can find the full code here:
<a href=""https://github.com/jostosh/theano_utils/blob/master/lcn.py"">https://github.com/jostosh/theano_utils/blob/master/lcn.py</a> </p>

<pre><code>class LecunLCN(object):
def __init__(self, X, image_shape, threshold=1e-4, radius=9, use_divisor=True):
    """"""
    Allocate an LCN.

    :type X: theano.tensor.dtensor4
    :param X: symbolic image tensor, of shape image_shape

    :type image_shape: tuple or list of length 4
    :param image_shape: (batch size, num input feature maps,
                         image height, image width)
    :type threshold: double
    :param threshold: the threshold will be used to avoid division by zeros

    :type radius: int
    :param radius: determines size of Gaussian filter patch (default 9x9)

    :type use_divisor: Boolean
    :param use_divisor: whether or not to apply divisive normalization
    """"""

    # Get Gaussian filter
    filter_shape = (1, image_shape[1], radius, radius)

    self.filters = theano.shared(self.gaussian_filter(filter_shape), borrow=True)

    # Compute the Guassian weighted average by means of convolution
    convout = conv.conv2d(
        input=X,
        filters=self.filters,
        image_shape=image_shape,
        filter_shape=filter_shape,
        border_mode='full'
    )

    # Subtractive step
    mid = int(numpy.floor(filter_shape[2] / 2.))

    # Make filter dimension broadcastable and subtract
    centered_X = X - T.addbroadcast(convout[:, :, mid:-mid, mid:-mid], 1)

    # Boolean marks whether or not to perform divisive step
    if use_divisor:
        # Note that the local variances can be computed by using the centered_X
        # tensor. If we convolve this with the mean filter, that should give us
        # the variance at each point. We simply take the square root to get our
        # denominator

        # Compute variances
        sum_sqr_XX = conv.conv2d(
            input=T.sqr(centered_X),
            filters=self.filters,
            image_shape=image_shape,
            filter_shape=filter_shape,
            border_mode='full'
        )


        # Take square root to get local standard deviation
        denom = T.sqrt(sum_sqr_XX[:,:,mid:-mid,mid:-mid])

        per_img_mean = denom.mean(axis=[2,3])
        divisor = T.largest(per_img_mean.dimshuffle(0, 1, 'x', 'x'), denom)
        # Divisise step
        new_X = centered_X / T.maximum(T.addbroadcast(divisor, 1), threshold)
    else:
        new_X = centered_X

    self.output = new_X


def gaussian_filter(self, kernel_shape):
    x = numpy.zeros(kernel_shape, dtype=theano.config.floatX)

    def gauss(x, y, sigma=2.0):
        Z = 2 * numpy.pi * sigma ** 2
        return  1. / Z * numpy.exp(-(x ** 2 + y ** 2) / (2. * sigma ** 2))

    mid = numpy.floor(kernel_shape[-1] / 2.)
    for kernel_idx in xrange(0, kernel_shape[1]):
        for i in xrange(0, kernel_shape[2]):
            for j in xrange(0, kernel_shape[3]):
                x[0, kernel_idx, i, j] = gauss(i - mid, j - mid)

    return x / numpy.sum(x)
</code></pre>
"
"1835351","1271","<python><numpy><machine-learning><vectorization>","27948363","13","Numpy Broadcast to perform euclidean distance vectorized","<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
","2015-01-14 16:57:46","27948463","1409938","18821","19","27948463","<p>Simply use <code>np.newaxis</code> at the right place:</p>

<pre><code> np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
</code></pre>
"
"1835351","1271","<python><numpy><machine-learning><vectorization>","27948363","13","Numpy Broadcast to perform euclidean distance vectorized","<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
","2015-01-14 16:57:46","27948463","2476444","11889","4","27953679","<p>This functionality is already included in <a href=""http://docs.scipy.org/doc/scipy/reference/spatial.distance.html"" rel=""nofollow"">scipy's spatial module</a> and I recommend using it as it will be vectorized and highly optimized under the hood. But, as evident by the other answer, there are ways you can do this yourself.</p>

<pre><code>import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
from scipy.spatial.distance import cdist
cdist(a,b)
# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>
"
"1835351","1271","<python><numpy><machine-learning><vectorization>","27948363","13","Numpy Broadcast to perform euclidean distance vectorized","<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
","2015-01-14 16:57:46","27948463","5348749","561","23","35814006","<p>I had the same problem recently working with deep learning(stanford cs231n,Assignment1),but when I used</p>

<pre><code> np.sqrt((np.square(a[:,np.newaxis]-b).sum(axis=2)))
</code></pre>

<p>There was a error</p>

<pre><code>MemoryError
</code></pre>

<p>That means I ran out of memory(In fact,that produced a array of 500*5000*1024 in the middle.It's so huge!)</p>

<p>To prevent that error,we can use a formula to simplify:</p>

<p><img src=""https://latex.codecogs.com/gif.latex?(a-b)%5E2&space;=&space;a%5E2&space;-&space;2ab&plus;b%5E2"" title=""(a-b)^2 = a^2 - 2ab+b^2"" /></p>

<p>code:</p>

<pre><code>import numpy as np
aSumSquare = np.sum(np.square(a),axis=1);
bSumSquare = np.sum(np.square(b),axis=1);
mul = np.dot(a,b.T);
dists = np.sqrt(aSumSquare[:,np.newaxis]+bSumSquare-2*mul)
</code></pre>
"
"1835351","1271","<python><numpy><machine-learning><vectorization>","27948363","13","Numpy Broadcast to perform euclidean distance vectorized","<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
","2015-01-14 16:57:46","27948463","4561314","28608","34","37903795","<p>Here are the original input variables:</p>

<pre><code>A = np.array([[1,1,1,1],[2,2,2,2]])
B = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
A
# array([[1, 1, 1, 1],
#        [2, 2, 2, 2]])
B
# array([[1, 2, 3, 4],
#        [1, 1, 1, 1],
#        [1, 2, 1, 9]])
</code></pre>

<p>A is a 2x4 array.
B is a 3x4 array.</p>

<p>We want to compute the Euclidean distance matrix operation in one entirely vectorized operation, where <code>dist[i,j]</code> contains the distance between the ith instance in A and jth instance in B. So <code>dist</code> is 2x3 in this example.</p>

<p>The distance </p>

<p><a href=""https://i.stack.imgur.com/syuCq.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/syuCq.png"" alt=""enter image description here""></a></p>

<p>could ostensibly be written with numpy as</p>

<pre><code>dist = np.sqrt(np.sum(np.square(A-B))) # DOES NOT WORK
# Traceback (most recent call last):
#   File ""&lt;stdin&gt;"", line 1, in &lt;module&gt;
# ValueError: operands could not be broadcast together with shapes (2,4) (3,4)
</code></pre>

<p>However, as shown above, the problem is that the element-wise subtraction operation <code>A-B</code> involves incompatible array sizes, specifically the 2 and 3 in the first dimension.</p>

<pre><code>A has dimensions 2 x 4
B has dimensions 3 x 4
</code></pre>

<p>In order to do element-wise subtraction, we have to pad either A or B to satisfy numpy's broadcast rules. I'll choose to pad A with an extra dimension so that it becomes 2 x 1 x 4, which allows the arrays' dimensions to line up for broadcasting. For more on numpy broadcasting, see the <a href=""http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html"" rel=""noreferrer"">tutorial in the scipy manual</a> and the final example in <a href=""http://scipy.github.io/old-wiki/pages/EricsBroadcastingDoc"" rel=""noreferrer"">this tutorial</a>.</p>

<p>You can perform the padding with either <code>np.newaxis</code> value or with the <code>np.reshape</code> command. I show both below:</p>

<pre><code># First approach is to add the extra dimension to A with np.newaxis
A[:,np.newaxis,:] has dimensions 2 x 1 x 4
B has dimensions                     3 x 4

# Second approach is to reshape A with np.reshape
np.reshape(A, (2,1,4)) has dimensions 2 x 1 x 4
B has dimensions                          3 x 4
</code></pre>

<p>As you can see, using either approach will allow the dimensions to line up. I'll use the first approach with <code>np.newaxis</code>. So now, this will work to create A-B, which is a 2x3x4 array:</p>

<pre><code>diff = A[:,np.newaxis,:] - B
# Alternative approach:
# diff = np.reshape(A, (2,1,4)) - B
diff.shape
# (2, 3, 4)
</code></pre>

<p>Now we can put that difference expression into the <code>dist</code> equation statement to get the final result:</p>

<pre><code>dist = np.sqrt(np.sum(np.square(A[:,np.newaxis,:] - B), axis=2))
dist
# array([[ 3.74165739,  0.        ,  8.06225775],
#        [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>

<p>Note that the <code>sum</code> is over <code>axis=2</code>, which means take the sum over the 2x3x4 array's third axis (where the axis id starts with 0).</p>

<p>If your arrays are small, then the above command will work just fine. However, if you have large arrays, then you may run into memory issues. Note that in the above example, numpy internally created a 2x3x4 array to perform the broadcasting. If we generalize A to have dimensions <code>a x z</code> and B to have dimensions <code>b x z</code>, then numpy will internally create an <code>a x b x z</code> array for broadcasting.</p>

<p>We can avoid creating this intermediate array by doing some mathematical manipulation. Because you are computing the Euclidean distance as a sum-of-squared-differences, we can take advantage of the mathematical fact that sum-of-squared-differences can be rewritten.</p>

<p><a href=""https://i.stack.imgur.com/Hf8Gj.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/Hf8Gj.png"" alt=""enter image description here""></a></p>

<p>Note that the middle term involves the sum over <strong>element-wise</strong> multiplication. This sum over multiplcations is better known as a dot product. Because A and B are each a matrix, then this operation is actually a matrix multiplication. We can thus rewrite the above as:</p>

<p><a href=""https://i.stack.imgur.com/QxNaK.png"" rel=""noreferrer""><img src=""https://i.stack.imgur.com/QxNaK.png"" alt=""enter image description here""></a></p>

<p>We can then write the following numpy code:</p>

<pre><code>threeSums = np.sum(np.square(A)[:,np.newaxis,:], axis=2) - 2 * A.dot(B.T) + np.sum(np.square(B), axis=1)
dist = np.sqrt(threeSums)
dist
# array([[ 3.74165739,  0.        ,  8.06225775],
#        [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>

<p>Note that the answer above is exactly the same as the previous implementation. Again, the advantage here is the we do not need to create the intermediate 2x3x4 array for broadcasting.</p>

<p>For completeness, let's double-check that the dimensions of each summand in <code>threeSums</code> allowed broadcasting.</p>

<pre><code>np.sum(np.square(A)[:,np.newaxis,:], axis=2) has dimensions 2 x 1
2 * A.dot(B.T) has dimensions                               2 x 3
np.sum(np.square(B), axis=1) has dimensions                 1 x 3
</code></pre>

<p>So, as expected, the final <code>dist</code> array has dimensions 2x3.</p>

<p>This use of the dot product in lieu of sum of element-wise multiplication is also discussed in <a href=""http://eli.thegreenplace.net/2015/broadcasting-arrays-in-numpy/"" rel=""noreferrer"">this tutorial</a>.</p>
"
"1835351","1271","<python><numpy><machine-learning><vectorization>","27948363","13","Numpy Broadcast to perform euclidean distance vectorized","<p>I have matrices that are 2 x 4 and 3 x 4. I want to find the euclidean distance across rows, and get a 2 x 3 matrix at the end. Here is the code with one for loop that computes the euclidean distance for every row vector in a against all b row vectors. How do I do the same without using for loops?</p>

<pre><code> import numpy as np
a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
dists = np.zeros((2, 3))
for i in range(2):
      dists[i] = np.sqrt(np.sum(np.square(a[i] - b), axis=1))
</code></pre>
","2015-01-14 16:57:46","27948463","570918","41091","2","42184755","<p>Using <a href=""https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html"" rel=""nofollow noreferrer"">numpy.linalg.norm</a> also works well with broadcasting. Specifying an integer value for <code>axis</code> will use a vector norm, which defaults to Euclidean norm.</p>

<pre><code>import numpy as np

a = np.array([[1,1,1,1],[2,2,2,2]])
b = np.array([[1,2,3,4],[1,1,1,1],[1,2,1,9]])
np.linalg.norm(a[:, np.newaxis] - b, axis = 2)

# array([[ 3.74165739,  0.        ,  8.06225775],
#       [ 2.44948974,  2.        ,  7.14142843]])
</code></pre>
"
"4140027","3975","<python><machine-learning><nlp><scikit-learn>","27978507","0","How to select hyper parameters for SVC estimator in scikit learn?","<p>I'm classifiying some texts with SVC and I would like to run a grid search so I followed the example provided at the <a href=""http://scikit-learn.org/stable/auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py"" rel=""nofollow"">documentation</a>. In the example they are using SGDClassifier with the following parameters:</p>

<pre><code>parameters = {
    'vect__max_df': (0.5, 0.75, 1.0),
    #'vect__max_features': (None, 5000, 10000, 50000),
    'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
    #'tfidf__use_idf': (True, False),
    #'tfidf__norm': ('l1', 'l2'),
    'clf__alpha': (0.00001, 0.000001),
    'clf__penalty': ('l2', 'elasticnet'),
    #'clf__n_iter': (10, 50, 80),
}
</code></pre>

<p>My issue is, what kind of parameters should I try for SVC classifier, tfidf, hashing vectorizer and CountVectorizer?. How should I select this parameters if this is a multi class classification problem?.</p>
","2015-01-16 06:36:55","27980068","4370183","371","2","27980068","<p>You should read what those parameters mean and decide for yourself.</p>

<p>I optimize following: 
SVC: gamma and C
TFIDF: max_features, max_df, min_df</p>

<p>That really depends on your data and model. If you don't know run very wide optimization on many parameters and many possible values to narrow it down.
This is time consuming so you may want to subsample your data.</p>
"
"4420776","2669","<r><machine-learning><r-caret>","27980793","0","caret: RFE with tuneLength and rerank","<p>When using rfe() with tuneLength set and rerank = TRUE, is the parameter in tuneLength optimized when rankings are recalculated on the subset of features?
Here is an exmple of the analysis I am doing:</p>

<pre><code>library(""caret"")
set.seed(342)
train &lt;- as.data.frame ( matrix( rnorm(1e4) , 100, 100 ) )

ctrl &lt;- rfeControl(functions = caretFuncs,        
               rerank = TRUE,
               method = ""repeatedcv"",
               number=2, 
               repeats=1,
               verbose =TRUE
)

pls.fit.rfe &lt;- rfe(V1 ~ .,
               data = train,   
               method = ""pls"",                    
               sizes =  c(2,5),
               tuneLength = 5, 
               rfeControl = ctrl
)
</code></pre>
","2015-01-16 09:28:18","","1078601","11334","0","27987421","<p>The re-ranking happens after the each PLS fit. I don't think that the rankings have any direct impact on the model tuning. </p>

<p>Max</p>
"
"492372","24812","<java><machine-learning><weka><svm>","27985072","1","What does this exception mean when running LibSVM using Weka?","<p>I am writing this code to do a 5-fold cross validation using LibSVM using Weka.</p>

<pre><code>    LibSVM svm = new LibSVM();
    svm.setKernelType(new SelectedTag(2, LibSVM.TAGS_KERNELTYPE));
    svm.setDegree(2);
    //Run a cross validation to select the right parameters
    CVParameterSelection ps = new CVParameterSelection();
    ps.setClassifier(svm);
    ps.setNumFolds(5);  // using 5-fold CV
    ps.addCVParameter(""G 1 10 .1"");

    // build and output best options
    //I get exception below -&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;
    ps.buildClassifier(isTrainingSet);
    System.out.println(Utils.joinOptions(ps.getBestClassifierOptions()));
</code></pre>

<p>But it throws an exception saying</p>

<pre><code>Exception in thread ""main"" java.lang.Exception: Error: gamma &lt; 0
at weka.classifiers.functions.LibSVM.buildClassifier(LibSVM.java:1690)
at weka.classifiers.meta.CVParameterSelection.findParamsByCrossValidation(CVParameterSelection.java:377)
at weka.classifiers.meta.CVParameterSelection.findParamsByCrossValidation(CVParameterSelection.java:354)
at weka.classifiers.meta.CVParameterSelection.buildClassifier(CVParameterSelection.java:628)
at ExtractTopics.main(ExtractTopics.java:164)
</code></pre>

<p>I think the parameter gamme never is less than zero, right? the initial value is 1 and final value is 10 with 10 steps. So, why is this exception then?</p>
","2015-01-16 13:30:37","","492372","24812","3","27985412","<p>Answering my own question below.</p>

<p>The last parameter among the three values is not the amount of step size to take. It is the number of steps to perform. So it was incorrect to say above that I want to perform 0.1 steps. It should have been 100 steps so that Weka can infer that it needs to take 0.1 step size internally.</p>

<p>Using [1 10 100] helped and it ran fine.  </p>

<p>The answer I found is here - <a href=""http://weka.8497.n7.nabble.com/Problem-evaluating-classifier-C-lt-0-error-td31971.html"" rel=""nofollow"">http://weka.8497.n7.nabble.com/Problem-evaluating-classifier-C-lt-0-error-td31971.html</a></p>
"
"492372","24812","<machine-learning><weka><svm><libsvm>","27986849","1","How to do a grid search programmatically for SVM in Weka","<p>What is the piece of code in Java that I can use to do a grid search for SVM parameters using LibSVM in Weka?</p>

<p>Currently, I am able to search for a good value for only 1 parameter using the following piece of code:</p>

<pre><code>LibSVM svm = new LibSVM();
    svm.setKernelType(new SelectedTag(2, LibSVM.TAGS_KERNELTYPE));
    svm.setDegree(2);
    //Run a cross validation to select the right parameters
    CVParameterSelection ps = new CVParameterSelection();
    ps.setClassifier(svm);
    ps.setNumFolds(5);  // using 5-fold CV
    ps.addCVParameter(""G 0.1 10 100"");
</code></pre>
","2015-01-16 15:03:44","","4462487","110","0","27991386","<p>This seems like a reliable API:
<a href=""http://java-ml.sourceforge.net/api/0.1.6/libsvm/GridSearch.html"" rel=""nofollow"">http://java-ml.sourceforge.net/api/0.1.6/libsvm/GridSearch.html</a></p>

<p>The GridSearch constructor accepts a LibSVM object, Dataset object and an integer representing the number of folds to use for the grid search. The search method can then be called on the GridSearch object. The inputs for this method are the initial SVM parameters (including C and Gamma) and the outputs are the optimal C and Gamma values.</p>
"
"3785667","135","<azure><machine-learning><cors><azure-machine-learning-studio>","27987910","12","Azure Machine Learning - CORS","<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!</p>
","2015-01-16 16:00:31","35145088","4589073","1231","4","35145088","<p>Currently, we don't support disabling CORS on API side but you can either use the above option or you can use the API management service to disable CORS. The links below should help you with this</p>

<p>Here are the links: <a href=""http://azure.microsoft.com/en-us/documentation/articles/api-management-get-started/"" rel=""noreferrer"">step by step</a> guide, also this <a href=""http://channel9.msdn.com/Blogs/AzureApiMgmt/Last-mile-Security"" rel=""noreferrer"">video</a> on setting headers, and <a href=""https://msdn.microsoft.com/en-us/library/azure/dn894084.aspx#JSONP"" rel=""noreferrer"">this doc</a> on policies.</p>

<p>API Management service allow CORS by enabling it in the API configuration page</p>
"
"3785667","135","<azure><machine-learning><cors><azure-machine-learning-studio>","27987910","12","Azure Machine Learning - CORS","<p>I've searched for hours for this and can't find a single thing that answers the question. I've created and published a new Azure Machine Learning service, and have created an endpoint. I can call the service using the Postman REST CLient, but accessing it via a JavaScript webpage returns a console log saying that CORS is enabled for the service. Now, for the life of me, I can't figure out how to disable CORS for Azure Machine Learning services. Any help would be much appreciated, thanks!</p>
","2015-01-16 16:00:31","35145088","9929041","530","1","52111270","<p>Just an excerpt from the Azure ML Book (one may find it useful):</p>

<blockquote>
  <p>This CORS restriction really means that if you wish to fully exploit
  Azure Machine Learning web services for deployment, testing, and
  production for a wide variety of (web) clients, you will need to host
  your own server-side applications. You basically have two choices.</p>
</blockquote>

<ol>
<li>Host a web application, such as an ASP.NET webpage, and invoke the Azure Machine Learning web service server-side to conform to the current Azure Machine Learning CORS restrictions.</li>
<li>Host your own web service that does provide CORS support and can in turn invoke the Azure Machine Learning web service on behalf of a wide variety of web and mobile clients via modern protocols and data formats like REST and JSON.</li>
</ol>
"
"356729","6214","<machine-learning><scikit-learn><random-forest>","28002991","2","How to use whole training example to estimate class probabilities in sklearn RandomForest","<p>I want to use scikit-learn RandomForestClassifier to estimate the probabilities of a given example to belong to a set of classes, after prior training of course.</p>

<p>I know I can get the class probabilities using the <a href=""http://scikit-learn.org/dev/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict_proba"" rel=""nofollow noreferrer""><code>predict_proba</code></a> method, that calculates them as </p>

<blockquote>
  <p>[...] the mean predicted class probabilities of the trees in the forest.</p>
</blockquote>

<p>In <a href=""https://stackoverflow.com/a/14193569/356729"">this question</a> it is mentioned that:</p>

<blockquote>
  <p>The probabilities returned by a single tree are the normalized class
  histograms of the leaf a sample lands in.</p>
</blockquote>

<p>Now, I've been reading some papers on probability estimation and realized there isn't a trivial solution. According to <a href=""http://people.dsv.su.se/~henke/papers/bostrom07c.pdf"" rel=""nofollow noreferrer"">Estimating Class Probabilities in Random Forests (Böstrom)</a>:</p>

<blockquote>
  <p>using the same examples to both grow the trees and estimate the
  probabilities, [...] by necessity will lead to pure (and therefore
  small) estimation sets</p>
</blockquote>

<p>And this is bad. The solution appears to be to use all the examples in the training set, instead of only the ones in the bootstrap sample used to grow the tree.</p>

<p>Scikit-learn does use only the bootstrap sample for each tree to calculate the probability estimate of each class, right? <strong>Does somebody have any pointers about how to proceed to make the class probabilities come from the whole training set of the RandomForest instead?</strong></p>

<p>I assume this would need some special <code>Tree</code> subclassing that doesn't assign class probabilities to the leaves of the trees and then some procedure to assign them from the RandomForest classifier using the whole training set.</p>
","2015-01-17 18:46:24","28367138","676634","23137","2","28367138","<blockquote>
  <p>Scikit-learn does use only the bootstrap sample for each tree to calculate the probability estimate of each class, right? </p>
</blockquote>

<p>No, it uses only the in-sample part, and therefore will not give very calibrated probability outputs (which I guess is what the paper suggests).</p>

<p>You could get better probability estimates using the out-of-sample estimates, and maybe that would even be done easily with the current code base. Maybe it would be better to use a calibration method as post-processing (using the out-of-bag samples).</p>

<p>Anyhow, what you want to achieve is the default.</p>
"
"2102122","1112","<machine-learning><kernel-density><probability-density>","27998945","0","Calculate Bias of Parzen WIndows analytically","<p>I'm still having some trouble understanding what Bias and Variance for a specific estimator actually are. </p>

<p>I'm working with the definition of Bias as it is found on Wikipedia:</p>

<p><img src=""https://i.stack.imgur.com/piQ4C.png"" alt=""Bias of an estimator""></p>

<p>If we define kernel-density-estimates as</p>

<p><img src=""https://i.stack.imgur.com/tohFa.png"" alt=""Wikipedia definition of kernel density estimate""></p>

<p>But how can I apply this to kernel density estimation, or to be more exact Parzen Windows? Can someone at least give me an idea how the estimated density f_hat(x) relates to Bias (and Variance)?</p>

<p>Qualitative I can already tell, that a box-window containing the whole data space will have maximum bias and no variance as the estimated density will simply be the average of the whole training data set. </p>
","2015-01-17 11:23:26","27999150","2102122","1112","0","27999150","<p>I think I just figured it out myself. The parameter theta in the case of density estimation is .. drumroll... the density function f(x). So the bias is defined as </p>

<p><strong>Bias = E[f_hat(x)] - f(x)</strong></p>

<p>The E[f_hat(x)] term is the expected value or the <em>mean</em> of the window function. Calculating it involves a simple integral. </p>

<p><em>f(x)</em> is the <em>true</em> density function of the data, which in reality is likely to be unknown. </p>
"
"4403855","193","<python><machine-learning><scikit-learn>","28005307","6","GridSearchCV no reporting on high verbosity","<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
","2015-01-17 23:04:33","28056038","4370183","371","1","28021176","<p>I noticed that GridSearch does not output anything when run with more than 1 thread. 
When all threads are finished it prints out everything.
And multithreading does not work in Windows - use n_jobs > 1 only in Linux.</p>
"
"4403855","193","<python><machine-learning><scikit-learn>","28005307","6","GridSearchCV no reporting on high verbosity","<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
","2015-01-17 23:04:33","28056038","4403855","193","6","28056038","<p>Okay, so just to summarize everything I've figured out about it over the past few days.</p>

<ul>
<li>On Windows 8.1 don't set n_jobs to anything other than 1 if you still want it to be verbose.</li>
<li>In my case, even though I only have n_jobs = 1, all of my processor cores were still involved in the calculations, so either this is a bug or should be better documented.</li>
<li>I made the horrible mistake of using a list of csr matrices, so basically, read the documentation and then read it again before you ask questions.</li>
</ul>

<p>Again I'd like to thank @Barmaley.exe for the initial tip.</p>
"
"4403855","193","<python><machine-learning><scikit-learn>","28005307","6","GridSearchCV no reporting on high verbosity","<p>Okay, I'm just going to say starting out that I'm entirely new to SciKit-Learn and data science. But here is the issue and my current research on the problem. Code at the bottom.</p>

<h2>Summary</h2>

<p>I'm trying to do type recognition (like digits, for example) with a BernoulliRBM and I'm trying to find the correct parameters with GridSearchCV. However I don't see anything going on. With a lot of examples using verbosity settings I see output and progress, but with mine it just says,</p>

<pre><code>Fitting 3 folds for each of 15 candidates, totalling 45 fits
</code></pre>

<p>Then it sits there and does nothing....forever (or 8 hours, the longest I've waited with high verbosity settings).</p>

<p>I have a pretty large data set (1000 2D arrays each of size 428 by 428), so this might be the problem but I've also set the verbosity to 10 so I feel like I should be seeing some kind of output or progress. Also, in terms of my ""target"", it is just either a 0 or a 1, either it is the object I'm looking for (1), or it isn't (0).</p>

<h2>Previous Research</h2>

<ul>
<li>I looked into sklearn.preprocessing to see if that was necessary, it doesn't seem to be the issue (but again, I'm entirely new to this).</li>
<li>I increased verbosity</li>
<li>I switched from using a 3D list of data to using a list of scipy csr matrices.</li>
<li>I waited 8 hours with high verbosity settings, I still don't see anything happening.</li>
<li>I switched from not using a pipeline, to using a pipeline</li>
<li><p>I tampered with various parameters of gridsearchcv and tried creating fake (smaller) data sets to practice on.</p>

<pre><code>def network_trainer(self, data, files):
    train_x, test_x, train_y, test_y = train_test_split(data, files, test_size=0.2, random_state=0)

    parameters = {'learning_rate':np.arange(.25, .75, .1), 'n_iter':[5, 10, 20]}
    model = BernoulliRBM(random_state=0, verbose=True)
    model.cv = 2
    model.n_components = 2

    logistic = linear_model.LogisticRegression()
    pipeline = Pipeline(steps=[('model', model), ('clf', logistic)])

    gscv = grid_search.GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=10)
    gscv.fit(train_x, train_y)
    print gscv.best_params_
</code></pre></li>
</ul>

<p>I'd really appreciate a nudge in the right direction here. Thanks for considering my issue.</p>
","2015-01-17 23:04:33","28056038","3052835","1281","0","36575885","<p>Are you using ipython notebook and Python 2.x? If yes then multiprocessing module doesn't work with this combination. You can export (save as) your ipython notebook as a regular .py file and run it with regular python interpreter. Then you can use n_jobs=-1</p>
"
"46190","7581","<machine-learning><neural-network><conv-neural-network>","28005462","6","How to determine the number of feature maps to use in a convolutional neural network layer?","<p>I've been doing a lot of reading on Conv Nets and even some playing using Julia's Mocha.jl package (which looks a lot like Caffe, but you can play with it in the Julia REPL).</p>

<p>In a Conv net, Convolution layers are followed by ""feature map"" layers. What I'm wondering is how does one determine how many feature maps a network needs to have to solve some particular problem? Is there any science to this or is it more art? I can see that if you're trying to make a classification at least that last layer should have number of feature maps == number of classes (unless you've got a fully connected MLP at the top of the network, I suppose).</p>

<p>In my case, I 'm not doing a classification so much as trying to come up with a value for every pixel in an image (I suppose this could be seen as a classification where the classes are from 0 to 255).</p>

<p>Edit: as pointed out in the comments, I'm trying to solve a regression problem where the outputs are in a range from 0 to 255 (grayscale in this case). Still, the question remains: How does one determine how many feature maps to use at any given convolution layer? Does this differ for a regression problem vs. a classification problem?</p>
","2015-01-17 23:23:42","28021139","4367179","1031","5","28021139","<p>Basically, like any other hyperparameter - by evaluting results on separate development set and finding what number works best. It also worth checking publications that deal with similar problem and finding what number of feature maps they were using. </p>
"
"46190","7581","<machine-learning><neural-network><conv-neural-network>","28005462","6","How to determine the number of feature maps to use in a convolutional neural network layer?","<p>I've been doing a lot of reading on Conv Nets and even some playing using Julia's Mocha.jl package (which looks a lot like Caffe, but you can play with it in the Julia REPL).</p>

<p>In a Conv net, Convolution layers are followed by ""feature map"" layers. What I'm wondering is how does one determine how many feature maps a network needs to have to solve some particular problem? Is there any science to this or is it more art? I can see that if you're trying to make a classification at least that last layer should have number of feature maps == number of classes (unless you've got a fully connected MLP at the top of the network, I suppose).</p>

<p>In my case, I 'm not doing a classification so much as trying to come up with a value for every pixel in an image (I suppose this could be seen as a classification where the classes are from 0 to 255).</p>

<p>Edit: as pointed out in the comments, I'm trying to solve a regression problem where the outputs are in a range from 0 to 255 (grayscale in this case). Still, the question remains: How does one determine how many feature maps to use at any given convolution layer? Does this differ for a regression problem vs. a classification problem?</p>
","2015-01-17 23:23:42","28021139","3634699","94","1","41476572","<p>More art. The only difference between imagenet winners that use conv-nets has been changing the structure of layers and maybe some novel ways of training.</p>

<p>VGG is a neat example. Begins with filter sizes beginning with 2^7, then 2^8, then 2^9 followed by fully connected layers, then an output layer which will give you your classes. Your maps and layer depths can be completely unrelated to the number of output classes.</p>

<p>You would not want a fully connected layer at the top. That kind of defeats the purpose that convolutional nets were designed to solve (overfitting and optimizing hundreds of thousands of weights per neuron)</p>

<p>Training on big sets will require some heavy computational resources. If you're working with imagenet - there's a set of pre-trained models with caffe that you could build on top of <a href=""http://caffe.berkeleyvision.org/model_zoo.html"" rel=""nofollow noreferrer"">http://caffe.berkeleyvision.org/model_zoo.html</a></p>

<p>I'm not sure if you can port these to mocha. There's a port to tensor flow though if you're interested in that <a href=""https://github.com/ethereon/caffe-tensorflow"" rel=""nofollow noreferrer"">https://github.com/ethereon/caffe-tensorflow</a></p>
"
"3378649","4178","<python><machine-learning><scikit-learn>","28005537","3","How do I avoid re-training machine learning models","<p>self-learner here. </p>

<p>I am building a web application that predict events. </p>

<p>Let's consider this quick example. </p>

<pre><code>X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y) 

print(neigh.predict([[1.1]]))
</code></pre>

<p>How can I keep the state of <code>neigh</code> so when I enter a new value like <code>neigh.predict([[1.2]])</code> I don't need to re-train the model. Is there any good practice, or hint to start solving the problem  ? </p>
","2015-01-17 23:34:35","28008058","577088","123600","10","28006451","<p>You've chosen a slightly confusing example for a couple of reasons. First, when you say <code>neigh.predict([[1.2]])</code>, you aren't adding a new training point, you're just doing a new prediction, so that doesn't require any changes at all. Second, KNN algorithms aren't really ""trained"" -- KNN is an <a href=""http://en.wikipedia.org/wiki/Instance-based_learning"" rel=""noreferrer"">instance-based</a> algorithm, which means that ""training"" amounts to storing the training data in a suitable structure. As a result, this question has two different answers. I'll try to answer the KNN question first.</p>

<p><strong>K Nearest Neighbors</strong></p>

<p>For KNN, adding new training data amounts to appending new data points to the structure. However, it appears that <code>scikit-learn</code> doesn't provide any such functionality. (That's reasonable enough -- since KNN explicitly stores <em>every</em> training point, you can't just keep giving it new training points indefinitely.) </p>

<p>If you aren't using many training points, a simple list might be good enough for your needs! In that case, you could skip <code>sklearn</code> altogether, and just append new data points to your list. To make a prediction, do a linear search, saving the <code>k</code> nearest neighbors, and then make a prediction based on a simple ""majority vote"" -- if out of five neighbors, three or more are red, then return red, and so on. But keep in mind that every training point you add will slow the algorithm.</p>

<p>If you need to use many training points, you'll want to use a more efficient structure for nearest neighbor search, like a <a href=""http://en.wikipedia.org/wiki/K-d_tree"" rel=""noreferrer"">K-D Tree</a>. There's a <a href=""http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.KDTree.html"" rel=""noreferrer""><code>scipy</code></a> K-D Tree implementation that ought to work. The <code>query</code> method allows you to find the <code>k</code> nearest neighbors. It will be more efficient than a list, but it will still get slower as you add more training data.</p>

<p><strong>Online Learning</strong></p>

<p>A more general answer to your question is that you are (unbeknownst to yourself) trying to do something called <a href=""http://en.wikipedia.org/wiki/Online_machine_learning"" rel=""noreferrer"">online learning</a>. Online learning algorithms allow you to use individual training points as they arrive, and discard them once they've been used. For this to make sense, you need to be storing not the training points themselves (as in KNN) but a set of parameters, which you optimize.</p>

<p>This means that some algorithms are better suited to this than others. <code>sklearn</code> provides just a few algorithms <a href=""http://scikit-learn.org/stable/modules/scaling_strategies.html"" rel=""noreferrer"">capable of online learning</a>. These all have a <code>partial_fit</code> method that will allow you to pass training data in batches. The <a href=""http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier"" rel=""noreferrer""><code>SKDClassifier</code></a> with <code>'hinge'</code> or <code>'log'</code> loss is probably a good starting point. </p>
"
"3378649","4178","<python><machine-learning><scikit-learn>","28005537","3","How do I avoid re-training machine learning models","<p>self-learner here. </p>

<p>I am building a web application that predict events. </p>

<p>Let's consider this quick example. </p>

<pre><code>X = [[0], [1], [2], [3]]
y = [0, 0, 1, 1]
from sklearn.neighbors import KNeighborsClassifier
neigh = KNeighborsClassifier(n_neighbors=3)
neigh.fit(X, y) 

print(neigh.predict([[1.1]]))
</code></pre>

<p>How can I keep the state of <code>neigh</code> so when I enter a new value like <code>neigh.predict([[1.2]])</code> I don't need to re-train the model. Is there any good practice, or hint to start solving the problem  ? </p>
","2015-01-17 23:34:35","28008058","4370183","371","6","28008058","<p>Or maybe you just want to save your model after fitting</p>

<pre><code>joblib.dump(neigh, FName)
</code></pre>

<p>and load it when needed</p>

<pre><code>neigh = joblib.load(FName)
neigh.predict([[1.1]])
</code></pre>
"
"3070617","240","<machine-learning><classification><svm><svmlight>","28000132","1","How to provide cost for balancing training by imbalanced train dataset as available in svmlight?","<p>Cost in e1071's SVM doesn't seems same as svmlight's Cost. The manual of e1071 library states the following definition for its cost parameter:</p>

<pre><code>cost of constraints violation (default: 1)—it is the ‘C’-constant of the regular-
ization term in the Lagrange formulation
</code></pre>

<p>This is basically the allowance of miss-classification. There is one weight as provided by svmlight, described in its manual as:</p>

<pre><code>Cost: cost-factor, by which training errors on
      positive examples outweight errors on negative
      examples (default 1)
</code></pre>

<p>This cost is basically to allow balancing in case the train data doesn't has equal number of positive and negative data points. Is there anything similar in e1071's SVM implementation?</p>
","2015-01-17 13:50:22","28001773","4465180","26","0","28001773","<p>You probably want to look at the argument: class.weights (which is explained on the help page).</p>

<p>Best
David</p>
"
"969294","1431","<python><machine-learning><scikit-learn><classification>","28003222","1","Classifying text with scikit","<p>I'm learning Scikit machine-learning for a project and while I'm beginning to grasp the general process the details are a bit fuzzy still. </p>

<p>Earlier I managed to build a classifier, train it and test it with test set. I saved it to disk with cPickle. Now I want to create a class which loads this classifier and lets user to classify single tweets with it.</p>

<p>I thought this would be trivial but I seem to get ValueError('dimension mismatch') from X_new_tfidf = <em>self.tfidf_transformer.fit_transform(fitTweetVec)</em> line with following code:</p>

<pre><code>class TweetClassifier:

classifier = None
vect = TfidfVectorizer()
tfidf_transformer = TfidfTransformer()

#open the classifier saved to disk to be utilized later
def openClassifier(self, name):
    with open(name+'.pkl', 'rb') as fid:
        return cPickle.load(fid)

def __init__(self, classifierName):
    self.classifier = self.openClassifier(classifierName)
    self.classifyTweet(np.array([u""Helvetin vittu miksi aina pitää sataa vettä???""]))

def classifyTweet(self, tweetText):

    fitTweetVec = self.vect.fit_transform(tweetText)
    print self.vect.get_feature_names()
    X_new_tfidf = self.tfidf_transformer.fit_transform(fitTweetVec)
    print self.classifier.predict(X_new_tfidf)
</code></pre>

<p>What I'm doing wrong here? I used similar code while I made the classifier and ran test set for it. Have I forgotten some important step here?</p>

<p>Now I admit that I don't fully understand yet the fitting and transforming here since I found the Scikit's tutorial a bit ambiguous about it. If someone knows an as clear explanation of them as possible, I'm all for links :)</p>
","2015-01-17 19:11:46","28004192","1330293","33641","2","28004192","<p>The problem is that your classifier was trained with a fixed number of features (the length of the vocabulary of your previous data) and now when you <code>fit_transform</code> the new tweet, the <code>TfidfTransformer</code> will produce a new vocabulary and a new number of features, and will represent the new tweet in this space.</p>

<p>The solution is to also save the previously fitted <code>TfidfTransformer</code> (which contains the old vocabulary), load it with the classifier and <code>.transform</code> (not <code>fit_transform</code> because it was already fitted to the old data) the new tweet in this same representation. </p>

<p>You can also use a <code>Pipeline</code> that contains both the <code>TfidfTransformer</code> and the <code>Classifier</code> and pickle the <code>Pipeline</code>, this is easier and recommended.</p>
"
"4240359","113","<machine-learning><pos-tagger>","28002136","0","POS tagger and chunker","<p>I want to make a POS tagger and chunker using JAVA. But I am unable to figure out that from where should I start. What all libraries would be required?</p>
","2015-01-17 17:24:05","","4464328","33","0","28002388","<p>I think you should read articles or reports to know what they did. I' m working with Vietnamese processing, not know what libraries use in English. But I saw Stanford nlp while using Vietnamese tagger libraries.
I think grammar and lexicon are very important. 
This is NLP Stanford, try this. <a href=""http://nlp.stanford.edu/software/tagger.shtml"" rel=""nofollow"">http://nlp.stanford.edu/software/tagger.shtml</a></p>
"
"4240359","113","<machine-learning><pos-tagger>","28002136","0","POS tagger and chunker","<p>I want to make a POS tagger and chunker using JAVA. But I am unable to figure out that from where should I start. What all libraries would be required?</p>
","2015-01-17 17:24:05","","2798955","7938","1","28047388","<p>you can use various libraries</p>

<ul>
<li><a href=""http://nlp.stanford.edu/software/corenlp.shtml"" rel=""nofollow"">Stanford Core NLP</a> </li>
<li><a href=""https://opennlp.apache.org/"" rel=""nofollow"">OpenNLP</a></li>
<li><a href=""https://gate.ac.uk/gate/doc/plugins.html"" rel=""nofollow"">Gate</a></li>
</ul>

<p>I used OpenNLP in my project. I think this instructions will help you to go through OpenNLP Library. Follow this <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html"" rel=""nofollow"">document</a></p>

<ol>
<li>First download the models from this <a href=""http://opennlp.sourceforge.net/models-1.5/"" rel=""nofollow"">page</a></li>
<li>Then add those into your project</li>
<li>You also need <code>Tokenizer</code> model to break sentences into tokens. Then pass these tokens into POS Tagger.</li>
</ol>

<p>Code Samples</p>

<hr>

<p><strong>Load Model</strong></p>

<pre><code>InputStream modelIn = null;

try {
  modelIn = new FileInputStream(""en-pos-maxent.bin"");
  POSModel model = new POSModel(modelIn);
}
catch (IOException e) {
  // Model loading failed, handle the error
  e.printStackTrace();
}
finally {
  if (modelIn != null) {
    try {
      modelIn.close();
    }
    catch (IOException e) {
    }
  }
}
</code></pre>

<hr>

<p><strong>Instantiate POSTaggerME</strong> </p>

<pre><code>POSTaggerME tagger = new POSTaggerME(model);
</code></pre>

<hr>

<p><strong>Generate TAGS</strong></p>

<pre><code>    String sent[] = new String[]{""Most"", ""large"", ""cities"", ""in"", ""the"", ""US"", ""had"",
                                 ""morning"", ""and"", ""afternoon"", ""newspapers"", "".""};
//This is manual String tokens of a sentence. To Generate word token use [Tokenizer Model][6]         
    String tags[] = tagger.tag(sent);
</code></pre>

<p>Links</p>

<ul>
<li>Implement <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.tokenizer"" rel=""nofollow"">Tokenizer Model</a></li>
<li>Implement <a href=""https://opennlp.apache.org/documentation/1.5.3/manual/opennlp.html#tools.chunker"" rel=""nofollow"">Chunker</a></li>
</ul>
"
"194106","3252","<machine-learning><classification><svm><libsvm>","28023554","0","max-margin linear separator using libsvm","<p>I have a set of N data points X with +/- labels for which I'd like to calculate the max-margin linear separator (aka classifier, hyperplane) or fail if no such linear separator exist. </p>

<p>I do not want to avoid overfitting in the context of this question, as I do so elsewhere. So no slack variables ; no cross-validation ; no limits on the number of support vectors ; just find max-margin separator or fail.</p>

<p>How to I use libsvm to do so? I believe you can't give c=0 in C-SVM and you can't give nu=1 in nu-svm.</p>

<p>Related question (which I think didn't provide an answer):
<a href=""https://stackoverflow.com/questions/9375502/which-of-the-parameters-in-libsvm-is-the-slack-variable"">Which of the parameters in LibSVM is the slack variable?</a></p>
","2015-01-19 11:25:42","28044905","682624","133","1","28044905","<p>In the case of C-SVM, you should use a linear kernel and a very large C value (or nu = 0.999... for nu-SVM). If you still have slacks with this setting, probably your data is not linearly separable.</p>

<p>Quick explanation: the C-SVM optimization function tries to find the hyperplane having maximum margin and lowest misclassification costs at the same time. The misclassification costs in the C-SVM formulation is defined by: distance from the misclassified point to its correct side of the hyperplane, multiplied by C. If you increase the C value (or nu value for nu-SVM), every misclassified point will be too costly and an hyperplane that separates the data perfectly will be preferable for the optimization function.</p>
"
"4469858","21","<python><machine-learning><scikit-learn>","28024191","2","Using precomputed Gram matrix in sklearn linear models (Lasso, Lars, etc)","<p>I'm trying to train a linear model on a very large dataset. 
The feature space is small but there are too many samples to hold in memory. 
I'm calculating the Gram matrix on-the-fly and trying to pass it as an argument to sklearn Lasso (or other algorithms) but, when I call fit, it needs the actual X and y matrices. </p>

<p>Any idea how to use the 'precompute' feature without storing the original matrices?</p>
","2015-01-19 12:00:38","","1403102","176","0","28024414","<p>(My answer is based on the usage of svm.SVC, Lasso may be different.)</p>

<p>I think that you are supposed pass the Gram matrix instead of X to the fit method.</p>

<p>Also, the Gram matrix has shape (n_samples, n_samples) so it should also be too large for memory in your case, right?</p>
"
"2590727","359","<java><machine-learning><predictionio>","28027507","1","EasyRec vs PredictionIO vs Apache Mahout","<p>I would like to develop real-time analytic tools for my website ( engineering project). My application will be written in JAVA-EE and maven. I have found three tools (topic). I knew that PredictionIO is using Apache Mahout. But I can't decide which algorithm use. If someone knew what are the pros and cons of this algorithms let them write. What do you think, which will be the best ?</p>
","2015-01-19 15:00:27","28248817","2212075","46","2","28248817","<p>As of version 0.8 of PredictionIO the stack has been rebuilt on Apache Spark and now integrates <a href=""https://spark.apache.org/docs/0.9.1/mllib-guide.html"" rel=""nofollow"">MLlib</a> library. It is hard to advise about pros and cons of different algos without knowing your use case and in more detail feel free to ask on our PredictionIO support forum.</p>

<p>Databricks (creators of Apache Spark) also have some <a href=""https://databricks.com/blog/2014/07/23/scalable-collaborative-filtering-with-spark-mllib.html"" rel=""nofollow"">performance benchmarks for MLlib versus Mahout</a>.</p>
"
"1458453","87","<machine-learning><classification><cluster-analysis><supervised-learning><unsupervised-learning>","28028017","0","Is supervised learning synonymous to classification and unsupervised learning synonymous to clustering?","<p>I am a beginner in machine learning and recently read about supervised and unsupervised machine learning. It looks like supervised learning is synonymous to classification and unsupervised learning is  synonymous to clustering, is it so?</p>
","2015-01-19 15:25:56","28028879","1190430","5156","3","28028879","<p>No.</p>

<p>Supervised learning is when you know correct answers (targets). Depending on their type, it might be <a href=""http://en.wikipedia.org/wiki/Statistical_classification"" rel=""nofollow"">classification</a> (categorical targets), <a href=""http://en.wikipedia.org/wiki/Regression_analysis"" rel=""nofollow"">regression</a> (numerical targets) or <a href=""http://en.wikipedia.org/wiki/Learning_to_rank"" rel=""nofollow"">learning to rank</a> (ordinal targets) (this list is by no means complete, there might be other types that I either forgot or unaware of).</p>

<p>On the contrary, in unsupervised learning setting we don't know correct answers, and we try to infer, learn some structure from data. Be it cluster number or low-dimensional approximation (<a href=""http://en.wikipedia.org/wiki/Dimensionality_reduction"" rel=""nofollow"">dimensionality reduction</a>, actually, one might think of clusterization as of extreme 1D case of dimensionality reduction). Again, this might be far away from completeness, but the general idea is about hidden structure, that we try to discover from data.</p>
"
"3393459","8722","<python><numpy><pandas><machine-learning><scikit-learn>","28035216","9","Ordered Logit in Python?","<p>I'm interested in running an ordered logit regression in python (using pandas, numpy, sklearn, or something that ecosystem). But I cannot find any way to do this. Is my google-skill lacking? Or is this not something that's been implemented in a standard package?</p>
","2015-01-19 23:20:07","32007463","744239","1056","5","32007463","<p>If you're looking for Ordered Logistic Regression, it looks like you can find it in <a href=""https://github.com/fabianp/minirank/blob/master/minirank/logistic.py"" rel=""noreferrer"">Fabian Pedregosa's <code>minirank</code> repo on GitHub</a>.</p>

<p>(Hattip to @elyase, who originally provided the link in a comment on the question.)</p>
"
"2028043","5341","<machine-learning><classification><nearest-neighbor><knn>","28046735","1","KNN choosing classlabel when k=4","<p>In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). </p>

<ol>
<li>If k = 1, then the object is simply assigned to the class of that single nearest neighbor.</li>
<li>If k=3, and the classlabels are Good =2 Bad=1,then the predicted classlabel will be Good,which contains the magority vote.</li>
<li><strong>If k=4, and the classlabels are Good =2 Bad=2, What will be the classlabel?</strong> </li>
</ol>
","2015-01-20 13:37:46","28046982","2328763","11296","2","28046982","<p>There are different approaches. For example Matlab uses 'random' or 'nearest' as documented <a href=""http://de.mathworks.com/help/bioinfo/ref/knnclassify.html"" rel=""nofollow"">here</a>.</p>

<blockquote>
  <p>When classifying to more than two groups or when using an even value
  for k, it might be necessary to break a tie in the number of nearest
  neighbors. Options are 'random', which selects a random tiebreaker,
  and 'nearest', which uses the nearest neighbor among the tied groups
  to break the tie.</p>
</blockquote>
"
"2028043","5341","<machine-learning><classification><nearest-neighbor><knn>","28046735","1","KNN choosing classlabel when k=4","<p>In k-NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). </p>

<ol>
<li>If k = 1, then the object is simply assigned to the class of that single nearest neighbor.</li>
<li>If k=3, and the classlabels are Good =2 Bad=1,then the predicted classlabel will be Good,which contains the magority vote.</li>
<li><strong>If k=4, and the classlabels are Good =2 Bad=2, What will be the classlabel?</strong> </li>
</ol>
","2015-01-20 13:37:46","28046982","1060350","70392","0","28055307","<p>This problem is <strong>not specific to k=4</strong>.</p>

<p>Consider a data set with 3 classes. At k=2, two different classes may arise. At k=3, three different classes may arise, at k=4, it may be 0,2,2... any k beyond 1 bears the risk of a <em>tie</em>.</p>

<p>Choose one at random, or use weighting (i.e. give the 1NN more weight than the 2nd nearest neighbor etc.) to further reduce the risk of ties.</p>
"
"3016483","576","<matlab><opencv><machine-learning><computer-vision><mex>","28048441","1","Running Stretchable Models for Human parsing Code","<p>I'm trying to run the code given in the following link on Ubuntu 13.04 (64 bit):</p>

<p><a href=""https://github.com/bensapp/Stretchable-Models-for-Motion-Parsing"" rel=""nofollow"">https://github.com/bensapp/Stretchable-Models-for-Motion-Parsing</a></p>

<p>I'm getting the following error:</p>

<p>INVALID MEX FILE: 'filepath/cps/utils/mex_opencv_boosting.mexa64': filepath/cps/utils/mex_opencv_bosting.mexa64: undefined symbol: _ZN7CvBoostC1EPKc.</p>

<p>Can someone help me with fixing this error?</p>

<p>Thank you</p>

<hr>

<p>When I run the command lld mex_opencv_boosting.mexa64, I get the following output. Everything seems to be fine.</p>

<pre><code>
linux-vdso.so.1 =>  (0x00007fff7c588000)
    libcxcore.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libcxcore.so.4 (0x00007ff8f63b0000)
    libcv.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libcv.so.4 (0x00007ff8f5f2b000)
    libml.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libml.so.4 (0x00007ff8f5c9e000)
    libhighgui.so.4 => filepath/cps/thirdparty/OpenCV-2.0.0/lib/libhighgui.so.4 (0x00007ff8f5a70000)
    libmx.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmx.so (0x00007ff8f5748000)
    libmex.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmex.so (0x00007ff8f5523000)
    libmat.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmat.so (0x00007ff8f52d3000)
    libstdc++.so.6 => /usr/local/MATLAB/R2014a/sys/os/glnxa64/libstdc++.so.6 (0x00007ff8f4fcb000)
    libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff8f4c9a000)
    libgcc_s.so.1 => /usr/local/MATLAB/R2014a/sys/os/glnxa64/libgcc_s.so.1 (0x00007ff8f4a85000)
    libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff8f4867000)
    libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff8f449f000)
    librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff8f4297000)
    libz.so.1 => /lib/x86_64-linux-gnu/libz.so.1 (0x00007ff8f407d000)
    libdl.so.2 => /lib/x86_64-linux-gnu/libdl.so.2 (0x00007ff8f3e79000)
    libgomp.so.1 => /usr/lib/x86_64-linux-gnu/libgomp.so.1 (0x00007ff8f3c6a000)
    libpng12.so.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libpng12.so.0 (0x00007ff8f3a43000)
    libmwresource_core.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwresource_core.so (0x00007ff8f3841000)
    libmwi18n.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwi18n.so (0x00007ff8f3567000)
    libut.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libut.so (0x00007ff8f32ad000)
    libmwfl.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwfl.so (0x00007ff8f2eb1000)
    libmwMATLAB_res.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwMATLAB_res.so (0x00007ff8f28ab000)
    libboost_date_time.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_date_time.so.1.49.0 (0x00007ff8f269a000)
    libboost_signals.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_signals.so.1.49.0 (0x00007ff8f2482000)
    libboost_system.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_system.so.1.49.0 (0x00007ff8f227e000)
    libboost_thread.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_thread.so.1.49.0 (0x00007ff8f2063000)
    libmwcpp11compat.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwcpp11compat.so (0x00007ff8f1e56000)
    libboost_log.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_log.so.1.49.0 (0x00007ff8f1b77000)
    libboost_log_setup.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_log_setup.so.1.49.0 (0x00007ff8f1665000)
    libicudata.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicudata.so.49 (0x00007ff8f0345000)
    libicuuc.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicuuc.so.49 (0x00007ff8effbc000)
    libicui18n.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicui18n.so.49 (0x00007ff8efbad000)
    libicuio.so.49 => /usr/local/MATLAB/R2014a/bin/glnxa64/libicuio.so.49 (0x00007ff8ef9a0000)
    libtbb.so.2 => /usr/local/MATLAB/R2014a/bin/glnxa64/libtbb.so.2 (0x00007ff8ef859000)
    libtbbmalloc.so.2 => /usr/local/MATLAB/R2014a/bin/glnxa64/libtbbmalloc.so.2 (0x00007ff8ef724000)
    libmwservices.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwservices.so (0x00007ff8ef158000)
    libmwmpath.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwmpath.so (0x00007ff8eef11000)
    libmwm_dispatcher.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwm_dispatcher.so (0x00007ff8eec3d000)
    libboost_filesystem.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_filesystem.so.1.49.0 (0x00007ff8eea1d000)
    libhdf5_hl.so.6 => /usr/local/MATLAB/R2014a/bin/glnxa64/libhdf5_hl.so.6 (0x00007ff8ee7f0000)
    libhdf5.so.6 => /usr/local/MATLAB/R2014a/bin/glnxa64/libhdf5.so.6 (0x00007ff8ee371000)
    /lib64/ld-linux-x86-64.so.2 (0x00007ff8f6a53000)
    libexpat.so.1 => /usr/local/MATLAB/R2014a/bin/glnxa64/libexpat.so.1 (0x00007ff8ee149000)
    libcrypt.so.1 => /lib/x86_64-linux-gnu/libcrypt.so.1 (0x00007ff8edf0f000)
    libboost_chrono.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_chrono.so.1.49.0 (0x00007ff8edd08000)
    libboost_regex.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_regex.so.1.49.0 (0x00007ff8ed9ec000)
    libboost_serialization.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_serialization.so.1.49.0 (0x00007ff8ed774000)
    libunwind.so.8 => /usr/local/MATLAB/R2014a/bin/glnxa64/libunwind.so.8 (0x00007ff8ed556000)
    libmwregexp.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwregexp.so (0x00007ff8ed30d000)
    libmwmlutil.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwmlutil.so (0x00007ff8ecca9000)
    libmwsettingscore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwsettingscore.so (0x00007ff8ec83b000)
    libmwms.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwms.so (0x00007ff8ec296000)
    libmwMATLAB_settings_res.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwMATLAB_settings_res.so (0x00007ff8ec073000)
    libmwnativedisplay.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwnativedisplay.so (0x00007ff8ebe6a000)
    libmwopccore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopccore.so (0x00007ff8ebc0b000)
    libmwopcmodel.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopcmodel.so (0x00007ff8eb953000)
    libmwopczippackage.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopczippackage.so (0x00007ff8eb737000)
    libmwopcmwservices.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwopcmwservices.so (0x00007ff8eb4dd000)
    libboost_iostreams.so.1.49.0 => /usr/local/MATLAB/R2014a/bin/glnxa64/libboost_iostreams.so.1.49.0 (0x00007ff8eb2c7000)
    libxerces-c.so.27 => /usr/local/MATLAB/R2014a/bin/glnxa64/libxerces-c.so.27 (0x00007ff8eace4000)
    libncurses.so.5 => /lib/x86_64-linux-gnu/libncurses.so.5 (0x00007ff8eaac1000)
    libmwxmlcore.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libmwxmlcore.so (0x00007ff8ea84c000)
    libminizip.so => /usr/local/MATLAB/R2014a/bin/glnxa64/libminizip.so (0x00007ff8ea63f000)
    libtinfo.so.5 => /lib/x86_64-linux-gnu/libtinfo.so.5 (0x00007ff8ea415000)</code></pre>
","2015-01-20 14:56:33","","6094282","1","0","36135062","<p>Try the following</p>

<pre><code>export LD_LIBRARY_PATH=filepath/cps/thirdparty/opencv/lib:$LD_LIBRARY_PATH
</code></pre>

<p>After this run MATLAB from terminal. Your code should run. </p>

<p>What I have found is that the symbol: _ZN7CvBoostC1EPKc is there in filepath/cps/thirdparty/opencv/lib/libml.so.4</p>

<pre><code>nm filepath/cps/thirdparty/opencv/lib/libml.so.4 | grep _ZN7CvBoostC1EPKc
</code></pre>

<p>Not in the file filepath/cps/thirdparty/OpenCV-2.0.0/lib/libml.so.4 and somehow setting the environment variable from the MATLAB is not working. Exporting LD_LIBRARY_PATH before running MATLAB is necessary.</p>
"
"4156402","233","<matlab><image-processing><machine-learning><computer-vision><classification>","28068485","0","Classification from a feature vector","<p>I'm quite new to this; I'm try to classify textures as defective or non-defective. I've used a Gabor filter bank with Matlab which outputs a column vector of the Gabor features of an image. I have a data set of non-defective images and defective images. </p>

<p>My question is, what can I now do with this (or these) feature vectors to classify the texture? I've read about many types of classification, but couldn't find any similar types of implementation to help me get an idea of what I'm doing. Many thanks. </p>
","2015-01-21 13:29:45","28070101","3584765","4292","1","28068723","<p>There are many ways to go if you have extracted your feature vectors.</p>

<ul>
<li><p>For example you can use an svm approach on your samples from your two classes.</p></li>
<li><p>Simpler approaches include nearest neighbor, nearest centroid etc</p></li>
</ul>

<p>Edit:</p>

<p>I thought this would be a comment but it's getting too big to fit.</p>

<p>As regards the separability of your samples:</p>

<ul>
<li>One way to determine the linear separability is to use linear svm as a boundary (unless you are concerned about time efiiciency so you are stuck with linear anyway). This svm model does not overtrain and can give a clue about separability.</li>
<li>Other options include a pca that will project your samples to fewer dimensions and these reduced dimensional samples can be easily plotted to examine it visually. This approach has the advantage of visual examination but it depends from the pca step how well it represent the separability of your samples. Maybe the separability lie in a non-principal component (i.e. dimension of your samples) and then pca just fails.</li>
<li>As a rough approximation I often plot random dimensions of my samples together to get a quick (and maybe inaccurate of course) look of them. If for example you have samples of 100 dimension you can plot the first two dimensions only (as if you had 2-D samples) to see if your two classes collide in a large degree. If they do, then you can check other dimensions but if they don't then you know that they are separable at least to some dimensions.</li>
</ul>
"